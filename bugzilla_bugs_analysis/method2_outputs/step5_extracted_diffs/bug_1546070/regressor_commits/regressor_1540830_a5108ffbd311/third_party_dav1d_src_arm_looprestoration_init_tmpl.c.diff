# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: third_party/dav1d/src/arm/looprestoration_init_tmpl.c
# Commit: a5108ffbd311
# Full Hash: a5108ffbd31101e7cb72dabc271d92027f953c3c
# Author: Alex Chronopoulos <achronop@gmail.com>
# Date: 2019-04-20 09:35:22
# Regressor Bug: 1540830
# File Overlap Count: 3
# Description:
#   Bug 1540830 - Update dav1d from upstream to 1f7a7e8. r=TD-Linux
#   
#   Differential Revision: https://phabricator.services.mozilla.com/D28200
# ==============================================================================

diff -r 98adabf295d0 -r a5108ffbd311 third_party/dav1d/src/arm/looprestoration_init_tmpl.c
--- a/third_party/dav1d/src/arm/looprestoration_init_tmpl.c	Fri Apr 19 20:49:36 2019 +0000
+++ b/third_party/dav1d/src/arm/looprestoration_init_tmpl.c	Fri Apr 19 20:36:10 2019 +0000
@@ -29,6 +29,7 @@
 #include "src/looprestoration.h"
 
 #include "common/attributes.h"
+#include "src/tables.h"
 
 #if BITDEPTH == 8
 // This calculates things slightly differently than the reference C version.
@@ -91,7 +92,171 @@
         dav1d_copy_narrow_neon(dst + (w & ~7), dst_stride, tmp, w & 7, h);
     }
 }
-#endif
+
+#if ARCH_AARCH64
+void dav1d_sgr_box3_h_neon(int32_t *sumsq, int16_t *sum,
+                           const pixel (*left)[4],
+                           const pixel *src, const ptrdiff_t stride,
+                           const int w, const int h,
+                           const enum LrEdgeFlags edges);
+void dav1d_sgr_box3_v_neon(int32_t *sumsq, int16_t *sum,
+                           const int w, const int h,
+                           const enum LrEdgeFlags edges);
+void dav1d_sgr_calc_ab1_neon(int32_t *a, int16_t *b,
+                             const int w, const int h, const int strength);
+void dav1d_sgr_finish_filter1_neon(coef *tmp,
+                                   const pixel *src, const ptrdiff_t stride,
+                                   const int32_t *a, const int16_t *b,
+                                   const int w, const int h);
+
+/* filter with a 3x3 box (radius=1) */
+static void dav1d_sgr_filter1_neon(coef *tmp,
+                                   const pixel *src, const ptrdiff_t stride,
+                                   const pixel (*left)[4],
+                                   const pixel *lpf, const ptrdiff_t lpf_stride,
+                                   const int w, const int h, const int strength,
+                                   const enum LrEdgeFlags edges)
+{
+    ALIGN_STK_16(int32_t, sumsq_mem, (384 + 16) * 68 + 8,);
+    int32_t *const sumsq = &sumsq_mem[(384 + 16) * 2 + 8], *const a = sumsq;
+    ALIGN_STK_16(int16_t, sum_mem, (384 + 16) * 68 + 16,);
+    int16_t *const sum = &sum_mem[(384 + 16) * 2 + 16], *const b = sum;
+
+    dav1d_sgr_box3_h_neon(sumsq, sum, left, src, stride, w, h, edges);
+    if (edges & LR_HAVE_TOP)
+        dav1d_sgr_box3_h_neon(&sumsq[-2 * (384 + 16)], &sum[-2 * (384 + 16)],
+                              NULL, lpf, lpf_stride, w, 1, edges);
+
+    if (edges & LR_HAVE_BOTTOM)
+        dav1d_sgr_box3_h_neon(&sumsq[h * (384 + 16)], &sum[h * (384 + 16)],
+                              NULL, lpf + 6 * PXSTRIDE(lpf_stride),
+                              lpf_stride, w, 2, edges);
+
+    dav1d_sgr_box3_v_neon(sumsq, sum, w, h, edges);
+    dav1d_sgr_calc_ab1_neon(a, b, w, h, strength);
+    dav1d_sgr_finish_filter1_neon(tmp, src, stride, a, b, w, h);
+}
+
+void dav1d_sgr_box5_h_neon(int32_t *sumsq, int16_t *sum,
+                           const pixel (*left)[4],
+                           const pixel *src, const ptrdiff_t stride,
+                           const int w, const int h,
+                           const enum LrEdgeFlags edges);
+void dav1d_sgr_box5_v_neon(int32_t *sumsq, int16_t *sum,
+                           const int w, const int h,
+                           const enum LrEdgeFlags edges);
+void dav1d_sgr_calc_ab2_neon(int32_t *a, int16_t *b,
+                             const int w, const int h, const int strength);
+void dav1d_sgr_finish_filter2_neon(coef *tmp,
+                                   const pixel *src, const ptrdiff_t stride,
+                                   const int32_t *a, const int16_t *b,
+                                   const int w, const int h);
+
+/* filter with a 5x5 box (radius=2) */
+static void dav1d_sgr_filter2_neon(coef *tmp,
+                                   const pixel *src, const ptrdiff_t stride,
+                                   const pixel (*left)[4],
+                                   const pixel *lpf, const ptrdiff_t lpf_stride,
+                                   const int w, const int h, const int strength,
+                                   const enum LrEdgeFlags edges)
+{
+    ALIGN_STK_16(int32_t, sumsq_mem, (384 + 16) * 68 + 8,);
+    int32_t *const sumsq = &sumsq_mem[(384 + 16) * 2 + 8], *const a = sumsq;
+    ALIGN_STK_16(int16_t, sum_mem, (384 + 16) * 68 + 16,);
+    int16_t *const sum = &sum_mem[(384 + 16) * 2 + 16], *const b = sum;
+
+    dav1d_sgr_box5_h_neon(sumsq, sum, left, src, stride, w, h, edges);
+    if (edges & LR_HAVE_TOP)
+        dav1d_sgr_box5_h_neon(&sumsq[-2 * (384 + 16)], &sum[-2 * (384 + 16)],
+                              NULL, lpf, lpf_stride, w, 2, edges);
+
+    if (edges & LR_HAVE_BOTTOM)
+        dav1d_sgr_box5_h_neon(&sumsq[h * (384 + 16)], &sum[h * (384 + 16)],
+                              NULL, lpf + 6 * PXSTRIDE(lpf_stride),
+                              lpf_stride, w, 2, edges);
+
+    dav1d_sgr_box5_v_neon(sumsq, sum, w, h, edges);
+    dav1d_sgr_calc_ab2_neon(a, b, w, h, strength);
+    dav1d_sgr_finish_filter2_neon(tmp, src, stride, a, b, w, h);
+}
+
+void dav1d_sgr_weighted1_neon(pixel *dst, const ptrdiff_t dst_stride,
+                              const pixel *src, const ptrdiff_t src_stride,
+                              const coef *t1, const int w, const int h,
+                              const int wt);
+void dav1d_sgr_weighted2_neon(pixel *dst, const ptrdiff_t dst_stride,
+                              const pixel *src, const ptrdiff_t src_stride,
+                              const coef *t1, const coef *t2,
+                              const int w, const int h,
+                              const int16_t wt[2]);
+
+static void sgr_filter_neon(pixel *const dst, const ptrdiff_t dst_stride,
+                             const pixel (*const left)[4],
+                             const pixel *lpf, const ptrdiff_t lpf_stride,
+                             const int w, const int h, const int sgr_idx,
+                             const int16_t sgr_wt[7], const enum LrEdgeFlags edges)
+{
+    if (!dav1d_sgr_params[sgr_idx][0]) {
+        ALIGN_STK_16(coef, tmp, 64 * 384,);
+        dav1d_sgr_filter1_neon(tmp, dst, dst_stride, left, lpf, lpf_stride,
+                               w, h, dav1d_sgr_params[sgr_idx][3], edges);
+        if (w >= 8)
+            dav1d_sgr_weighted1_neon(dst, dst_stride, dst, dst_stride,
+                                     tmp, w & ~7, h, (1 << 7) - sgr_wt[1]);
+        if (w & 7) {
+            // For uneven widths, do a full 8 pixel wide filtering into a temp
+            // buffer and copy out the narrow slice of pixels separately into
+            // dest.
+            ALIGN_STK_16(pixel, stripe, 64 * 8,);
+            dav1d_sgr_weighted1_neon(stripe, w & 7, dst + (w & ~7), dst_stride,
+                                     tmp + (w & ~7), w & 7, h,
+                                     (1 << 7) - sgr_wt[1]);
+            dav1d_copy_narrow_neon(dst + (w & ~7), dst_stride, stripe,
+                                   w & 7, h);
+        }
+    } else if (!dav1d_sgr_params[sgr_idx][1]) {
+        ALIGN_STK_16(coef, tmp, 64 * 384,);
+        dav1d_sgr_filter2_neon(tmp, dst, dst_stride, left, lpf, lpf_stride,
+                               w, h, dav1d_sgr_params[sgr_idx][2], edges);
+        if (w >= 8)
+            dav1d_sgr_weighted1_neon(dst, dst_stride, dst, dst_stride,
+                                     tmp, w & ~7, h, sgr_wt[0]);
+        if (w & 7) {
+            // For uneven widths, do a full 8 pixel wide filtering into a temp
+            // buffer and copy out the narrow slice of pixels separately into
+            // dest.
+            ALIGN_STK_16(pixel, stripe, 64 * 8,);
+            dav1d_sgr_weighted1_neon(stripe, w & 7, dst + (w & ~7), dst_stride,
+                                     tmp + (w & ~7), w & 7, h, sgr_wt[0]);
+            dav1d_copy_narrow_neon(dst + (w & ~7), dst_stride, stripe,
+                                   w & 7, h);
+        }
+    } else {
+        ALIGN_STK_16(coef, tmp1, 64 * 384,);
+        ALIGN_STK_16(coef, tmp2, 64 * 384,);
+        dav1d_sgr_filter2_neon(tmp1, dst, dst_stride, left, lpf, lpf_stride,
+                               w, h, dav1d_sgr_params[sgr_idx][2], edges);
+        dav1d_sgr_filter1_neon(tmp2, dst, dst_stride, left, lpf, lpf_stride,
+                               w, h, dav1d_sgr_params[sgr_idx][3], edges);
+        const int16_t wt[2] = { sgr_wt[0], 128 - sgr_wt[0] - sgr_wt[1] };
+        if (w >= 8)
+            dav1d_sgr_weighted2_neon(dst, dst_stride, dst, dst_stride,
+                                     tmp1, tmp2, w & ~7, h, wt);
+        if (w & 7) {
+            // For uneven widths, do a full 8 pixel wide filtering into a temp
+            // buffer and copy out the narrow slice of pixels separately into
+            // dest.
+            ALIGN_STK_16(pixel, stripe, 64 * 8,);
+            dav1d_sgr_weighted2_neon(stripe, w & 7, dst + (w & ~7), dst_stride,
+                                     tmp1 + (w & ~7), tmp2 + (w & ~7),
+                                     w & 7, h, wt);
+            dav1d_copy_narrow_neon(dst + (w & ~7), dst_stride, stripe,
+                                   w & 7, h);
+        }
+    }
+}
+#endif // ARCH_AARCH64
+#endif // BITDEPTH == 8
 
 void bitfn(dav1d_loop_restoration_dsp_init_arm)(Dav1dLoopRestorationDSPContext *const c) {
     const unsigned flags = dav1d_get_cpu_flags();
@@ -100,5 +265,8 @@
 
 #if BITDEPTH == 8
     c->wiener = wiener_filter_neon;
+#if ARCH_AARCH64
+    c->selfguided = sgr_filter_neon;
+#endif
 #endif
 }