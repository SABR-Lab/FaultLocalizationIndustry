# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: js/src/vm/JSAtom.cpp
# Commit: e85578d93db4
# Full Hash: e85578d93db4c1060065cfbeffe849d414fcf9d4
# Author: Jon Coppeard <jcoppeard@mozilla.com>
# Date: 2019-05-01 15:57:23
# Regressor Bug: 1547565
# File Overlap Count: 1
# Description:
#   Bug 1547565 - Remove AtomStateEntry and use ReadBarriered<JSAtom*> instead in the atoms table r=jandem
#   
#   Differential Revision: https://phabricator.services.mozilla.com/D29179
# ==============================================================================

diff -r 20ca9c77f630 -r e85578d93db4 js/src/vm/JSAtom.cpp
--- a/js/src/vm/JSAtom.cpp	Mon Apr 29 16:06:09 2019 +0100
+++ b/js/src/vm/JSAtom.cpp	Mon Apr 29 11:09:33 2019 +0100
@@ -120,9 +120,9 @@
 
 inline HashNumber js::AtomHasher::hash(const Lookup& l) { return l.hash; }
 
-MOZ_ALWAYS_INLINE bool js::AtomHasher::match(const AtomStateEntry& entry,
+MOZ_ALWAYS_INLINE bool js::AtomHasher::match(const ReadBarriered<JSAtom*>& entry,
                                              const Lookup& lookup) {
-  JSAtom* key = entry.asPtrUnbarriered();
+  JSAtom* key = entry.unbarrieredGet();
   if (lookup.atom) {
     return lookup.atom == key;
   }
@@ -181,14 +181,6 @@
   return false;
 }
 
-inline JSAtom* js::AtomStateEntry::asPtr(JSContext* cx) const {
-  JSAtom* atom = asPtrUnbarriered();
-  if (!cx->helperThread()) {
-    JSString::readBarrier(atom);
-  }
-  return atom;
-}
-
 UniqueChars js::AtomToPrintableString(JSContext* cx, JSAtom* atom) {
   return QuoteString(cx, atom);
 }
@@ -405,12 +397,10 @@
 
 inline void AtomsTable::tracePinnedAtomsInSet(JSTracer* trc, AtomSet& atoms) {
   for (auto r = atoms.all(); !r.empty(); r.popFront()) {
-    const AtomStateEntry& entry = r.front();
-    MOZ_ASSERT(entry.isPinned() == entry.asPtrUnbarriered()->isPinned());
-    if (entry.isPinned()) {
-      JSAtom* atom = entry.asPtrUnbarriered();
+    JSAtom* atom = r.front().unbarrieredGet();
+    if (atom->isPinned()) {
       TraceRoot(trc, &atom, "interned_atom");
-      MOZ_ASSERT(entry.asPtrUnbarriered() == atom);
+      MOZ_ASSERT(r.front().unbarrieredGet() == atom);
     }
   }
 }
@@ -435,8 +425,7 @@
 
 static void TracePermanentAtoms(JSTracer* trc, AtomSet::Range atoms) {
   for (; !atoms.empty(); atoms.popFront()) {
-    const AtomStateEntry& entry = atoms.front();
-    JSAtom* atom = entry.asPtrUnbarriered();
+    JSAtom* atom = atoms.front().unbarrieredGet();
     MOZ_ASSERT(atom->isPinned());
     TraceProcessGlobalRoot(trc, atom, "permanent atom");
   }
@@ -481,7 +470,7 @@
     AutoLock lock(rt, partitions[i]->lock);
     AtomSet& atoms = partitions[i]->atoms;
     for (AtomSet::Enum e(atoms); !e.empty(); e.popFront()) {
-      JSAtom* atom = e.front().asPtrUnbarriered();
+      JSAtom* atom = e.front().unbarrieredGet();
       if (IsAboutToBeFinalizedUnbarriered(&atom)) {
         e.removeFront();
       }
@@ -524,7 +513,7 @@
 
 inline JSAtom* AtomsTable::SweepIterator::front() const {
   MOZ_ASSERT(!empty());
-  return atomsIter->front().asPtrUnbarriered();
+  return atomsIter->front().unbarrieredGet();
 }
 
 inline void AtomsTable::SweepIterator::removeFront() {
@@ -576,8 +565,8 @@
   part.atomsAddedWhileSweeping = nullptr;
 
   for (auto r = newAtoms->all(); !r.empty(); r.popFront()) {
-    if (!part.atoms.putNew(AtomHasher::Lookup(r.front().asPtrUnbarriered()),
-                           r.front())) {
+    if (!part.atoms.putNew(AtomHasher::Lookup(r.front().unbarrieredGet()),
+                           r.front().unbarrieredGet())) {
       oomUnsafe.crash("Adding atom from secondary table after sweep");
     }
   }
@@ -678,7 +667,7 @@
       // The cache is purged on GC so if we're in the middle of an
       // incremental GC we should have barriered the atom when we put
       // it in the cache.
-      JSAtom* atom = zonePtr.ref()->asPtrUnbarriered();
+      JSAtom* atom = zonePtr.ref()->unbarrieredGet();
       MOZ_ASSERT(AtomIsMarked(zone, atom));
       return atom;
     }
@@ -694,9 +683,9 @@
 
   AtomSet::Ptr pp = cx->permanentAtoms().readonlyThreadsafeLookup(lookup);
   if (pp) {
-    JSAtom* atom = pp->asPtr(cx);
-    if (zonePtr && MOZ_UNLIKELY(!zone->atomCache().add(
-                       *zonePtr, AtomStateEntry(atom, false)))) {
+    // Permanent atoms don't need a read barrier.
+    JSAtom* atom = pp->unbarrieredGet();
+    if (zonePtr && MOZ_UNLIKELY(!zone->atomCache().add(*zonePtr, atom))) {
       ReportOutOfMemory(cx);
       return nullptr;
     }
@@ -721,8 +710,7 @@
     return nullptr;
   }
 
-  if (zonePtr && MOZ_UNLIKELY(!zone->atomCache().add(
-                     *zonePtr, AtomStateEntry(atom, false)))) {
+  if (zonePtr && MOZ_UNLIKELY(!zone->atomCache().add(*zonePtr, atom))) {
     ReportOutOfMemory(cx);
     return nullptr;
   }
@@ -766,7 +754,7 @@
     // is dead.
     if (!p) {
       if (AtomSet::AddPtr p2 = atoms.lookupForAdd(lookup)) {
-        JSAtom* atom = p2->asPtrUnbarriered();
+        JSAtom* atom = p2->unbarrieredGet();
         if (!IsAboutToBeFinalizedUnbarriered(&atom)) {
           p = p2;
         }
@@ -775,10 +763,18 @@
   }
 
   if (p) {
-    JSAtom* atom = p->asPtr(cx);
+    // A read barrier is required for main thread atom lookup. We don't allow
+    // off-thread parsing while collecting the atoms zone so this is not
+    // necessary for helper threads.
+    JSAtom* atom;
+    if (!cx->helperThread()) {
+      atom = *p;
+    } else {
+      atom = p->unbarrieredGet();
+    }
+
     if (pin && !atom->isPinned()) {
       atom->setPinned();
-      p->setPinned(true);
     }
     return atom;
   }
@@ -793,7 +789,7 @@
   // p is still valid.
   AtomSet* addSet =
       part.atomsAddedWhileSweeping ? part.atomsAddedWhileSweeping : &atoms;
-  if (MOZ_UNLIKELY(!addSet->add(p, AtomStateEntry(atom, bool(pin))))) {
+  if (MOZ_UNLIKELY(!addSet->add(p, atom))) {
     ReportOutOfMemory(cx); /* SystemAllocPolicy does not report OOM. */
     return nullptr;
   }
@@ -826,7 +822,8 @@
   AtomSet& atoms = *rt->permanentAtomsDuringInit();
   AtomSet::AddPtr p = atoms.lookupForAdd(lookup);
   if (p) {
-    return p->asPtr(cx);
+    // No read barrier is required for permanent atoms.
+    return p->unbarrieredGet();
   }
 
   JSAtom* atom =
@@ -840,13 +837,12 @@
   // We are single threaded at this point, and the operations we've done since
   // then can't GC; therefore the atoms table has not been modified and p is
   // still valid.
-  if (!atoms.add(p, AtomStateEntry(atom, true))) {
+  if (!atoms.add(p, atom)) {
     ReportOutOfMemory(cx); /* SystemAllocPolicy does not report OOM. */
     return nullptr;
   }
 
-  if (zonePtr && MOZ_UNLIKELY(!cx->zone()->atomCache().add(
-                     *zonePtr, AtomStateEntry(atom, false)))) {
+  if (zonePtr && MOZ_UNLIKELY(!cx->zone()->atomCache().add(*zonePtr, atom))) {
     ReportOutOfMemory(cx);
     return nullptr;
   }
@@ -999,10 +995,9 @@
   }
 
   MOZ_ASSERT(p);  // Unpinned atoms must exist in atoms table.
-  MOZ_ASSERT(p->asPtrUnbarriered() == atom);
+  MOZ_ASSERT(p->unbarrieredGet() == atom);
 
   atom->setPinned();
-  p->setPinned(true);
 }
 
 JSAtom* js::Atomize(JSContext* cx, const char* bytes, size_t length,
