# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: tools/profiler/core/ProfileBuffer.cpp
# Commit: 524f84e0fc78
# Full Hash: 524f84e0fc78eb8689dc6782f16f5bf71e4e5edf
# Author: Gerald Squelart <gsquelart@mozilla.com>
# Date: 2019-09-18 09:56:40
# Regressor Bug: 1576551
# File Overlap Count: 2
# Description:
#   Bug 1576551 - Use BlocksRingBuffer in ProfileBuffer - r=gregtatum
#   
#   This just replaces `ProfileBuffer`'s self-managed circular buffer with a
#   `BlocksRingBuffer`.
#   
# ==============================================================================

diff -r af093f06fb74 -r 524f84e0fc78 tools/profiler/core/ProfileBuffer.cpp
--- a/tools/profiler/core/ProfileBuffer.cpp	Wed Sep 18 01:19:10 2019 +0000
+++ b/tools/profiler/core/ProfileBuffer.cpp	Wed Sep 18 01:19:12 2019 +0000
@@ -9,6 +9,7 @@
 #include "ProfilerMarker.h"
 
 #include "BaseProfiler.h"
+#include "js/GCAPI.h"
 #include "jsfriendapi.h"
 #include "mozilla/MathAlgorithms.h"
 #include "nsJSPrincipals.h"
@@ -16,11 +17,14 @@
 
 using namespace mozilla;
 
+// 65536 bytes should be plenty for a single backtrace.
+static constexpr auto DuplicationBufferBytes = MakePowerOfTwo32<65536>();
+
+// mEntries doesn't need its own mutex, because it is guarded by gPSMutex.
 ProfileBuffer::ProfileBuffer(PowerOfTwo32 aCapacity)
-    : mEntries(MakeUnique<ProfileBufferEntry[]>(aCapacity.Value())),
-      mEntryIndexMask(aCapacity.Mask()),
-      mRangeStart(0),
-      mRangeEnd(0) {}
+    : mEntries(BlocksRingBuffer::ThreadSafety::WithoutMutex, aCapacity),
+      mDuplicationBuffer(MakeUnique<BlocksRingBuffer::Byte[]>(
+          DuplicationBufferBytes.Value())) {}
 
 ProfileBuffer::~ProfileBuffer() {
   while (mStoredMarkers.peek()) {
@@ -28,26 +32,46 @@
   }
 }
 
-// Called from signal, call only reentrant functions
-void ProfileBuffer::AddEntry(const ProfileBufferEntry& aEntry) {
-  GetEntry(mRangeEnd++) = aEntry;
+/* static */
+BlocksRingBuffer::BlockIndex ProfileBuffer::AddEntry(
+    BlocksRingBuffer& aBlocksRingBuffer, const ProfileBufferEntry& aEntry) {
+  switch (aEntry.GetKind()) {
+#define SWITCH_KIND(KIND, TYPE, SIZE)                                        \
+  case ProfileBufferEntry::Kind::KIND: {                                     \
+    /* Rooting analysis cannot get through `BlocksRingBuffer`'s heavy use of \
+     * lambdas and `std::function`s, which then trips it when used from      \
+     * `MergeStacks()` where unrooted js objects are manipulated. */         \
+    JS::AutoSuppressGCAnalysis nogc;                                         \
+    return aBlocksRingBuffer.PutFrom(&aEntry, 1 + (SIZE));                   \
+  }
 
-  // The distance between mRangeStart and mRangeEnd must never exceed
-  // capacity, so advance mRangeStart if necessary.
-  if (mRangeEnd - mRangeStart > mEntryIndexMask.MaskValue() + 1) {
-    mRangeStart++;
+    FOR_EACH_PROFILE_BUFFER_ENTRY_KIND(SWITCH_KIND)
+
+#undef SWITCH_KIND
+    default:
+      MOZ_ASSERT(false, "Unhandled ProfilerBuffer entry KIND");
+      return BlockIndex{};
   }
 }
 
-uint64_t ProfileBuffer::AddThreadIdEntry(int aThreadId) {
-  uint64_t pos = mRangeEnd;
-  AddEntry(ProfileBufferEntry::ThreadId(aThreadId));
-  return pos;
+// Called from signal, call only reentrant functions
+uint64_t ProfileBuffer::AddEntry(const ProfileBufferEntry& aEntry) {
+  return AddEntry(mEntries, aEntry).ConvertToU64();
 }
 
-void ProfileBuffer::AddStoredMarker(ProfilerMarker* aStoredMarker) {
-  aStoredMarker->SetPositionInBuffer(mRangeEnd);
-  mStoredMarkers.insert(aStoredMarker);
+/* static */
+BlocksRingBuffer::BlockIndex ProfileBuffer::AddThreadIdEntry(
+    BlocksRingBuffer& aBlocksRingBuffer, int aThreadId) {
+  return AddEntry(aBlocksRingBuffer, ProfileBufferEntry::ThreadId(aThreadId));
+}
+
+uint64_t ProfileBuffer::AddThreadIdEntry(int aThreadId) {
+  return AddThreadIdEntry(mEntries, aThreadId).ConvertToU64();
+}
+
+void ProfileBuffer::AddMarker(ProfilerMarker* aMarker) {
+  aMarker->SetPositionInBuffer(AddEntry(ProfileBufferEntry::Marker(aMarker)));
+  mStoredMarkers.insert(aMarker);
 }
 
 void ProfileBuffer::CollectCodeLocation(
@@ -93,22 +117,21 @@
   // Delete markers of samples that have been overwritten due to circular
   // buffer wraparound.
   while (mStoredMarkers.peek() &&
-         mStoredMarkers.peek()->HasExpired(mRangeStart)) {
+         mStoredMarkers.peek()->HasExpired(BufferRangeStart())) {
     delete mStoredMarkers.popHead();
   }
 }
 
-size_t ProfileBuffer::SizeOfIncludingThis(
-    mozilla::MallocSizeOf aMallocSizeOf) const {
-  size_t n = aMallocSizeOf(this);
-  n += aMallocSizeOf(mEntries.get());
-
+size_t ProfileBuffer::SizeOfExcludingThis(MallocSizeOf aMallocSizeOf) const {
   // Measurement of the following members may be added later if DMD finds it
   // is worthwhile:
   // - memory pointed to by the elements within mEntries
   // - mStoredMarkers
+  return mEntries.SizeOfExcludingThis(aMallocSizeOf);
+}
 
-  return n;
+size_t ProfileBuffer::SizeOfIncludingThis(MallocSizeOf aMallocSizeOf) const {
+  return aMallocSizeOf(this) + SizeOfExcludingThis(aMallocSizeOf);
 }
 
 void ProfileBuffer::CollectOverheadStats(TimeDuration aSamplingTime,
@@ -146,9 +169,10 @@
 }
 
 ProfilerBufferInfo ProfileBuffer::GetProfilerBufferInfo() const {
-  return {mRangeStart,  mRangeEnd,    mEntryIndexMask.MaskValue() + 1,
-          mIntervalsNs, mOverheadsNs, mLockingsNs,
-          mCleaningsNs, mCountersNs,  mThreadsNs};
+  return {
+      BufferRangeStart(), BufferRangeEnd(), mEntries.BufferLength()->Value(),
+      mIntervalsNs,       mOverheadsNs,     mLockingsNs,
+      mCleaningsNs,       mCountersNs,      mThreadsNs};
 }
 
 /* ProfileBufferCollector */