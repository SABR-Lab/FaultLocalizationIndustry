# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: js/src/wasm/WasmCode.h
# Commit: d6506b5ee07b
# Full Hash: d6506b5ee07b5c65f889ce7c2214b0172f205898
# Author: Dmitry Bezhetskov <dbezhetskov@igalia.com>
# Date: 2021-09-16 09:43:03
# Regressor Bug: 1639153
# File Overlap Count: 1
# Description:
#   Bug 1639153 - Introduce indirect stubs to optimize call_indirect. r=lth
#   
#   This patch introduces indirect stubs.
#   An indirect stub is a stub that takes care of any switching activities needed for call_indirect.
#   Before this patch, we have to conservatively assume that any call_indirect's target can be from a foreign instance,
# ==============================================================================

diff -r 80a164c1054e -r d6506b5ee07b js/src/wasm/WasmCode.h
--- a/js/src/wasm/WasmCode.h	Wed Sep 15 11:02:57 2021 +0000
+++ b/js/src/wasm/WasmCode.h	Wed Sep 15 11:10:15 2021 +0000
@@ -72,6 +72,21 @@
 
 namespace wasm {
 
+struct IndirectStubTarget {
+  uint32_t functionIdx;
+  void* checkedCallEntryAddress;
+  TlsData* tls;
+
+  bool operator==(const IndirectStubTarget& other) const {
+    return functionIdx == other.functionIdx &&
+           checkedCallEntryAddress == other.checkedCallEntryAddress &&
+           tls == other.tls;
+  }
+};
+
+using VectorOfIndirectStubTarget =
+    Vector<IndirectStubTarget, 8, SystemAllocPolicy>;
+
 struct MetadataTier;
 struct Metadata;
 
@@ -489,7 +504,7 @@
 using UniqueMetadataTier = UniquePtr<MetadataTier>;
 
 // LazyStubSegment is a code segment lazily generated for function entry stubs
-// (both interpreter and jit ones).
+// (both interpreter and jit ones) and for indirect stubs.
 //
 // Because a stub is usually small (a few KiB) and an executable code segment
 // isn't (64KiB), a given stub segment can contain entry stubs of many
@@ -521,6 +536,11 @@
                 const CodeRangeVector& codeRanges, uint8_t** codePtr,
                 size_t* indexFirstInsertedCodeRange);
 
+  bool addIndirectStubs(size_t codeLength,
+                        const VectorOfIndirectStubTarget& targets,
+                        const CodeRangeVector& codeRanges, uint8_t** codePtr,
+                        size_t* indexFirstInsertedCodeRange);
+
   const CodeRangeVector& codeRanges() const { return codeRanges_; }
   const CodeRange* lookupRange(const void* pc) const;
 
@@ -545,8 +565,27 @@
 
 using LazyFuncExportVector = Vector<LazyFuncExport, 0, SystemAllocPolicy>;
 
+// IndirectStub provides a mapping between function indices and
+// indirect stubs code ranges.
+
+struct IndirectStub {
+  size_t funcIndex;
+  size_t segmentIndex;
+  size_t codeRangeIndex;
+  void* tls;
+  IndirectStub(size_t funcIndex, size_t segmentIndex, size_t codeRangeIndex,
+               TlsData* tls)
+      : funcIndex(funcIndex),
+        segmentIndex(segmentIndex),
+        codeRangeIndex(codeRangeIndex),
+        tls(tls) {}
+};
+
+using IndirectStubVector = Vector<IndirectStub, 0, SystemAllocPolicy>;
+
 // LazyStubTier contains all the necessary information for lazy function entry
-// stubs that are generated at runtime. None of its data is ever serialized.
+// stubs and indirect stubs that are generated at runtime.
+// None of its data are ever serialized.
 //
 // It must be protected by a lock, because the main thread can both read and
 // write lazy stubs at any time while a background thread can regenerate lazy
@@ -555,25 +594,36 @@
 class LazyStubTier {
   LazyStubSegmentVector stubSegments_;
   LazyFuncExportVector exports_;
+  IndirectStubVector indirectStubVector_;
   size_t lastStubSegmentIndex_;
 
-  bool createMany(const Uint32Vector& funcExportIndices,
-                  const CodeTier& codeTier, bool flushAllThreadsIcaches,
-                  size_t* stubSegmentIndex);
+  bool createManyEntryStubs(const Uint32Vector& funcExportIndices,
+                            const CodeTier& codeTier,
+                            bool flushAllThreadsIcaches,
+                            size_t* stubSegmentIndex);
 
  public:
   LazyStubTier() : lastStubSegmentIndex_(0) {}
 
-  bool empty() const { return stubSegments_.empty(); }
-  bool hasStub(uint32_t funcIndex) const;
+  // Creates one lazy stub for the exported function, for which the jit entry
+  // will be set to the lazily-generated one.
+  bool createOneEntryStub(uint32_t funcExportIndex, const CodeTier& codeTier);
 
-  // Returns a pointer to the raw interpreter entry of a given function which
-  // stubs have been lazily generated.
+  bool entryStubsEmpty() const { return stubSegments_.empty(); }
+  bool hasEntryStub(uint32_t funcIndex) const;
+
+  // Returns a pointer to the raw interpreter entry of a given function for
+  // which stubs have been lazily generated.
   void* lookupInterpEntry(uint32_t funcIndex) const;
 
-  // Creates one lazy stub for the exported function, for which the jit entry
-  // will be set to the lazily-generated one.
-  bool createOne(uint32_t funcExportIndex, const CodeTier& codeTier);
+  // Creates many indirect stubs.
+  bool createManyIndirectStubs(const VectorOfIndirectStubTarget& targets,
+                               const CodeTier& codeTier);
+
+  // Returns a pointer to the indirect stub of a given function.
+  void* lookupIndirectStub(uint32_t funcIndex, void* tls) const;
+
+  const CodeRange* lookupRange(const void* pc) const;
 
   // Create one lazy stub for all the functions in funcExportIndices, putting
   // them in a single stub. Jit entries won't be used until
@@ -795,6 +845,7 @@
 
   const CallSite* lookupCallSite(void* returnAddress) const;
   const CodeRange* lookupFuncRange(void* pc) const;
+  const CodeRange* lookupIndirectStubRange(void* pc) const;
   const StackMap* lookupStackMap(uint8_t* nextPC) const;
 #ifdef ENABLE_WASM_EXCEPTIONS
   const WasmTryNote* lookupWasmTryNote(void* pc, Tier* tier) const;