# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: js/src/wasm/WasmBCMemory.cpp
# Commit: 1ab8383d17ee
# Full Hash: 1ab8383d17ee1e5db164339236d64bcb7168b225
# Author: Lars T Hansen <lhansen@mozilla.com>
# Date: 2021-10-18 16:09:29
# Regressor Bug: 1727084
# File Overlap Count: 1
# Description:
#   Bug 1727084 - Memory64 - Atomic-rmw baseline operations. r=yury
#   
#   Atomic ADD, SUB, AND, OR, XOR, XCHG, CMPXCHG, as well as WAIT and
#   NOTIFY for the baseline compiler.
#   
# ==============================================================================

diff -r 6089b4360edc -r 1ab8383d17ee js/src/wasm/WasmBCMemory.cpp
--- a/js/src/wasm/WasmBCMemory.cpp	Mon Oct 18 07:31:55 2021 +0000
+++ b/js/src/wasm/WasmBCMemory.cpp	Mon Oct 18 07:31:56 2021 +0000
@@ -209,12 +209,6 @@
   return pop<RegType>();
 }
 
-// This is temporary, it will disappear once the atomics are all done.
-RegI32 BaseCompiler::popMemory32Access(MemoryAccessDesc* access,
-                                       AccessCheck* check) {
-  return popMemoryAccess<RegI32>(access, check);
-}
-
 #ifdef JS_64BIT
 static inline RegI64 RegPtrToRegIntptr(RegPtr r) {
   return RegI64(Register64(Register(r)));
@@ -821,6 +815,16 @@
   }
 }
 
+// Convert something that may contain a heap index into a Register that can be
+// used in an access.
+
+static inline Register ToRegister(RegI32 r) { return Register(r); }
+#ifdef JS_PUNBOX64
+static inline Register ToRegister(RegI64 r) { return r.reg; }
+#else
+static inline Register ToRegister(RegI64 r) { return r.low; }
+#endif
+
 //////////////////////////////////////////////////////////////////////////////
 //
 // Atomic operations.
@@ -835,31 +839,37 @@
 
 #ifdef RABALDR_HAS_HEAPREG
 
+// RegIndexType is RegI32 for Memory32 and RegI64 for Memory64.
+template <typename RegIndexType>
 BaseIndex BaseCompiler::prepareAtomicMemoryAccess(MemoryAccessDesc* access,
                                                   AccessCheck* check,
-                                                  RegPtr tls, RegI32 ptr) {
+                                                  RegPtr tls,
+                                                  RegIndexType ptr) {
   // 64-bit offset will be supported later.
   static_assert(sizeof(access->offset()) == sizeof(uint32_t));
 
   MOZ_ASSERT(needTlsForAccess(*check) == tls.isValid());
   prepareMemoryAccess(access, check, tls, ptr);
-  return BaseIndex(HeapReg, ptr, TimesOne, access->offset());
+  return BaseIndex(HeapReg, ToRegister(ptr), TimesOne, access->offset());
 }
 
 #else
 
 // Some consumers depend on the returned Address not incorporating tls, as tls
 // may be the scratch register.
+//
+// RegIndexType is RegI32 for Memory32 and RegI64 for Memory64.
+template <typename RegIndexType>
 Address BaseCompiler::prepareAtomicMemoryAccess(MemoryAccessDesc* access,
                                                 AccessCheck* check, RegPtr tls,
-                                                RegI32 ptr) {
+                                                RegIndexType ptr) {
   // 64-bit offset will be supported later.
   static_assert(sizeof(access->offset()) == sizeof(uint32_t));
 
   MOZ_ASSERT(needTlsForAccess(*check) == tls.isValid());
   prepareMemoryAccess(access, check, tls, ptr);
-  masm.addPtr(Address(tls, offsetof(TlsData, memoryBase)), ptr);
-  return Address(ptr, access->offset());
+  masm.addPtr(Address(tls, offsetof(TlsData, memoryBase)), ToRegister(ptr));
+  return Address(ToRegister(ptr), access->offset());
 }
 
 #endif
@@ -912,22 +922,14 @@
 
 }  // namespace atomic_load64
 
-void BaseCompiler::atomicLoad(MemoryAccessDesc* access, ValType type) {
-  Scalar::Type viewType = access->type();
-  if (Scalar::byteSize(viewType) <= sizeof(void*)) {
-    loadCommon(access, AccessCheck(), type);
-    return;
-  }
-
-  MOZ_ASSERT(isMem32());
-  MOZ_ASSERT(type == ValType::I64 && Scalar::byteSize(viewType) == 8);
-
 #if !defined(JS_64BIT)
+template <typename RegIndexType>
+void BaseCompiler::atomicLoad64(MemoryAccessDesc* access) {
   RegI64 rd, temp;
   atomic_load64::Allocate(this, &rd, &temp);
 
   AccessCheck check;
-  RegI32 rp = popMemoryAccess<RegI32>(access, &check);
+  RegIndexType rp = popMemoryAccess<RegIndexType>(access, &check);
 
 #  ifdef RABALDR_HAS_HEAPREG
   RegPtr tls = maybeLoadTlsForAccess(check);
@@ -942,10 +944,30 @@
   MOZ_ASSERT(tls == scratch);
 #  endif
 
-  freeI32(rp);
+  free(rp);
   atomic_load64::Deallocate(this, temp);
   pushI64(rd);
-#endif  // JS_64BIT
+}
+#endif
+
+void BaseCompiler::atomicLoad(MemoryAccessDesc* access, ValType type) {
+  Scalar::Type viewType = access->type();
+  if (Scalar::byteSize(viewType) <= sizeof(void*)) {
+    loadCommon(access, AccessCheck(), type);
+    return;
+  }
+
+  MOZ_ASSERT(type == ValType::I64 && Scalar::byteSize(viewType) == 8);
+
+#if !defined(JS_64BIT)
+  if (isMem32()) {
+    atomicLoad64<RegI32>(access);
+  } else {
+    atomicLoad64<RegI64>(access);
+  }
+#else
+  MOZ_CRASH("Should not happen");
+#endif
 }
 
 void BaseCompiler::atomicStore(MemoryAccessDesc* access, ValType type) {
@@ -956,13 +978,16 @@
     return;
   }
 
-  MOZ_ASSERT(isMem32());
   MOZ_ASSERT(type == ValType::I64 && Scalar::byteSize(viewType) == 8);
 
-#ifdef JS_64BIT
+#if !defined(JS_64BIT)
+  if (isMem32()) {
+    atomicXchg64<RegI32>(access, WantResult(false));
+  } else {
+    atomicXchg64<RegI64>(access, WantResult(false));
+  }
+#else
   MOZ_CRASH("Should not happen");
-#else
-  atomicXchg64(access, WantResult(false));
 #endif
 }
 
@@ -972,13 +997,20 @@
 
 void BaseCompiler::atomicRMW(MemoryAccessDesc* access, ValType type,
                              AtomicOp op) {
-  MOZ_ASSERT(isMem32());
   Scalar::Type viewType = access->type();
   if (Scalar::byteSize(viewType) <= 4) {
-    atomicRMW32(access, type, op);
+    if (isMem32()) {
+      atomicRMW32<RegI32>(access, type, op);
+    } else {
+      atomicRMW32<RegI64>(access, type, op);
+    }
   } else {
     MOZ_ASSERT(type == ValType::I64 && Scalar::byteSize(viewType) == 8);
-    atomicRMW64(access, type, op);
+    if (isMem32()) {
+      atomicRMW64<RegI32>(access, type, op);
+    } else {
+      atomicRMW64<RegI64>(access, type, op);
+    }
   }
 }
 
@@ -987,7 +1019,11 @@
 #if defined(JS_CODEGEN_X64) || defined(JS_CODEGEN_X86)
 
 struct Temps {
+  // On x86 we use the ScratchI32 for the temp, otherwise we'd run out of
+  // registers for 64-bit operations.
+#  if defined(JS_CODEGEN_X64)
   RegI32 t0;
+#  endif
 };
 
 static void PopAndAllocate(BaseCompiler* bc, ValType type,
@@ -1014,13 +1050,7 @@
       *rv = bc->popI32();
     }
     *rd = bc->specific_.eax;
-#  if defined(JS_CODEGEN_X86)
-    // Single-byte is a special case handled very locally with ScratchI8, see
-    // AtomicRMW32 below.
-    if (Scalar::byteSize(viewType) > 1) {
-      temps->t0 = bc->needI32();
-    }
-#  else
+#  ifdef JS_CODEGEN_X64
     temps->t0 = bc->needI32();
 #  endif
   }
@@ -1029,15 +1059,13 @@
 template <typename T>
 static void Perform(BaseCompiler* bc, const MemoryAccessDesc& access, T srcAddr,
                     AtomicOp op, RegI32 rv, RegI32 rd, const Temps& temps) {
+#  ifdef JS_CODEGEN_X64
   RegI32 temp = temps.t0;
-#  ifdef JS_CODEGEN_X86
-  ScratchI8 scratch(*bc);
-  if (access.type() == Scalar::Uint8) {
-    // The temp, if used, must be a byte register.
-    MOZ_ASSERT(temp.isInvalid());
-    if (op != AtomicFetchAddOp && op != AtomicFetchSubOp) {
-      temp = scratch;
-    }
+#  else
+  RegI32 temp;
+  ScratchI32 scratch(*bc);
+  if (op != AtomicFetchAddOp && op != AtomicFetchSubOp) {
+    temp = scratch;
   }
 #  endif
   bc->masm.wasmAtomicFetchOp(access, op, rv, srcAddr, temp, rd);
@@ -1047,7 +1075,9 @@
   if (rv != bc->specific_.eax) {
     bc->freeI32(rv);
   }
+#  ifdef JS_CODEGEN_X64
   bc->maybeFree(temps.t0);
+#  endif
 }
 
 #elif defined(JS_CODEGEN_ARM) || defined(JS_CODEGEN_ARM64)
@@ -1123,6 +1153,7 @@
 
 }  // namespace atomic_rmw32
 
+template <typename RegIndexType>
 void BaseCompiler::atomicRMW32(MemoryAccessDesc* access, ValType type,
                                AtomicOp op) {
   Scalar::Type viewType = access->type();
@@ -1131,7 +1162,7 @@
   atomic_rmw32::PopAndAllocate(this, type, viewType, op, &rd, &rv, &temps);
 
   AccessCheck check;
-  RegI32 rp = popMemory32Access(access, &check);
+  RegIndexType rp = popMemoryAccess<RegIndexType>(access, &check);
   RegPtr tls = maybeLoadTlsForAccess(check);
 
   auto memaddr = prepareAtomicMemoryAccess(access, &check, tls, rp);
@@ -1139,7 +1170,7 @@
 
   maybeFree(tls);
   atomic_rmw32::Deallocate(this, rv, temps);
-  freeI32(rp);
+  free(rp);
 
   if (type == ValType::I64) {
     pushU32AsI64(rd);
@@ -1182,22 +1213,26 @@
 
 #elif defined(JS_CODEGEN_X86)
 
-// Here we'll use cmpxchg8b, so we need rd=edx:eax and value=ecx:ebx.  However,
-// ebx is in use also for the address, so we first pop the value to ecx:tmp,
-// whence it will be stored in memory before the operation, and ecx:ebx is then
-// free.  The temp variable goes unused here.
+// Register allocation is tricky, see comments at atomic_xchg64 below.
+//
+// - Initially rv=ecx:edx and eax is reserved, rd=unallocated.
+// - Then rp is popped into esi+edi because those are the only available.
+// - The Setup operation makes rd=edx:eax.
+// - Deallocation then frees only the ecx part of rv.
+//
+// The temp is unused here.
 
 static void PopAndAllocate(BaseCompiler* bc, AtomicOp op, RegI64* rd,
                            RegI64* rv, RegI64*) {
-  bc->needI64(bc->specific_.edx_eax);
-  *rd = bc->specific_.edx_eax;
-
+  bc->needI32(bc->specific_.eax);
   bc->needI32(bc->specific_.ecx);
-  RegI32 rvLow = bc->needI32();
-  *rv = RegI64(Register64(bc->specific_.ecx, rvLow));
+  bc->needI32(bc->specific_.edx);
+  *rv = RegI64(Register64(bc->specific_.ecx, bc->specific_.edx));
   bc->popI64ToSpecific(*rv);
 }
 
+static void Setup(BaseCompiler* bc, RegI64* rd) { *rd = bc->specific_.edx_eax; }
+
 static void Perform(BaseCompiler* bc, const MemoryAccessDesc& access,
                     Address srcAddr, AtomicOp op, RegI64 rv, RegI64, RegI64 rd,
                     const ScratchAtomicNoHeapReg& scratch) {
@@ -1214,8 +1249,8 @@
   bc->fr.popBytes(8);
 }
 
-static void Deallocate(BaseCompiler* bc, AtomicOp op, RegI64 rv, RegI64) {
-  bc->freeI64(rv);
+static void Deallocate(BaseCompiler* bc, AtomicOp, RegI64, RegI64) {
+  bc->freeI32(bc->specific_.ecx);
 }
 
 #elif defined(JS_CODEGEN_ARM)
@@ -1274,13 +1309,14 @@
 
 }  // namespace atomic_rmw64
 
+template <typename RegIndexType>
 void BaseCompiler::atomicRMW64(MemoryAccessDesc* access, ValType type,
                                AtomicOp op) {
   RegI64 rd, rv, temp;
   atomic_rmw64::PopAndAllocate(this, op, &rd, &rv, &temp);
 
   AccessCheck check;
-  RegI32 rp = popMemory32Access(access, &check);
+  RegIndexType rp = popMemoryAccess<RegIndexType>(access, &check);
 
 #if defined(RABALDR_HAS_HEAPREG)
   RegPtr tls = maybeLoadTlsForAccess(check);
@@ -1291,11 +1327,12 @@
   ScratchAtomicNoHeapReg scratch(*this);
   RegPtr tls = maybeLoadTlsForAccess(check, RegIntptrToRegPtr(scratch));
   auto memaddr = prepareAtomicMemoryAccess(access, &check, tls, rp);
+  atomic_rmw64::Setup(this, &rd);
   atomic_rmw64::Perform(this, *access, memaddr, op, rv, temp, rd, scratch);
   MOZ_ASSERT(tls == scratch);
 #endif
 
-  freeI32(rp);
+  free(rp);
   atomic_rmw64::Deallocate(this, op, rv, temp);
 
   pushI64(rd);
@@ -1306,13 +1343,20 @@
 // Atomic exchange (also used for atomic store in some cases).
 
 void BaseCompiler::atomicXchg(MemoryAccessDesc* access, ValType type) {
-  MOZ_ASSERT(isMem32());
   Scalar::Type viewType = access->type();
   if (Scalar::byteSize(viewType) <= 4) {
-    atomicXchg32(access, type);
+    if (isMem32()) {
+      atomicXchg32<RegI32>(access, type);
+    } else {
+      atomicXchg32<RegI64>(access, type);
+    }
   } else {
     MOZ_ASSERT(type == ValType::I64 && Scalar::byteSize(viewType) == 8);
-    atomicXchg64(access, WantResult(true));
+    if (isMem32()) {
+      atomicXchg64<RegI32>(access, WantResult(true));
+    } else {
+      atomicXchg64<RegI64>(access, WantResult(true));
+    }
   }
 }
 
@@ -1429,6 +1473,7 @@
 
 }  // namespace atomic_xchg32
 
+template <typename RegIndexType>
 void BaseCompiler::atomicXchg32(MemoryAccessDesc* access, ValType type) {
   Scalar::Type viewType = access->type();
 
@@ -1438,14 +1483,14 @@
 
   AccessCheck check;
 
-  RegI32 rp = popMemory32Access(access, &check);
+  RegIndexType rp = popMemoryAccess<RegIndexType>(access, &check);
   RegPtr tls = maybeLoadTlsForAccess(check);
 
   auto memaddr = prepareAtomicMemoryAccess(access, &check, tls, rp);
   atomic_xchg32::Perform(this, *access, memaddr, rv, rd, temps);
 
   maybeFree(tls);
-  freeI32(rp);
+  free(rp);
   atomic_xchg32::Deallocate(this, rv, temps);
 
   if (type == ValType::I64) {
@@ -1470,29 +1515,45 @@
 
 #elif defined(JS_CODEGEN_X86)
 
-// We'll use cmpxchg8b, so rv must be in ecx:ebx, and rd must be edx:eax.  We
-// can't use ebx for rv here because we need ebx for a scratch also, so use a
-// separate temp and move the value to ebx just before the operation.
+// Register allocation is tricky in several ways.
+//
+// - For a 64-bit access on memory64 we need six registers for rd, rv, and rp,
+//   but have only five (as the temp ebx is needed too), so we target all
+//   registers explicitly to make sure there's space.
+//
+// - We'll be using cmpxchg8b, and when we do the operation, rv must be in
+//   ecx:ebx, and rd must be edx:eax.  We can't use ebx for rv initially because
+//   we need ebx for a scratch also, so use a separate temp and move the value
+//   to ebx just before the operation.
+//
+// In sum:
+//
+// - Initially rv=ecx:edx and eax is reserved, rd=unallocated.
+// - Then rp is popped into esi+edi because those are the only available.
+// - The Setup operation makes rv=ecx:ebx and rd=edx:eax and moves edx->ebx.
+// - Deallocation then frees only the ecx part of rv.
+
 static void PopAndAllocate(BaseCompiler* bc, RegI64* rd, RegI64* rv) {
   bc->needI32(bc->specific_.ecx);
-  bc->needI64(bc->specific_.edx_eax);
-  RegI32 tmp = bc->needI32();
-  *rv = RegI64(Register64(bc->specific_.ecx, tmp));
+  bc->needI32(bc->specific_.edx);
+  bc->needI32(bc->specific_.eax);
+  *rv = RegI64(Register64(bc->specific_.ecx, bc->specific_.edx));
   bc->popI64ToSpecific(*rv);
-  *rd = bc->specific_.edx_eax;
 }
 
-static RegI64 Setup(BaseCompiler* bc, RegI64 rv,
-                    const ScratchAtomicNoHeapReg& scratch) {
-  MOZ_ASSERT(rv.high == bc->specific_.ecx);
+static void Setup(BaseCompiler* bc, RegI64* rv, RegI64* rd,
+                  const ScratchAtomicNoHeapReg& scratch) {
+  MOZ_ASSERT(rv->high == bc->specific_.ecx);
   MOZ_ASSERT(Register(scratch) == js::jit::ebx);
-  bc->masm.move32(rv.low, scratch);
-  return bc->specific_.ecx_ebx;
+  bc->masm.move32(rv->low, scratch);
+  *rv = bc->specific_.ecx_ebx;
+  *rd = bc->specific_.edx_eax;
 }
 
 static void Deallocate(BaseCompiler* bc, RegI64 rd, RegI64 rv) {
-  bc->freeI64(rv);
+  MOZ_ASSERT(rd == bc->specific_.edx_eax || rd == RegI64::Invalid());
   bc->maybeFree(rd);
+  bc->freeI32(bc->specific_.ecx);
 }
 
 #elif defined(JS_CODEGEN_ARM64) || defined(JS_CODEGEN_MIPS64)
@@ -1529,29 +1590,31 @@
 
 }  // namespace atomic_xchg64
 
+template <typename RegIndexType>
 void BaseCompiler::atomicXchg64(MemoryAccessDesc* access,
                                 WantResult wantResult) {
   RegI64 rd, rv;
   atomic_xchg64::PopAndAllocate(this, &rd, &rv);
 
   AccessCheck check;
-  RegI32 rp = popMemory32Access(access, &check);
+  RegIndexType rp = popMemoryAccess<RegIndexType>(access, &check);
 
 #ifdef RABALDR_HAS_HEAPREG
   RegPtr tls = maybeLoadTlsForAccess(check);
-  auto memaddr = prepareAtomicMemoryAccess(access, &check, tls, rp);
+  auto memaddr =
+      prepareAtomicMemoryAccess<RegIndexType>(access, &check, tls, rp);
   masm.wasmAtomicExchange64(*access, memaddr, rv, rd);
   maybeFree(tls);
 #else
   ScratchAtomicNoHeapReg scratch(*this);
   RegPtr tls = maybeLoadTlsForAccess(check, RegIntptrToRegPtr(scratch));
-  auto memaddr = prepareAtomicMemoryAccess(access, &check, tls, rp);
-  RegI64 rvNew = atomic_xchg64::Setup(this, rv, scratch);
-  masm.wasmAtomicExchange64(*access, memaddr, rvNew, rd);
+  Address memaddr = prepareAtomicMemoryAccess(access, &check, tls, rp);
+  atomic_xchg64::Setup(this, &rv, &rd, scratch);
+  masm.wasmAtomicExchange64(*access, memaddr, rv, rd);
   MOZ_ASSERT(tls == scratch);
 #endif
 
-  freeI32(rp);
+  free(rp);
   if (wantResult) {
     pushI64(rd);
     rd = RegI64::Invalid();
@@ -1564,13 +1627,20 @@
 // Atomic compare-exchange.
 
 void BaseCompiler::atomicCmpXchg(MemoryAccessDesc* access, ValType type) {
-  MOZ_ASSERT(isMem32());
   Scalar::Type viewType = access->type();
   if (Scalar::byteSize(viewType) <= 4) {
-    atomicCmpXchg32(access, type);
+    if (isMem32()) {
+      atomicCmpXchg32<RegI32>(access, type);
+    } else {
+      atomicCmpXchg32<RegI64>(access, type);
+    }
   } else {
     MOZ_ASSERT(type == ValType::I64 && Scalar::byteSize(viewType) == 8);
-    atomicCmpXchg64(access, type);
+    if (isMem32()) {
+      atomicCmpXchg64<RegI32>(access, type);
+    } else {
+      atomicCmpXchg64<RegI64>(access, type);
+    }
   }
 }
 
@@ -1701,6 +1771,7 @@
 
 }  // namespace atomic_cmpxchg32
 
+template <typename RegIndexType>
 void BaseCompiler::atomicCmpXchg32(MemoryAccessDesc* access, ValType type) {
   Scalar::Type viewType = access->type();
   RegI32 rexpect, rnew, rd;
@@ -1709,14 +1780,14 @@
                                    &temps);
 
   AccessCheck check;
-  RegI32 rp = popMemory32Access(access, &check);
+  RegIndexType rp = popMemoryAccess<RegIndexType>(access, &check);
   RegPtr tls = maybeLoadTlsForAccess(check);
 
   auto memaddr = prepareAtomicMemoryAccess(access, &check, tls, rp);
   atomic_cmpxchg32::Perform(this, *access, memaddr, rexpect, rnew, rd, temps);
 
   maybeFree(tls);
-  freeI32(rp);
+  free(rp);
   atomic_cmpxchg32::Deallocate(this, rexpect, rnew, temps);
 
   if (type == ValType::I64) {
@@ -1728,8 +1799,19 @@
 
 namespace atomic_cmpxchg64 {
 
+// The templates are needed for x86 code generation, which needs complicated
+// register allocation for memory64.
+
+template <typename RegIndexType>
+static void PopAndAllocate(BaseCompiler* bc, RegI64* rexpect, RegI64* rnew,
+                           RegI64* rd);
+
+template <typename RegIndexType>
+static void Deallocate(BaseCompiler* bc, RegI64 rexpect, RegI64 rnew);
+
 #if defined(JS_CODEGEN_X64)
 
+template <typename RegIndexType>
 static void PopAndAllocate(BaseCompiler* bc, RegI64* rexpect, RegI64* rnew,
                            RegI64* rd) {
   // For cmpxchg, the expected value and the result are both in rax.
@@ -1744,30 +1826,38 @@
   bc->masm.wasmCompareExchange64(access, srcAddr, rexpect, rnew, rd);
 }
 
+template <typename RegIndexType>
 static void Deallocate(BaseCompiler* bc, RegI64 rexpect, RegI64 rnew) {
   bc->freeI64(rnew);
 }
 
 #elif defined(JS_CODEGEN_X86)
 
-static void PopAndAllocate(BaseCompiler* bc, RegI64* rexpect, RegI64* rnew,
-                           RegI64* rd) {
-  // For cmpxchg8b, the expected value and the result are both in edx:eax, and
-  // the replacement value is in ecx:ebx.  But we can't allocate ebx here
-  // because we need it later for a scratch, so instead we allocate a temp to
-  // hold the low word of 'new'.
+template <typename RegIndexType>
+static void Perform(BaseCompiler* bc, const MemoryAccessDesc& access,
+                    Address srcAddr, RegI64 rexpect, RegI64 rnew, RegI64 rd,
+                    ScratchAtomicNoHeapReg& scratch);
+
+// Memory32: For cmpxchg8b, the expected value and the result are both in
+// edx:eax, and the replacement value is in ecx:ebx.  But we can't allocate ebx
+// initially because we need it later for a scratch, so instead we allocate a
+// temp to hold the low word of 'new'.
+
+template <>
+void PopAndAllocate<RegI32>(BaseCompiler* bc, RegI64* rexpect, RegI64* rnew,
+                            RegI64* rd) {
   bc->needI64(bc->specific_.edx_eax);
   bc->needI32(bc->specific_.ecx);
-
   RegI32 tmp = bc->needI32();
   *rnew = bc->popI64ToSpecific(RegI64(Register64(bc->specific_.ecx, tmp)));
   *rexpect = bc->popI64ToSpecific(bc->specific_.edx_eax);
   *rd = *rexpect;
 }
 
-static void Perform(BaseCompiler* bc, const MemoryAccessDesc& access,
-                    Address srcAddr, RegI64 rexpect, RegI64 rnew, RegI64 rd,
-                    ScratchAtomicNoHeapReg& scratch) {
+template <>
+void Perform<RegI32>(BaseCompiler* bc, const MemoryAccessDesc& access,
+                     Address srcAddr, RegI64 rexpect, RegI64 rnew, RegI64 rd,
+                     ScratchAtomicNoHeapReg& scratch) {
   MOZ_ASSERT(Register(scratch) == js::jit::ebx);
   MOZ_ASSERT(rnew.high == bc->specific_.ecx);
   bc->masm.move32(rnew.low, ebx);
@@ -1775,12 +1865,62 @@
                                  bc->specific_.ecx_ebx, rd);
 }
 
-static void Deallocate(BaseCompiler* bc, RegI64 rexpect, RegI64 rnew) {
+template <>
+void Deallocate<RegI32>(BaseCompiler* bc, RegI64 rexpect, RegI64 rnew) {
   bc->freeI64(rnew);
 }
 
+// Memory64: Register allocation is particularly hairy here.  With memory64, we
+// have up to seven live values: i64 expected-value, i64 new-value, i64 pointer,
+// and tls.  The tls can use the scratch but there's no avoiding that we'll run
+// out of registers.
+//
+// Unlike for the rmw ops, we can't use edx as the rnew.low since it's used
+// for the rexpect.high.  And we can't push anything onto the stack while we're
+// popping the memory address because the memory address may be on the stack.
+
+template <>
+void PopAndAllocate<RegI64>(BaseCompiler* bc, RegI64* rexpect, RegI64* rnew,
+                            RegI64* rd) {
+  // We reserve these (and ebx).  The 64-bit pointer will end up in esi+edi.
+  bc->needI32(bc->specific_.eax);
+  bc->needI32(bc->specific_.ecx);
+  bc->needI32(bc->specific_.edx);
+
+  // Pop the 'new' value and stash it in the Tls scratch area.  Do not
+  // initialize *rnew to anything.
+  RegI64 tmp(Register64(bc->specific_.ecx, bc->specific_.edx));
+  bc->popI64ToSpecific(tmp);
+  {
+    ScratchPtr tlsScratch(*bc);
+    bc->stashI64(tlsScratch, tmp);
+  }
+
+  *rexpect = bc->popI64ToSpecific(bc->specific_.edx_eax);
+  *rd = *rexpect;
+}
+
+template <>
+void Perform<RegI64>(BaseCompiler* bc, const MemoryAccessDesc& access,
+                     Address srcAddr, RegI64 rexpect, RegI64 rnew, RegI64 rd,
+                     ScratchAtomicNoHeapReg& scratch) {
+  MOZ_ASSERT(rnew.isInvalid());
+  rnew = bc->specific_.ecx_ebx;
+
+  bc->unstashI64(RegPtr(Register(bc->specific_.ecx)), rnew);
+  bc->masm.wasmCompareExchange64(access, srcAddr, rexpect, rnew, rd);
+}
+
+template <>
+void Deallocate<RegI64>(BaseCompiler* bc, RegI64 rexpect, RegI64 rnew) {
+  // edx:ebx have been pushed as the result, and the pointer was freed
+  // separately in the caller, so just free ecx.
+  bc->free(bc->specific_.ecx);
+}
+
 #elif defined(JS_CODEGEN_ARM)
 
+template <typename RegIndexType>
 static void PopAndAllocate(BaseCompiler* bc, RegI64* rexpect, RegI64* rnew,
                            RegI64* rd) {
   // The replacement value and the result must both be odd/even pairs.
@@ -1794,6 +1934,7 @@
   bc->masm.wasmCompareExchange64(access, srcAddr, rexpect, rnew, rd);
 }
 
+template <typename RegIndexType>
 static void Deallocate(BaseCompiler* bc, RegI64 rexpect, RegI64 rnew) {
   bc->freeI64(rexpect);
   bc->freeI64(rnew);
@@ -1801,6 +1942,7 @@
 
 #elif defined(JS_CODEGEN_ARM64) || defined(JS_CODEGEN_MIPS64)
 
+template <typename RegIndexType>
 static void PopAndAllocate(BaseCompiler* bc, RegI64* rexpect, RegI64* rnew,
                            RegI64* rd) {
   *rnew = bc->popI64();
@@ -1813,6 +1955,7 @@
   bc->masm.wasmCompareExchange64(access, srcAddr, rexpect, rnew, rd);
 }
 
+template <typename RegIndexType>
 static void Deallocate(BaseCompiler* bc, RegI64 rexpect, RegI64 rnew) {
   bc->freeI64(rexpect);
   bc->freeI64(rnew);
@@ -1820,23 +1963,26 @@
 
 #elif defined(JS_CODEGEN_NONE)
 
+template <typename RegIndexType>
 static void PopAndAllocate(BaseCompiler* bc, RegI64* rexpect, RegI64* rnew,
                            RegI64* rd) {}
 static void Perform(BaseCompiler* bc, const MemoryAccessDesc& access,
                     BaseIndex srcAddr, RegI64 rexpect, RegI64 rnew, RegI64 rd) {
 }
+template <typename RegIndexType>
 static void Deallocate(BaseCompiler* bc, RegI64 rexpect, RegI64 rnew) {}
 
 #endif
 
 }  // namespace atomic_cmpxchg64
 
+template <typename RegIndexType>
 void BaseCompiler::atomicCmpXchg64(MemoryAccessDesc* access, ValType type) {
   RegI64 rexpect, rnew, rd;
-  atomic_cmpxchg64::PopAndAllocate(this, &rexpect, &rnew, &rd);
+  atomic_cmpxchg64::PopAndAllocate<RegIndexType>(this, &rexpect, &rnew, &rd);
 
   AccessCheck check;
-  RegI32 rp = popMemory32Access(access, &check);
+  RegIndexType rp = popMemoryAccess<RegIndexType>(access, &check);
 
 #ifdef RABALDR_HAS_HEAPREG
   RegPtr tls = maybeLoadTlsForAccess(check);
@@ -1846,13 +1992,14 @@
 #else
   ScratchAtomicNoHeapReg scratch(*this);
   RegPtr tls = maybeLoadTlsForAccess(check, RegIntptrToRegPtr(scratch));
-  auto memaddr = prepareAtomicMemoryAccess(access, &check, tls, rp);
-  atomic_cmpxchg64::Perform(this, *access, memaddr, rexpect, rnew, rd, scratch);
+  Address memaddr = prepareAtomicMemoryAccess(access, &check, tls, rp);
+  atomic_cmpxchg64::Perform<RegIndexType>(this, *access, memaddr, rexpect, rnew,
+                                          rd, scratch);
   MOZ_ASSERT(tls == scratch);
 #endif
 
-  freeI32(rp);
-  atomic_cmpxchg64::Deallocate(this, rexpect, rnew);
+  free(rp);
+  atomic_cmpxchg64::Deallocate<RegIndexType>(this, rexpect, rnew);
 
   pushI64(rd);
 }
@@ -1868,13 +2015,17 @@
       RegI64 timeout = popI64();
       RegI32 val = popI32();
 
-      MOZ_ASSERT(isMem32());
-      computeEffectiveAddress<RegI32>(access);
+      if (isMem32()) {
+        computeEffectiveAddress<RegI32>(access);
+      } else {
+        computeEffectiveAddress<RegI64>(access);
+      }
 
       pushI32(val);
       pushI64(timeout);
 
-      if (!emitInstanceCall(lineOrBytecode, SASigWaitI32)) {
+      if (!emitInstanceCall(lineOrBytecode,
+                            isMem32() ? SASigWaitI32M32 : SASigWaitI32M64)) {
         return false;
       }
       break;
@@ -1883,13 +2034,31 @@
       RegI64 timeout = popI64();
       RegI64 val = popI64();
 
-      MOZ_ASSERT(isMem32());
-      computeEffectiveAddress<RegI32>(access);
+      if (isMem32()) {
+        computeEffectiveAddress<RegI32>(access);
+      } else {
+#ifdef JS_CODEGEN_X86
+        {
+          ScratchPtr scratch(*this);
+          stashI64(scratch, val);
+          freeI64(val);
+        }
+#endif
+        computeEffectiveAddress<RegI64>(access);
+#ifdef JS_CODEGEN_X86
+        {
+          ScratchPtr scratch(*this);
+          val = needI64();
+          unstashI64(scratch, val);
+        }
+#endif
+      }
 
       pushI64(val);
       pushI64(timeout);
 
-      if (!emitInstanceCall(lineOrBytecode, SASigWaitI64)) {
+      if (!emitInstanceCall(lineOrBytecode,
+                            isMem32() ? SASigWaitI64M32 : SASigWaitI64M64)) {
         return false;
       }
       break;
@@ -1905,11 +2074,15 @@
                               uint32_t lineOrBytecode) {
   RegI32 count = popI32();
 
-  MOZ_ASSERT(isMem32());
-  computeEffectiveAddress<RegI32>(access);
+  if (isMem32()) {
+    computeEffectiveAddress<RegI32>(access);
+  } else {
+    computeEffectiveAddress<RegI64>(access);
+  }
 
   pushI32(count);
-  return emitInstanceCall(lineOrBytecode, SASigWake);
+  return emitInstanceCall(lineOrBytecode,
+                          isMem32() ? SASigWakeM32 : SASigWakeM64);
 }
 
 //////////////////////////////////////////////////////////////////////////////