# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: js/src/wasm/WasmIonCompile.cpp
# Commit: 27db0e6a7010
# Full Hash: 27db0e6a7010f290ac14e5d260c0b4fe48442486
# Author: Lars T Hansen <lhansen@mozilla.com>
# Date: 2021-10-18 16:09:29
# Regressor Bug: 1727084
# File Overlap Count: 6
# Description:
#   Bug 1727084 - Memory64 - Ion operations. r=yury
#   
#   Add memory64 generalizations for Ion for all operations (except bulk
#   memory, done in an earlier patch).
#   
# ==============================================================================

diff -r 1ab8383d17ee -r 27db0e6a7010 js/src/wasm/WasmIonCompile.cpp
--- a/js/src/wasm/WasmIonCompile.cpp	Mon Oct 18 07:31:56 2021 +0000
+++ b/js/src/wasm/WasmIonCompile.cpp	Mon Oct 18 07:31:56 2021 +0000
@@ -854,11 +854,7 @@
   // If the bounds checking strategy requires it, load the bounds check limit
   // from the Tls.
   MWasmLoadTls* maybeLoadBoundsCheckLimit(MIRType type) {
-#ifdef JS_64BIT
     MOZ_ASSERT(type == MIRType::Int32 || type == MIRType::Int64);
-#else
-    MOZ_ASSERT(type == MIRType::Int32);
-#endif
     if (moduleEnv_.hugeMemoryEnabled()) {
       return nullptr;
     }
@@ -886,8 +882,14 @@
 
     // If the EA is known and aligned it will need no checks.
     if (base->isConstant()) {
-      int32_t ptr = base->toConstant()->toInt32();
-      // OK to wrap around the address computation here.
+      // We only care about the low bits, so overflow is OK, as is chopping off
+      // the high bits of an i64 pointer.
+      uint32_t ptr = 0;
+      if (isMem64()) {
+        ptr = uint32_t(base->toConstant()->toInt64());
+      } else {
+        ptr = base->toConstant()->toInt32();
+      }
       if (((ptr + access->offset()) & (access->byteSize() - 1)) == 0) {
         return false;
       }
@@ -908,14 +910,27 @@
         GetMaxOffsetGuardLimit(moduleEnv_.hugeMemoryEnabled());
 
     if ((*base)->isConstant()) {
-      uint32_t basePtr = (*base)->toConstant()->toInt32();
+      uint64_t basePtr = 0;
+      if (isMem64()) {
+        basePtr = uint64_t((*base)->toConstant()->toInt64());
+      } else {
+        basePtr = uint64_t(int64_t((*base)->toConstant()->toInt32()));
+      }
+
       uint32_t offset = access->offset();
 
       if (offset < offsetGuardLimit && basePtr < offsetGuardLimit - offset) {
-        auto* ins = MConstant::New(alloc(), Int32Value(0), MIRType::Int32);
+        offset += uint32_t(basePtr);
+        access->setOffset(offset);
+
+        MConstant* ins = nullptr;
+        if (isMem64()) {
+          ins = MConstant::NewInt64(alloc(), 0);
+        } else {
+          ins = MConstant::New(alloc(), Int32Value(0), MIRType::Int32);
+        }
         curBlock_->add(ins);
         *base = ins;
-        access->setOffset(access->offset() + basePtr);
       }
     }
   }
@@ -933,7 +948,7 @@
     }
   }
 
-  MWasmLoadTls* needBoundsCheck(bool* limitIs64Bits_) {
+  MWasmLoadTls* needBoundsCheck() {
 #ifdef JS_64BIT
     // For 32-bit base pointers:
     //
@@ -944,38 +959,35 @@
     //
     // If the memory's max size is known to be smaller than 64K pages exactly,
     // we can use a 32-bit check and avoid extension and wrapping.
-    bool limitIs64Bits =
-        !moduleEnv_.memory->boundsCheckLimitIs32Bits() &&
+    bool mem32LimitIs64Bits =
+        isMem32() && !moduleEnv_.memory->boundsCheckLimitIs32Bits() &&
         ArrayBufferObject::maxBufferByteLength() >= 0x100000000;
 #else
     // On 32-bit platforms we have no more than 2GB memory and the limit for a
     // 32-bit base pointer is never a 64-bit value.
-    bool limitIs64Bits = false;
+    bool mem32LimitIs64Bits = false;
 #endif
-    MWasmLoadTls* boundsCheckLimit = maybeLoadBoundsCheckLimit(
-        limitIs64Bits ? MIRType::Int64 : MIRType::Int32);
-    if (boundsCheckLimit) {
-      *limitIs64Bits_ = limitIs64Bits;
-    }
-    return boundsCheckLimit;
-  }
-
-  void performBoundsCheck(MDefinition** base, MWasmLoadTls* boundsCheckLimit,
-                          bool limitIs64Bits) {
-    // At the outset, actualBase could be the result of pretty much any i32
-    // operation, or it could be the load of an i32 constant.  We may assume
-    // the value has a canonical representation for the platform, see doc
-    // block in MacroAssembler.h.
+    return maybeLoadBoundsCheckLimit(
+        mem32LimitIs64Bits || isMem64() ? MIRType::Int64 : MIRType::Int32);
+  }
+
+  void performBoundsCheck(MDefinition** base, MWasmLoadTls* boundsCheckLimit) {
+    // At the outset, actualBase could be the result of pretty much any integer
+    // operation, or it could be the load of an integer constant.  If its type
+    // is i32, we may assume the value has a canonical representation for the
+    // platform, see doc block in MacroAssembler.h.
     MDefinition* actualBase = *base;
 
-    // Extend the index value to perform a 64-bit bounds check if the memory
-    // can be 4GB.
-
-    if (limitIs64Bits) {
+    // Extend an i32 index value to perform a 64-bit bounds check if the memory
+    // can be 4GB or larger.
+    bool extendAndWrapIndex =
+        isMem32() && boundsCheckLimit->type() == MIRType::Int64;
+    if (extendAndWrapIndex) {
       auto* extended = MWasmExtendU32Index::New(alloc(), actualBase);
       curBlock_->add(extended);
       actualBase = extended;
     }
+
     auto* ins = MWasmBoundsCheck::New(alloc(), actualBase, boundsCheckLimit,
                                       bytecodeOffset());
     curBlock_->add(ins);
@@ -984,9 +996,8 @@
     // If we're masking, then we update *base to create a dependency chain
     // through the masked index.  But we will first need to wrap the index
     // value if it was extended above.
-
     if (JitOptions.spectreIndexMasking) {
-      if (limitIs64Bits) {
+      if (extendAndWrapIndex) {
         auto* wrapped = MWasmWrapU32Index::New(alloc(), actualBase);
         curBlock_->add(wrapped);
         actualBase = wrapped;
@@ -997,6 +1008,12 @@
 
   // Perform all necessary checking before a wasm heap access, based on the
   // attributes of the access and base pointer.
+  //
+  // For 64-bit indices on platforms that are limited to indices that fit into
+  // 32 bits (all 32-bit platforms and mips64), this returns a bounds-checked
+  // `base` that has type Int32.  Lowering code depends on this and will assert
+  // that the base has this type.  See the end of this function.
+
   void checkOffsetAndAlignmentAndBounds(MemoryAccessDesc* access,
                                         MDefinition** base) {
     MOZ_ASSERT(!inDeadCode());
@@ -1025,11 +1042,25 @@
 
     // Emit the bounds check if necessary; it traps if it fails.  This may
     // update *base.
-    bool limitIs64Bits = false;
-    MWasmLoadTls* boundsCheckLimit = needBoundsCheck(&limitIs64Bits);
+    MWasmLoadTls* boundsCheckLimit = needBoundsCheck();
     if (boundsCheckLimit) {
-      performBoundsCheck(base, boundsCheckLimit, limitIs64Bits);
-    }
+      performBoundsCheck(base, boundsCheckLimit);
+    }
+
+#ifndef JS_64BIT
+    if (isMem64()) {
+      // We must have had an explicit bounds check (or one was elided if it was
+      // proved redundant), and on 32-bit systems the index will for sure fit in
+      // 32 bits: the max memory is 2GB.  So chop the index down to 32-bit to
+      // simplify the back-end.
+      MOZ_ASSERT((*base)->type() == MIRType::Int64);
+      MOZ_ASSERT(!moduleEnv_.hugeMemoryEnabled());
+      auto* chopped = MWasmWrapU32Index::New(alloc(), *base);
+      MOZ_ASSERT(chopped->type() == MIRType::Int32);
+      curBlock_->add(chopped);
+      *base = chopped;
+    }
+#endif
   }
 
   bool isSmallerAccessForI64(ValType result, const MemoryAccessDesc* access) {
@@ -1043,6 +1074,7 @@
 
  public:
   bool isMem32() { return moduleEnv_.memory->indexType() == IndexType::I32; }
+  bool isMem64() { return moduleEnv_.memory->indexType() == IndexType::I64; }
 
   // Add the offset into the pointer to yield the EA; trap on overflow.
   MDefinition* computeEffectiveAddress(MDefinition* base,
@@ -1076,6 +1108,9 @@
                                  access->type());
     } else {
       checkOffsetAndAlignmentAndBounds(access, &base);
+#ifndef JS_64BIT
+      MOZ_ASSERT(base->type() == MIRType::Int32);
+#endif
       load =
           MWasmLoad::New(alloc(), memoryBase, base, *access, ToMIRType(result));
     }
@@ -1101,6 +1136,9 @@
                                    access->type(), v);
     } else {
       checkOffsetAndAlignmentAndBounds(access, &base);
+#ifndef JS_64BIT
+      MOZ_ASSERT(base->type() == MIRType::Int32);
+#endif
       store = MWasmStore::New(alloc(), memoryBase, base, *access, v);
     }
     if (!store) {
@@ -1118,6 +1156,9 @@
     }
 
     checkOffsetAndAlignmentAndBounds(access, &base);
+#ifndef JS_64BIT
+    MOZ_ASSERT(base->type() == MIRType::Int32);
+#endif
 
     if (isSmallerAccessForI64(result, access)) {
       auto* cvtOldv =
@@ -1155,6 +1196,9 @@
     }
 
     checkOffsetAndAlignmentAndBounds(access, &base);
+#ifndef JS_64BIT
+    MOZ_ASSERT(base->type() == MIRType::Int32);
+#endif
 
     if (isSmallerAccessForI64(result, access)) {
       auto* cvtValue =
@@ -1188,6 +1232,9 @@
     }
 
     checkOffsetAndAlignmentAndBounds(access, &base);
+#ifndef JS_64BIT
+    MOZ_ASSERT(base->type() == MIRType::Int32);
+#endif
 
     if (isSmallerAccessForI64(result, access)) {
       auto* cvtValue =
@@ -1281,6 +1328,9 @@
     MDefinition* base = addr.base;
     MOZ_ASSERT(!moduleEnv_.isAsmJS());
     checkOffsetAndAlignmentAndBounds(&access, &base);
+#  ifndef JS_64BIT
+    MOZ_ASSERT(base->type() == MIRType::Int32);
+#  endif
     MInstruction* load = MWasmLoadLaneSimd128::New(
         alloc(), memoryBase, base, access, laneSize, laneIndex, src);
     if (!load) {
@@ -1302,6 +1352,9 @@
     MDefinition* base = addr.base;
     MOZ_ASSERT(!moduleEnv_.isAsmJS());
     checkOffsetAndAlignmentAndBounds(&access, &base);
+#  ifndef JS_64BIT
+    MOZ_ASSERT(base->type() == MIRType::Int32);
+#  endif
     MInstruction* store = MWasmStoreLaneSimd128::New(
         alloc(), memoryBase, base, access, laneSize, laneIndex, src);
     if (!store) {
@@ -3253,7 +3306,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   MemoryAccessDesc access(viewType, addr.align, addr.offset,
                           f.bytecodeIfNotAsmJS());
   auto* ins = f.load(addr.base, &access, type);
@@ -3274,7 +3326,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   MemoryAccessDesc access(viewType, addr.align, addr.offset,
                           f.bytecodeIfNotAsmJS());
 
@@ -3291,7 +3342,7 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
+  MOZ_ASSERT(f.isMem32());  // asm.js opcode
   MemoryAccessDesc access(viewType, addr.align, addr.offset,
                           f.bytecodeIfNotAsmJS());
 
@@ -3316,7 +3367,7 @@
     MOZ_CRASH("unexpected coerced store");
   }
 
-  MOZ_ASSERT(f.isMem32());
+  MOZ_ASSERT(f.isMem32());  // asm.js opcode
   MemoryAccessDesc access(viewType, addr.align, addr.offset,
                           f.bytecodeIfNotAsmJS());
 
@@ -3482,7 +3533,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   MemoryAccessDesc access(viewType, addr.align, addr.offset, f.bytecodeOffset(),
                           Synchronization::Full());
   auto* ins =
@@ -3502,7 +3552,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   MemoryAccessDesc access(viewType, addr.align, addr.offset, f.bytecodeOffset(),
                           Synchronization::Load());
   auto* ins = f.load(addr.base, &access, type);
@@ -3522,7 +3571,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   MemoryAccessDesc access(viewType, addr.align, addr.offset, f.bytecodeOffset(),
                           Synchronization::Full());
   auto* ins = f.atomicBinopHeap(op, addr.base, &access, type, value);
@@ -3542,7 +3590,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   MemoryAccessDesc access(viewType, addr.align, addr.offset, f.bytecodeOffset(),
                           Synchronization::Store());
   f.store(addr.base, &access, value);
@@ -3556,7 +3603,8 @@
   uint32_t lineOrBytecode = f.readCallSiteLineOrBytecode();
 
   const SymbolicAddressSignature& callee =
-      type == ValType::I32 ? SASigWaitI32M32 : SASigWaitI64M32;
+      f.isMem32() ? (type == ValType::I32 ? SASigWaitI32M32 : SASigWaitI64M32)
+                  : (type == ValType::I32 ? SASigWaitI32M64 : SASigWaitI64M64);
   CallCompileState args;
   if (!f.passInstance(callee.argTypes[0], &args)) {
     return false;
@@ -3569,7 +3617,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   MemoryAccessDesc access(type == ValType::I32 ? Scalar::Int32 : Scalar::Int64,
                           addr.align, addr.offset, f.bytecodeOffset());
   MDefinition* ptr = f.computeEffectiveAddress(addr.base, &access);
@@ -3615,7 +3662,8 @@
 static bool EmitWake(FunctionCompiler& f) {
   uint32_t lineOrBytecode = f.readCallSiteLineOrBytecode();
 
-  const SymbolicAddressSignature& callee = SASigWakeM32;
+  const SymbolicAddressSignature& callee =
+      f.isMem32() ? SASigWakeM32 : SASigWakeM64;
   CallCompileState args;
   if (!f.passInstance(callee.argTypes[0], &args)) {
     return false;
@@ -3627,7 +3675,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   MemoryAccessDesc access(Scalar::Int32, addr.align, addr.offset,
                           f.bytecodeOffset());
   MDefinition* ptr = f.computeEffectiveAddress(addr.base, &access);
@@ -3664,7 +3711,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   MemoryAccessDesc access(viewType, addr.align, addr.offset, f.bytecodeOffset(),
                           Synchronization::Full());
   MDefinition* ins = f.atomicExchangeHeap(addr.base, &access, type, value);
@@ -4605,7 +4651,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   f.iter().setResult(f.loadSplatSimd128(viewType, addr, splatOp));
   return true;
 }
@@ -4616,7 +4661,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   f.iter().setResult(f.loadExtendSimd128(addr, op));
   return true;
 }
@@ -4628,7 +4672,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   f.iter().setResult(f.loadZeroSimd128(viewType, numBytes, addr));
   return true;
 }
@@ -4641,7 +4684,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   f.iter().setResult(f.loadLaneSimd128(laneSize, addr, laneIndex, src));
   return true;
 }
@@ -4654,7 +4696,6 @@
     return false;
   }
 
-  MOZ_ASSERT(f.isMem32());
   f.storeLaneSimd128(laneSize, addr, laneIndex, src);
   return true;
 }
