# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: gfx/wgpu/wgpu-core/src/device/mod.rs
# Commit: ef5dc3e04e5f
# Full Hash: ef5dc3e04e5f271eea0636ab3a495e95cc912f1d
# Author: Dzmitry Malyshau <dmalyshau@mozilla.com>
# Date: 2021-09-04 09:40:24
# Regressor Bug: 1726626
# File Overlap Count: 3
# Description:
#   Bug 1726626 - Move gfx/wgpu into a 3rd party dependency r=jgilbert,bholley
#   
#   This update makes wgpu a vendored dependency instead of having it in gfx/wgpu.
#   
#   ## Notes
# ==============================================================================

diff --git a/gfx/wgpu/wgpu-core/src/device/mod.rs b/third_party/rust/wgpu-core/src/device/mod.rs
rename from gfx/wgpu/wgpu-core/src/device/mod.rs
rename to third_party/rust/wgpu-core/src/device/mod.rs
--- a/gfx/wgpu/wgpu-core/src/device/mod.rs
+++ b/third_party/rust/wgpu-core/src/device/mod.rs
@@ -1,156 +1,95 @@
-/* This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
-
 use crate::{
     binding_model, command, conv,
     device::life::WaitIdleError,
-    hub::{
-        GfxBackend, Global, GlobalIdentityHandlerFactory, Hub, Input, InvalidId, Storage, Token,
-    },
-    id, instance,
-    memory_init_tracker::{MemoryInitKind, MemoryInitTracker, MemoryInitTrackerAction},
-    pipeline, resource, swap_chain,
+    hub::{Global, GlobalIdentityHandlerFactory, HalApi, Hub, Input, InvalidId, Storage, Token},
+    id,
+    init_tracker::{BufferInitTracker, BufferInitTrackerAction, MemoryInitKind},
+    instance, pipeline, present, resource,
     track::{BufferState, TextureSelector, TextureState, TrackerSet, UsageConflict},
     validation::{self, check_buffer_usage, check_texture_usage},
-    FastHashMap, Label, LabelHelpers, LifeGuard, MultiRefCount, PrivateFeatures, Stored,
-    SubmissionIndex, MAX_BIND_GROUPS,
+    FastHashMap, Label, LabelHelpers as _, LifeGuard, MultiRefCount, Stored, SubmissionIndex,
+    DOWNLEVEL_ERROR_MESSAGE,
 };
 
 use arrayvec::ArrayVec;
 use copyless::VecHelper as _;
-use hal::{
-    command::CommandBuffer as _,
-    device::Device as _,
-    window::{PresentationSurface as _, Surface as _},
-};
+use hal::{CommandEncoder as _, Device as _};
 use parking_lot::{Mutex, MutexGuard};
+use smallvec::SmallVec;
 use thiserror::Error;
-use wgt::{BufferAddress, InputStepMode, TextureDimension, TextureFormat, TextureViewDimension};
-
-use std::{
-    borrow::Cow,
-    collections::{hash_map::Entry, BTreeMap},
-    iter,
-    marker::PhantomData,
-    mem,
-    ops::Range,
-    ptr,
-    sync::atomic::Ordering,
-};
-
-pub mod alloc;
-pub mod descriptor;
+use wgt::{BufferAddress, TextureFormat, TextureViewDimension};
+
+use std::{borrow::Cow, iter, marker::PhantomData, mem, ops::Range, ptr, sync::atomic::Ordering};
+
 mod life;
 pub mod queue;
 #[cfg(any(feature = "trace", feature = "replay"))]
 pub mod trace;
 
-use smallvec::SmallVec;
-
-pub const MAX_COLOR_TARGETS: usize = 4;
-pub const MAX_MIP_LEVELS: u32 = 16;
-pub const MAX_VERTEX_BUFFERS: usize = 16;
-pub const MAX_ANISOTROPY: u8 = 16;
 pub const SHADER_STAGE_COUNT: usize = 3;
+const CLEANUP_WAIT_MS: u32 = 5000;
 
 const IMPLICIT_FAILURE: &str = "failed implicit";
+const EP_FAILURE: &str = "EP is invalid";
 
 pub type DeviceDescriptor<'a> = wgt::DeviceDescriptor<Label<'a>>;
 
-pub fn all_buffer_stages() -> hal::pso::PipelineStage {
-    use hal::pso::PipelineStage as Ps;
-    Ps::DRAW_INDIRECT
-        | Ps::VERTEX_INPUT
-        | Ps::VERTEX_SHADER
-        | Ps::FRAGMENT_SHADER
-        | Ps::COMPUTE_SHADER
-        | Ps::TRANSFER
-        | Ps::HOST
-}
-pub fn all_image_stages() -> hal::pso::PipelineStage {
-    use hal::pso::PipelineStage as Ps;
-    Ps::EARLY_FRAGMENT_TESTS
-        | Ps::LATE_FRAGMENT_TESTS
-        | Ps::COLOR_ATTACHMENT_OUTPUT
-        | Ps::VERTEX_SHADER
-        | Ps::FRAGMENT_SHADER
-        | Ps::COMPUTE_SHADER
-        | Ps::TRANSFER
-}
-
 #[repr(C)]
 #[derive(Clone, Copy, Debug, PartialEq)]
 #[cfg_attr(feature = "trace", derive(serde::Serialize))]
 #[cfg_attr(feature = "replay", derive(serde::Deserialize))]
 pub enum HostMap {
     Read,
     Write,
 }
 
 #[derive(Clone, Debug, Hash, PartialEq)]
 #[cfg_attr(feature = "serial-pass", derive(serde::Deserialize, serde::Serialize))]
 pub(crate) struct AttachmentData<T> {
-    pub colors: ArrayVec<[T; MAX_COLOR_TARGETS]>,
-    pub resolves: ArrayVec<[T; MAX_COLOR_TARGETS]>,
+    pub colors: ArrayVec<T, { hal::MAX_COLOR_TARGETS }>,
+    pub resolves: ArrayVec<T, { hal::MAX_COLOR_TARGETS }>,
     pub depth_stencil: Option<T>,
 }
 impl<T: PartialEq> Eq for AttachmentData<T> {}
 impl<T> AttachmentData<T> {
-    pub(crate) fn all(&self) -> impl Iterator<Item = &T> {
-        self.colors
-            .iter()
-            .chain(&self.resolves)
-            .chain(&self.depth_stencil)
-    }
-
     pub(crate) fn map<U, F: Fn(&T) -> U>(&self, fun: F) -> AttachmentData<U> {
         AttachmentData {
             colors: self.colors.iter().map(&fun).collect(),
             resolves: self.resolves.iter().map(&fun).collect(),
             depth_stencil: self.depth_stencil.as_ref().map(&fun),
         }
     }
 }
 
-pub(crate) type AttachmentDataVec<T> = ArrayVec<[T; MAX_COLOR_TARGETS + MAX_COLOR_TARGETS + 1]>;
-pub(crate) type RenderPassKey = AttachmentData<(hal::pass::Attachment, hal::image::Layout)>;
-#[derive(Debug, Eq, Hash, PartialEq)]
-pub(crate) struct FramebufferKey {
-    pub(crate) attachments: AttachmentData<hal::image::FramebufferAttachment>,
-    pub(crate) extent: wgt::Extent3d,
-    pub(crate) samples: hal::image::NumSamples,
-}
-
 #[derive(Clone, Debug, Hash, PartialEq)]
 #[cfg_attr(feature = "serial-pass", derive(serde::Deserialize, serde::Serialize))]
 pub(crate) struct RenderPassContext {
     pub attachments: AttachmentData<TextureFormat>,
-    pub sample_count: u8,
+    pub sample_count: u32,
 }
 #[derive(Clone, Debug, Error)]
 pub enum RenderPassCompatibilityError {
     #[error("Incompatible color attachment: {0:?} != {1:?}")]
     IncompatibleColorAttachment(
-        ArrayVec<[TextureFormat; MAX_COLOR_TARGETS]>,
-        ArrayVec<[TextureFormat; MAX_COLOR_TARGETS]>,
+        ArrayVec<TextureFormat, { hal::MAX_COLOR_TARGETS }>,
+        ArrayVec<TextureFormat, { hal::MAX_COLOR_TARGETS }>,
     ),
     #[error("Incompatible depth-stencil attachment: {0:?} != {1:?}")]
     IncompatibleDepthStencilAttachment(Option<TextureFormat>, Option<TextureFormat>),
     #[error("Incompatible sample count: {0:?} != {1:?}")]
-    IncompatibleSampleCount(u8, u8),
+    IncompatibleSampleCount(u32, u32),
 }
 
 impl RenderPassContext {
     // Assumed the renderpass only contains one subpass
     pub(crate) fn check_compatible(
         &self,
-        other: &RenderPassContext,
+        other: &Self,
     ) -> Result<(), RenderPassCompatibilityError> {
         if self.attachments.colors != other.attachments.colors {
             return Err(RenderPassCompatibilityError::IncompatibleColorAttachment(
                 self.attachments.colors.clone(),
                 other.attachments.colors.clone(),
             ));
         }
         if self.attachments.depth_stencil != other.attachments.depth_stencil {
@@ -166,298 +105,316 @@ impl RenderPassContext {
                 self.sample_count,
                 other.sample_count,
             ));
         }
         Ok(())
     }
 }
 
-type BufferMapPendingCallback = (resource::BufferMapOperation, resource::BufferMapAsyncStatus);
-
-fn map_buffer<B: hal::Backend>(
-    raw: &B::Device,
-    buffer: &mut resource::Buffer<B>,
-    offset: hal::buffer::Offset,
+pub type BufferMapPendingClosure = (resource::BufferMapOperation, resource::BufferMapAsyncStatus);
+
+#[derive(Default)]
+pub struct UserClosures {
+    pub mappings: Vec<BufferMapPendingClosure>,
+    pub submissions: SmallVec<[queue::SubmittedWorkDoneClosure; 1]>,
+}
+
+impl UserClosures {
+    fn extend(&mut self, other: Self) {
+        self.mappings.extend(other.mappings);
+        self.submissions.extend(other.submissions);
+    }
+
+    unsafe fn fire(self) {
+        //Note: this logic is specifically moved out of `handle_mapping()` in order to
+        // have nothing locked by the time we execute users callback code.
+        for (operation, status) in self.mappings {
+            (operation.callback)(status, operation.user_data);
+        }
+        for closure in self.submissions {
+            (closure.callback)(closure.user_data);
+        }
+    }
+}
+
+fn map_buffer<A: hal::Api>(
+    raw: &A::Device,
+    buffer: &mut resource::Buffer<A>,
+    offset: BufferAddress,
     size: BufferAddress,
     kind: HostMap,
 ) -> Result<ptr::NonNull<u8>, resource::BufferAccessError> {
-    let &mut (_, ref mut block) = buffer
-        .raw
-        .as_mut()
-        .ok_or(resource::BufferAccessError::Destroyed)?;
-    let ptr = block.map(raw, offset, size).map_err(DeviceError::from)?;
+    let mapping = unsafe {
+        raw.map_buffer(buffer.raw.as_ref().unwrap(), offset..offset + size)
+            .map_err(DeviceError::from)?
+    };
 
     buffer.sync_mapped_writes = match kind {
-        HostMap::Read if !block.is_coherent() => {
-            block.invalidate_range(raw, offset, Some(size))?;
+        HostMap::Read if !mapping.is_coherent => unsafe {
+            raw.invalidate_mapped_ranges(
+                buffer.raw.as_ref().unwrap(),
+                iter::once(offset..offset + size),
+            );
             None
-        }
-        HostMap::Write if !block.is_coherent() => Some(hal::memory::Segment {
-            offset,
-            size: Some(size),
-        }),
+        },
+        HostMap::Write if !mapping.is_coherent => Some(offset..offset + size),
         _ => None,
     };
 
     // Zero out uninitialized parts of the mapping. (Spec dictates all resources behave as if they were initialized with zero)
     //
     // If this is a read mapping, ideally we would use a `fill_buffer` command before reading the data from GPU (i.e. `invalidate_range`).
     // However, this would require us to kick off and wait for a command buffer or piggy back on an existing one (the later is likely the only worthwhile option).
     // As reading uninitialized memory isn't a particular important path to support,
     // we instead just initialize the memory here and make sure it is GPU visible, so this happens at max only once for every buffer region.
     //
     // If this is a write mapping zeroing out the memory here is the only reasonable way as all data is pushed to GPU anyways.
-    let zero_init_needs_flush_now = !block.is_coherent() && buffer.sync_mapped_writes.is_none(); // No need to flush if it is flushed later anyways.
+    let zero_init_needs_flush_now = mapping.is_coherent && buffer.sync_mapped_writes.is_none(); // No need to flush if it is flushed later anyways.
     for uninitialized_range in buffer.initialization_status.drain(offset..(size + offset)) {
         let num_bytes = uninitialized_range.end - uninitialized_range.start;
         unsafe {
             ptr::write_bytes(
-                ptr.as_ptr().offset(uninitialized_range.start as isize),
+                mapping
+                    .ptr
+                    .as_ptr()
+                    .offset(uninitialized_range.start as isize),
                 0,
                 num_bytes as usize,
             )
         };
         if zero_init_needs_flush_now {
-            block.flush_range(raw, uninitialized_range.start, Some(num_bytes))?;
+            unsafe {
+                raw.flush_mapped_ranges(
+                    buffer.raw.as_ref().unwrap(),
+                    iter::once(uninitialized_range.start..uninitialized_range.start + num_bytes),
+                )
+            };
         }
     }
 
-    Ok(ptr)
+    Ok(mapping.ptr)
+}
+
+struct CommandAllocator<A: hal::Api> {
+    free_encoders: Vec<A::CommandEncoder>,
 }
 
-fn unmap_buffer<B: hal::Backend>(
-    raw: &B::Device,
-    buffer: &mut resource::Buffer<B>,
-) -> Result<(), resource::BufferAccessError> {
-    let &mut (_, ref mut block) = buffer
-        .raw
-        .as_mut()
-        .ok_or(resource::BufferAccessError::Destroyed)?;
-    if let Some(segment) = buffer.sync_mapped_writes.take() {
-        block.flush_range(raw, segment.offset, segment.size)?;
+impl<A: hal::Api> CommandAllocator<A> {
+    fn acquire_encoder(
+        &mut self,
+        device: &A::Device,
+        queue: &A::Queue,
+    ) -> Result<A::CommandEncoder, hal::DeviceError> {
+        match self.free_encoders.pop() {
+            Some(encoder) => Ok(encoder),
+            None => unsafe {
+                let hal_desc = hal::CommandEncoderDescriptor { label: None, queue };
+                device.create_command_encoder(&hal_desc)
+            },
+        }
     }
-    block.unmap(raw);
-    Ok(())
-}
-
-//Note: this logic is specifically moved out of `handle_mapping()` in order to
-// have nothing locked by the time we execute users callback code.
-fn fire_map_callbacks<I: IntoIterator<Item = BufferMapPendingCallback>>(callbacks: I) {
-    for (operation, status) in callbacks {
-        unsafe { (operation.callback)(status, operation.user_data) }
+
+    fn release_encoder(&mut self, encoder: A::CommandEncoder) {
+        self.free_encoders.push(encoder);
     }
-}
-
-#[derive(Debug)]
-pub(crate) struct RenderPassLock<B: hal::Backend> {
-    pub(crate) render_passes: FastHashMap<RenderPassKey, B::RenderPass>,
-    pub(crate) framebuffers: FastHashMap<FramebufferKey, B::Framebuffer>,
+
+    fn dispose(self, device: &A::Device) {
+        log::info!("Destroying {} command encoders", self.free_encoders.len());
+        for cmd_encoder in self.free_encoders {
+            unsafe {
+                device.destroy_command_encoder(cmd_encoder);
+            }
+        }
+    }
 }
 
 /// Structure describing a logical device. Some members are internally mutable,
 /// stored behind mutexes.
 /// TODO: establish clear order of locking for these:
 /// `mem_allocator`, `desc_allocator`, `life_tracke`, `trackers`,
 /// `render_passes`, `pending_writes`, `trace`.
 ///
 /// Currently, the rules are:
 /// 1. `life_tracker` is locked after `hub.devices`, enforced by the type system
 /// 1. `self.trackers` is locked last (unenforced)
 /// 1. `self.trace` is locked last (unenforced)
-#[derive(Debug)]
-pub struct Device<B: hal::Backend> {
-    pub(crate) raw: B::Device,
+pub struct Device<A: hal::Api> {
+    pub(crate) raw: A::Device,
     pub(crate) adapter_id: Stored<id::AdapterId>,
-    pub(crate) queue_group: hal::queue::QueueGroup<B>,
-    pub(crate) cmd_allocator: command::CommandAllocator<B>,
-    mem_allocator: Mutex<alloc::MemoryAllocator<B>>,
-    desc_allocator: Mutex<descriptor::DescriptorAllocator<B>>,
+    pub(crate) queue: A::Queue,
+    //pub(crate) cmd_allocator: command::CommandAllocator<A>,
+    //mem_allocator: Mutex<alloc::MemoryAllocator<A>>,
+    //desc_allocator: Mutex<descriptor::DescriptorAllocator<A>>,
     //Note: The submission index here corresponds to the last submission that is done.
     pub(crate) life_guard: LifeGuard,
+    command_allocator: Mutex<CommandAllocator<A>>,
     pub(crate) active_submission_index: SubmissionIndex,
+    fence: A::Fence,
     /// Has to be locked temporarily only (locked last)
     pub(crate) trackers: Mutex<TrackerSet>,
-    pub(crate) render_passes: Mutex<RenderPassLock<B>>,
     // Life tracker should be locked right after the device and before anything else.
-    life_tracker: Mutex<life::LifetimeTracker<B>>,
+    life_tracker: Mutex<life::LifetimeTracker<A>>,
     temp_suspected: life::SuspectedResources,
-    pub(crate) hal_limits: hal::Limits,
-    pub(crate) private_features: PrivateFeatures,
+    pub(crate) alignments: hal::Alignments,
     pub(crate) limits: wgt::Limits,
     pub(crate) features: wgt::Features,
-    pub(crate) downlevel: wgt::DownlevelProperties,
-    spv_options: naga::back::spv::Options,
+    pub(crate) downlevel: wgt::DownlevelCapabilities,
     //TODO: move this behind another mutex. This would allow several methods to switch
     // to borrow Device immutably, such as `write_buffer`, `write_texture`, and `buffer_unmap`.
-    pending_writes: queue::PendingWrites<B>,
+    pending_writes: queue::PendingWrites<A>,
     #[cfg(feature = "trace")]
     pub(crate) trace: Option<Mutex<trace::Trace>>,
 }
 
 #[derive(Clone, Debug, Error)]
 pub enum CreateDeviceError {
     #[error("not enough memory left")]
     OutOfMemory,
 }
 
-impl<B: GfxBackend> Device<B> {
+impl<A: hal::Api> Device<A> {
+    pub(crate) fn require_features(&self, feature: wgt::Features) -> Result<(), MissingFeatures> {
+        if self.features.contains(feature) {
+            Ok(())
+        } else {
+            Err(MissingFeatures(feature))
+        }
+    }
+
+    pub(crate) fn require_downlevel_flags(
+        &self,
+        flags: wgt::DownlevelFlags,
+    ) -> Result<(), MissingDownlevelFlags> {
+        if self.downlevel.flags.contains(flags) {
+            Ok(())
+        } else {
+            Err(MissingDownlevelFlags(flags))
+        }
+    }
+}
+
+impl<A: HalApi> Device<A> {
     #[allow(clippy::too_many_arguments)]
     pub(crate) fn new(
-        raw: B::Device,
+        open: hal::OpenDevice<A>,
         adapter_id: Stored<id::AdapterId>,
-        queue_group: hal::queue::QueueGroup<B>,
-        mem_props: hal::adapter::MemoryProperties,
-        hal_limits: hal::Limits,
-        private_features: PrivateFeatures,
-        downlevel: wgt::DownlevelProperties,
+        alignments: hal::Alignments,
+        downlevel: wgt::DownlevelCapabilities,
         desc: &DeviceDescriptor,
         trace_path: Option<&std::path::Path>,
     ) -> Result<Self, CreateDeviceError> {
-        let cmd_allocator = command::CommandAllocator::new(queue_group.family, &raw)
-            .or(Err(CreateDeviceError::OutOfMemory))?;
-
-        let mem_allocator = alloc::MemoryAllocator::new(mem_props, hal_limits);
-        let descriptors = descriptor::DescriptorAllocator::new();
         #[cfg(not(feature = "trace"))]
         if let Some(_) = trace_path {
             log::error!("Feature 'trace' is not enabled");
         }
-
-        let spv_options = {
-            use naga::back::spv;
-            let mut flags = spv::WriterFlags::empty();
-            flags.set(spv::WriterFlags::DEBUG, cfg!(debug_assertions));
-            //Note: we don't adjust the coordinate space, because `NDC_Y_UP` is required.
-            spv::Options {
-                lang_version: (1, 0),
-                //TODO: can be `None` once `spirv` is published
-                capabilities: Some(
-                    [
-                        spv::Capability::Shader,
-                        spv::Capability::DerivativeControl,
-                        spv::Capability::InterpolationFunction,
-                        spv::Capability::Matrix,
-                        spv::Capability::ImageQuery,
-                        spv::Capability::Sampled1D,
-                        spv::Capability::Image1D,
-                        spv::Capability::SampledCubeArray,
-                        spv::Capability::ImageCubeArray,
-                        spv::Capability::StorageImageExtendedFormats,
-                    ]
-                    .iter()
-                    .cloned()
-                    .collect(),
-                ),
-                flags,
-            }
+        let fence =
+            unsafe { open.device.create_fence() }.map_err(|_| CreateDeviceError::OutOfMemory)?;
+
+        let mut com_alloc = CommandAllocator {
+            free_encoders: Vec::new(),
         };
+        let pending_encoder = com_alloc
+            .acquire_encoder(&open.device, &open.queue)
+            .map_err(|_| CreateDeviceError::OutOfMemory)?;
+        let pending_writes = queue::PendingWrites::new(pending_encoder);
 
         Ok(Self {
-            raw,
+            raw: open.device,
             adapter_id,
-            cmd_allocator,
-            mem_allocator: Mutex::new(mem_allocator),
-            desc_allocator: Mutex::new(descriptors),
-            queue_group,
+            queue: open.queue,
             life_guard: LifeGuard::new("<device>"),
+            command_allocator: Mutex::new(com_alloc),
             active_submission_index: 0,
-            trackers: Mutex::new(TrackerSet::new(B::VARIANT)),
-            render_passes: Mutex::new(RenderPassLock {
-                render_passes: FastHashMap::default(),
-                framebuffers: FastHashMap::default(),
-            }),
+            fence,
+            trackers: Mutex::new(TrackerSet::new(A::VARIANT)),
             life_tracker: Mutex::new(life::LifetimeTracker::new()),
             temp_suspected: life::SuspectedResources::default(),
             #[cfg(feature = "trace")]
             trace: trace_path.and_then(|path| match trace::Trace::new(path) {
                 Ok(mut trace) => {
                     trace.add(trace::Action::Init {
                         desc: desc.clone(),
-                        backend: B::VARIANT,
+                        backend: A::VARIANT,
                     });
                     Some(Mutex::new(trace))
                 }
                 Err(e) => {
                     log::error!("Unable to start a trace in '{:?}': {:?}", path, e);
                     None
                 }
             }),
-            hal_limits,
-            private_features,
+            alignments,
             limits: desc.limits.clone(),
             features: desc.features,
             downlevel,
-            spv_options,
-            pending_writes: queue::PendingWrites::new(),
+            pending_writes,
         })
     }
 
-    pub(crate) fn require_features(&self, feature: wgt::Features) -> Result<(), MissingFeatures> {
-        if self.features.contains(feature) {
-            Ok(())
-        } else {
-            Err(MissingFeatures(feature))
-        }
-    }
-
-    pub(crate) fn last_completed_submission_index(&self) -> SubmissionIndex {
-        self.life_guard.submission_index.load(Ordering::Acquire)
-    }
-
-    fn lock_life_internal<'this, 'token: 'this>(
-        tracker: &'this Mutex<life::LifetimeTracker<B>>,
-        _token: &mut Token<'token, Self>,
-    ) -> MutexGuard<'this, life::LifetimeTracker<B>> {
-        tracker.lock()
-    }
-
     fn lock_life<'this, 'token: 'this>(
         &'this self,
         //TODO: fix this - the token has to be borrowed for the lock
-        token: &mut Token<'token, Self>,
-    ) -> MutexGuard<'this, life::LifetimeTracker<B>> {
-        Self::lock_life_internal(&self.life_tracker, token)
+        _token: &mut Token<'token, Self>,
+    ) -> MutexGuard<'this, life::LifetimeTracker<A>> {
+        self.life_tracker.lock()
     }
 
     fn maintain<'this, 'token: 'this, G: GlobalIdentityHandlerFactory>(
         &'this self,
-        hub: &Hub<B, G>,
+        hub: &Hub<A, G>,
         force_wait: bool,
         token: &mut Token<'token, Self>,
-    ) -> Result<Vec<BufferMapPendingCallback>, WaitIdleError> {
+    ) -> Result<UserClosures, WaitIdleError> {
         profiling::scope!("maintain", "Device");
         let mut life_tracker = self.lock_life(token);
 
         life_tracker
             .suspected_resources
             .extend(&self.temp_suspected);
 
         life_tracker.triage_suspected(
             hub,
             &self.trackers,
             #[cfg(feature = "trace")]
             self.trace.as_ref(),
             token,
         );
         life_tracker.triage_mapped(hub, token);
-        let last_done = life_tracker.triage_submissions(&self.raw, force_wait)?;
-        let callbacks = life_tracker.handle_mapping(hub, &self.raw, &self.trackers, token);
-        life_tracker.cleanup(&self.raw, &self.mem_allocator, &self.desc_allocator);
-
-        self.life_guard
-            .submission_index
-            .store(last_done, Ordering::Release);
-        self.cmd_allocator.maintain(&self.raw, last_done);
-        Ok(callbacks)
+
+        let last_done_index = if force_wait {
+            let current_index = self.active_submission_index;
+            unsafe {
+                self.raw
+                    .wait(&self.fence, current_index, CLEANUP_WAIT_MS)
+                    .map_err(DeviceError::from)?
+            };
+            current_index
+        } else {
+            unsafe {
+                self.raw
+                    .get_fence_value(&self.fence)
+                    .map_err(DeviceError::from)?
+            }
+        };
+
+        let submission_closures =
+            life_tracker.triage_submissions(last_done_index, &self.command_allocator);
+        let mapping_closures = life_tracker.handle_mapping(hub, &self.raw, &self.trackers, token);
+        life_tracker.cleanup(&self.raw);
+
+        Ok(UserClosures {
+            mappings: mapping_closures,
+            submissions: submission_closures,
+        })
     }
 
     fn untrack<'this, 'token: 'this, G: GlobalIdentityHandlerFactory>(
         &'this mut self,
-        hub: &Hub<B, G>,
+        hub: &Hub<A, G>,
         trackers: &TrackerSet,
         mut token: &mut Token<'token, Self>,
     ) {
         self.temp_suspected.clear();
         // As the tracker is cleared/dropped, we need to consider all the resources
         // that it references for destruction in the next GC pass.
         {
             let (bind_group_guard, mut token) = hub.bind_groups.read(&mut token);
@@ -518,130 +475,94 @@ impl<B: GfxBackend> Device<B> {
         self.temp_suspected.clear();
     }
 
     fn create_buffer(
         &self,
         self_id: id::DeviceId,
         desc: &resource::BufferDescriptor,
         transient: bool,
-    ) -> Result<resource::Buffer<B>, resource::CreateBufferError> {
-        debug_assert_eq!(self_id.backend(), B::VARIANT);
-        let (mut usage, _memory_properties) = conv::map_buffer_usage(desc.usage);
-        if desc.mapped_at_creation {
-            if desc.size % wgt::COPY_BUFFER_ALIGNMENT != 0 {
-                return Err(resource::CreateBufferError::UnalignedSize);
-            }
-            if !desc.usage.contains(wgt::BufferUsage::MAP_WRITE) {
-                // we are going to be copying into it, internally
-                usage |= hal::buffer::Usage::TRANSFER_DST;
-            }
-        } else {
-            // We are required to zero out (initialize) all memory.
-            // This is done on demand using fill_buffer which requires write transfer usage!
-            usage |= hal::buffer::Usage::TRANSFER_DST;
-        }
+    ) -> Result<resource::Buffer<A>, resource::CreateBufferError> {
+        debug_assert_eq!(self_id.backend(), A::VARIANT);
+        let mut usage = conv::map_buffer_usage(desc.usage);
 
         if desc.usage.is_empty() {
             return Err(resource::CreateBufferError::EmptyUsage);
         }
 
-        let mem_usage = {
-            use gpu_alloc::UsageFlags as Uf;
-            use wgt::BufferUsage as Bu;
-
-            let mut flags = Uf::empty();
-            let map_flags = desc.usage & (Bu::MAP_READ | Bu::MAP_WRITE);
-            let map_copy_flags =
-                desc.usage & (Bu::MAP_READ | Bu::MAP_WRITE | Bu::COPY_SRC | Bu::COPY_DST);
-            if map_flags.is_empty() || !(desc.usage - map_copy_flags).is_empty() {
-                flags |= Uf::FAST_DEVICE_ACCESS;
+        if desc.mapped_at_creation {
+            if desc.size % wgt::COPY_BUFFER_ALIGNMENT != 0 {
+                return Err(resource::CreateBufferError::UnalignedSize);
             }
-            if transient {
-                flags |= Uf::TRANSIENT;
+            if !desc.usage.contains(wgt::BufferUsages::MAP_WRITE) {
+                // we are going to be copying into it, internally
+                usage |= hal::BufferUses::COPY_DST;
             }
-
-            if !map_flags.is_empty() {
-                let upload_usage = Bu::MAP_WRITE | Bu::COPY_SRC;
-                let download_usage = Bu::MAP_READ | Bu::COPY_DST;
-
-                flags |= Uf::HOST_ACCESS;
-                if desc.usage.contains(upload_usage) {
-                    flags |= Uf::UPLOAD;
-                }
-                if desc.usage.contains(download_usage) {
-                    flags |= Uf::DOWNLOAD;
-                }
-
-                let is_native_only = self
-                    .features
-                    .contains(wgt::Features::MAPPABLE_PRIMARY_BUFFERS);
-                if !is_native_only
-                    && !upload_usage.contains(desc.usage)
-                    && !download_usage.contains(desc.usage)
-                {
-                    return Err(resource::CreateBufferError::UsageMismatch(desc.usage));
-                }
-            }
-
-            flags
+        } else {
+            // We are required to zero out (initialize) all memory.
+            // This is done on demand using fill_buffer which requires write transfer usage!
+            usage |= hal::BufferUses::COPY_DST;
+        }
+
+        let actual_size = if desc.size == 0 {
+            wgt::COPY_BUFFER_ALIGNMENT
+        } else if desc.usage.contains(wgt::BufferUsages::VERTEX) {
+            // Bumping the size by 1 so that we can bind an empty range at the end of the buffer.
+            desc.size + 1
+        } else {
+            desc.size
         };
 
-        let mut buffer = unsafe {
-            self.raw
-                .create_buffer(desc.size.max(1), usage, hal::memory::SparseFlags::empty())
-        }
-        .map_err(|err| match err {
-            hal::buffer::CreationError::OutOfMemory(_) => DeviceError::OutOfMemory,
-            _ => panic!("failed to create buffer: {}", err),
-        })?;
-        if let Some(ref label) = desc.label {
-            unsafe { self.raw.set_buffer_name(&mut buffer, label) };
-        }
-
-        let requirements = unsafe { self.raw.get_buffer_requirements(&buffer) };
-        let block = self
-            .mem_allocator
-            .lock()
-            .allocate(&self.raw, requirements, mem_usage)?;
-        block.bind_buffer(&self.raw, &mut buffer)?;
+        let mut memory_flags = hal::MemoryFlags::empty();
+        memory_flags.set(hal::MemoryFlags::TRANSIENT, transient);
+
+        let hal_desc = hal::BufferDescriptor {
+            label: desc.label.borrow_option(),
+            size: actual_size,
+            usage,
+            memory_flags,
+        };
+        let buffer = unsafe { self.raw.create_buffer(&hal_desc) }.map_err(DeviceError::from)?;
 
         Ok(resource::Buffer {
-            raw: Some((buffer, block)),
+            raw: Some(buffer),
             device_id: Stored {
                 value: id::Valid(self_id),
                 ref_count: self.life_guard.add_ref(),
             },
             usage: desc.usage,
             size: desc.size,
-            initialization_status: MemoryInitTracker::new(desc.size),
+            initialization_status: BufferInitTracker::new(desc.size),
             sync_mapped_writes: None,
             map_state: resource::BufferMapState::Idle,
             life_guard: LifeGuard::new(desc.label.borrow_or_default()),
         })
     }
 
-    fn create_texture(
+    fn create_texture_from_hal(
         &self,
+        hal_texture: A::Texture,
         self_id: id::DeviceId,
-        adapter: &crate::instance::Adapter<B>,
+        adapter: &crate::instance::Adapter<A>,
         desc: &resource::TextureDescriptor,
-    ) -> Result<resource::Texture<B>, resource::CreateTextureError> {
-        debug_assert_eq!(self_id.backend(), B::VARIANT);
+    ) -> Result<resource::Texture<A>, resource::CreateTextureError> {
+        debug_assert_eq!(self_id.backend(), A::VARIANT);
+
+        let hal_usage = conv::map_texture_usage(desc.usage, desc.format.into());
 
         let format_features = self
             .describe_format_features(adapter, desc.format)
             .map_err(|error| resource::CreateTextureError::MissingFeatures(desc.format, error))?;
 
         // Ensure `D24Plus` textures cannot be copied
         match desc.format {
             TextureFormat::Depth24Plus | TextureFormat::Depth24PlusStencil8 => {
                 if desc
                     .usage
-                    .intersects(wgt::TextureUsage::COPY_SRC | wgt::TextureUsage::COPY_DST)
+                    .intersects(wgt::TextureUsages::COPY_SRC | wgt::TextureUsages::COPY_DST)
                 {
                     return Err(resource::CreateTextureError::CannotCopyD24Plus);
                 }
             }
             _ => {}
         }
 
         if desc.usage.is_empty() {
@@ -651,666 +572,623 @@ impl<B: GfxBackend> Device<B> {
         let missing_allowed_usages = desc.usage - format_features.allowed_usages;
         if !missing_allowed_usages.is_empty() {
             return Err(resource::CreateTextureError::InvalidUsages(
                 missing_allowed_usages,
                 desc.format,
             ));
         }
 
-        let kind = conv::map_texture_dimension_size(
+        conv::check_texture_dimension_size(
             desc.dimension,
             desc.size,
             desc.sample_count,
             &self.limits,
         )?;
-        let format = conv::map_texture_format(desc.format, self.private_features);
-        let aspects = format.surface_desc().aspects;
-        let usage = conv::map_texture_usage(desc.usage, aspects);
-
-        let mip_level_count = desc.mip_level_count;
-        if mip_level_count == 0
-            || mip_level_count > MAX_MIP_LEVELS
-            || mip_level_count > kind.compute_num_levels() as u32
-        {
-            return Err(resource::CreateTextureError::InvalidMipLevelCount(
-                mip_level_count,
-            ));
+
+        let mips = desc.mip_level_count;
+        if mips == 0 || mips > hal::MAX_MIP_LEVELS || mips > desc.size.max_mips() {
+            return Err(resource::CreateTextureError::InvalidMipLevelCount(mips));
         }
-        let mut view_caps = hal::image::ViewCapabilities::empty();
-        // 2D textures with array layer counts that are multiples of 6 could be cubemaps
-        // Following gpuweb/gpuweb#68 always add the hint in that case
-        if desc.dimension == TextureDimension::D2
-            && desc.size.depth_or_array_layers % 6 == 0
-            && (desc.size.depth_or_array_layers == 6
-                || self
-                    .downlevel
-                    .flags
-                    .contains(wgt::DownlevelFlags::CUBE_ARRAY_TEXTURES))
-        {
-            view_caps |= hal::image::ViewCapabilities::KIND_CUBE;
-        };
-
-        // TODO: 2D arrays, cubemap arrays
-
-        let mut image = unsafe {
-            let mut image = self
-                .raw
-                .create_image(
-                    kind,
-                    desc.mip_level_count as hal::image::Level,
-                    format,
-                    hal::image::Tiling::Optimal,
-                    usage,
-                    hal::memory::SparseFlags::empty(),
-                    view_caps,
-                )
-                .map_err(|err| match err {
-                    hal::image::CreationError::OutOfMemory(_) => DeviceError::OutOfMemory,
-                    _ => panic!("failed to create texture: {}", err),
-                })?;
-            if let Some(ref label) = desc.label {
-                self.raw.set_image_name(&mut image, label);
-            }
-            image
-        };
-
-        let requirements = unsafe { self.raw.get_image_requirements(&image) };
-        let block = self.mem_allocator.lock().allocate(
-            &self.raw,
-            requirements,
-            gpu_alloc::UsageFlags::FAST_DEVICE_ACCESS,
-        )?;
-        block.bind_image(&self.raw, &mut image)?;
 
         Ok(resource::Texture {
-            raw: Some((image, block)),
+            inner: resource::TextureInner::Native {
+                raw: Some(hal_texture),
+            },
             device_id: Stored {
                 value: id::Valid(self_id),
                 ref_count: self.life_guard.add_ref(),
             },
-            usage: desc.usage,
-            aspects,
-            dimension: desc.dimension,
-            kind,
-            format: desc.format,
+            desc: desc.map_label(|_| ()),
+            hal_usage,
             format_features,
-            framebuffer_attachment: hal::image::FramebufferAttachment {
-                usage,
-                view_caps,
-                format,
-            },
             full_range: TextureSelector {
-                levels: 0..desc.mip_level_count as hal::image::Level,
-                layers: 0..kind.num_layers(),
+                levels: 0..desc.mip_level_count,
+                layers: 0..desc.array_layer_count(),
             },
             life_guard: LifeGuard::new(desc.label.borrow_or_default()),
         })
     }
 
+    fn create_texture(
+        &self,
+        self_id: id::DeviceId,
+        adapter: &crate::instance::Adapter<A>,
+        desc: &resource::TextureDescriptor,
+    ) -> Result<resource::Texture<A>, resource::CreateTextureError> {
+        let hal_usage = conv::map_texture_usage(desc.usage, desc.format.into());
+        let hal_desc = hal::TextureDescriptor {
+            label: desc.label.borrow_option(),
+            size: desc.size,
+            mip_level_count: desc.mip_level_count,
+            sample_count: desc.sample_count,
+            dimension: desc.dimension,
+            format: desc.format,
+            usage: hal_usage,
+            memory_flags: hal::MemoryFlags::empty(),
+        };
+        let raw = unsafe {
+            self.raw
+                .create_texture(&hal_desc)
+                .map_err(DeviceError::from)?
+        };
+
+        self.create_texture_from_hal(raw, self_id, adapter, desc)
+    }
+
     fn create_texture_view(
         &self,
-        texture: &resource::Texture<B>,
+        texture: &resource::Texture<A>,
         texture_id: id::TextureId,
         desc: &resource::TextureViewDescriptor,
-    ) -> Result<resource::TextureView<B>, resource::CreateTextureViewError> {
-        let &(ref texture_raw, _) = texture
-            .raw
-            .as_ref()
+    ) -> Result<resource::TextureView<A>, resource::CreateTextureViewError> {
+        let texture_raw = texture
+            .inner
+            .as_raw()
             .ok_or(resource::CreateTextureViewError::InvalidTexture)?;
 
-        let view_dim =
-            match desc.dimension {
-                Some(dim) => {
-                    use hal::image::Kind;
-
-                    let required_tex_dim = dim.compatible_texture_dimension();
-
-                    if required_tex_dim != texture.dimension {
-                        return Err(
-                            resource::CreateTextureViewError::InvalidTextureViewDimension {
-                                view: dim,
-                                image: texture.dimension,
-                            },
-                        );
-                    }
-
-                    if let Kind::D2(_, _, depth, _) = texture.kind {
-                        match dim {
-                            TextureViewDimension::Cube if depth != 6 => {
-                                return Err(
-                                    resource::CreateTextureViewError::InvalidCubemapTextureDepth {
-                                        depth,
-                                    },
-                                )
-                            }
-                            TextureViewDimension::CubeArray if depth % 6 != 0 => return Err(
-                                resource::CreateTextureViewError::InvalidCubemapArrayTextureDepth {
-                                    depth,
-                                },
-                            ),
-                            _ => {}
-                        }
-                    }
-
-                    dim
+        let view_dim = match desc.dimension {
+            Some(dim) => {
+                if texture.desc.dimension != dim.compatible_texture_dimension() {
+                    return Err(
+                        resource::CreateTextureViewError::InvalidTextureViewDimension {
+                            view: dim,
+                            texture: texture.desc.dimension,
+                        },
+                    );
                 }
-                None => match texture.kind {
-                    hal::image::Kind::D1(..) => wgt::TextureViewDimension::D1,
-                    hal::image::Kind::D2(_, _, depth, _)
-                        if depth > 1 && desc.range.array_layer_count.is_none() =>
-                    {
-                        wgt::TextureViewDimension::D2Array
-                    }
-                    hal::image::Kind::D2(..) => wgt::TextureViewDimension::D2,
-                    hal::image::Kind::D3(..) => wgt::TextureViewDimension::D3,
-                },
-            };
+                dim
+            }
+            None => match texture.desc.dimension {
+                wgt::TextureDimension::D1 => wgt::TextureViewDimension::D1,
+                wgt::TextureDimension::D2
+                    if texture.desc.size.depth_or_array_layers > 1
+                        && desc.range.array_layer_count.is_none() =>
+                {
+                    wgt::TextureViewDimension::D2Array
+                }
+                wgt::TextureDimension::D2 => wgt::TextureViewDimension::D2,
+                wgt::TextureDimension::D3 => wgt::TextureViewDimension::D3,
+            },
+        };
 
         let required_level_count =
             desc.range.base_mip_level + desc.range.mip_level_count.map_or(1, |count| count.get());
-        let required_layer_count = desc.range.base_array_layer
-            + desc.range.array_layer_count.map_or(1, |count| count.get());
+        let required_layer_count = match desc.range.array_layer_count {
+            Some(count) => desc.range.base_array_layer + count.get(),
+            None => texture.desc.array_layer_count(),
+        };
         let level_end = texture.full_range.levels.end;
         let layer_end = texture.full_range.layers.end;
-        if required_level_count > level_end as u32 {
+        if required_level_count > level_end {
             return Err(resource::CreateTextureViewError::TooManyMipLevels {
                 requested: required_level_count,
                 total: level_end,
             });
         }
-        if required_layer_count > layer_end as u32 {
+        if required_layer_count > layer_end {
             return Err(resource::CreateTextureViewError::TooManyArrayLayers {
                 requested: required_layer_count,
                 total: layer_end,
             });
         };
 
-        let aspects = match desc.range.aspect {
-            wgt::TextureAspect::All => texture.aspects,
-            wgt::TextureAspect::DepthOnly => hal::format::Aspects::DEPTH,
-            wgt::TextureAspect::StencilOnly => hal::format::Aspects::STENCIL,
-        };
-        if !texture.aspects.contains(aspects) {
+        match view_dim {
+            TextureViewDimension::Cube if required_layer_count != 6 => {
+                return Err(
+                    resource::CreateTextureViewError::InvalidCubemapTextureDepth {
+                        depth: required_layer_count,
+                    },
+                )
+            }
+            TextureViewDimension::CubeArray if required_layer_count % 6 != 0 => {
+                return Err(
+                    resource::CreateTextureViewError::InvalidCubemapArrayTextureDepth {
+                        depth: required_layer_count,
+                    },
+                )
+            }
+            _ => {}
+        }
+
+        let full_aspect = hal::FormatAspects::from(texture.desc.format);
+        let select_aspect = hal::FormatAspects::from(desc.range.aspect);
+        if (full_aspect & select_aspect).is_empty() {
             return Err(resource::CreateTextureViewError::InvalidAspect {
-                requested: aspects,
-                total: texture.aspects,
+                texture_format: texture.desc.format,
+                requested_aspect: desc.range.aspect,
             });
         }
 
         let end_level = desc
             .range
             .mip_level_count
-            .map_or(level_end, |_| required_level_count as u8);
+            .map_or(level_end, |_| required_level_count);
         let end_layer = desc
             .range
             .array_layer_count
-            .map_or(layer_end, |_| required_layer_count as u16);
+            .map_or(layer_end, |_| required_layer_count);
         let selector = TextureSelector {
-            levels: desc.range.base_mip_level as u8..end_level,
-            layers: desc.range.base_array_layer as u16..end_layer,
+            levels: desc.range.base_mip_level..end_level,
+            layers: desc.range.base_array_layer..end_layer,
         };
 
-        let view_layer_count = (selector.layers.end - selector.layers.start) as u32;
+        let view_layer_count = selector.layers.end - selector.layers.start;
         let layer_check_ok = match view_dim {
             wgt::TextureViewDimension::D1
             | wgt::TextureViewDimension::D2
             | wgt::TextureViewDimension::D3 => view_layer_count == 1,
             wgt::TextureViewDimension::D2Array => true,
             wgt::TextureViewDimension::Cube => view_layer_count == 6,
             wgt::TextureViewDimension::CubeArray => view_layer_count % 6 == 0,
         };
         if !layer_check_ok {
             return Err(resource::CreateTextureViewError::InvalidArrayLayerCount {
                 requested: view_layer_count,
                 dim: view_dim,
             });
         }
 
-        let format = desc.format.unwrap_or(texture.format);
-        let range = hal::image::SubresourceRange {
-            aspects,
-            level_start: desc.range.base_mip_level as _,
-            level_count: desc.range.mip_level_count.map(|v| v.get() as _),
-            layer_start: desc.range.base_array_layer as _,
-            layer_count: desc.range.array_layer_count.map(|v| v.get() as _),
+        let mut extent = texture
+            .desc
+            .mip_level_size(desc.range.base_mip_level)
+            .unwrap();
+        if view_dim != wgt::TextureViewDimension::D3 {
+            extent.depth_or_array_layers = view_layer_count;
+        }
+        let format = desc.format.unwrap_or(texture.desc.format);
+        if format != texture.desc.format {
+            return Err(resource::CreateTextureViewError::FormatReinterpretation {
+                texture: texture.desc.format,
+                view: format,
+            });
+        }
+
+        // filter the usages based on the other criteria
+        let usage = {
+            let mask_copy = !(hal::TextureUses::COPY_SRC | hal::TextureUses::COPY_DST);
+            let mask_dimension = match view_dim {
+                wgt::TextureViewDimension::Cube | wgt::TextureViewDimension::CubeArray => {
+                    hal::TextureUses::RESOURCE
+                }
+                wgt::TextureViewDimension::D3 => {
+                    hal::TextureUses::RESOURCE
+                        | hal::TextureUses::STORAGE_READ
+                        | hal::TextureUses::STORAGE_WRITE
+                }
+                _ => hal::TextureUses::all(),
+            };
+            let mask_mip_level = if selector.levels.end - selector.levels.start != 1 {
+                hal::TextureUses::RESOURCE
+            } else {
+                hal::TextureUses::all()
+            };
+            texture.hal_usage & mask_copy & mask_dimension & mask_mip_level
         };
-        let hal_extent = texture
-            .kind
-            .extent()
-            .at_level(desc.range.base_mip_level as _);
+
+        log::debug!(
+            "Create view for texture {:?} filters usages to {:?}",
+            texture_id,
+            usage
+        );
+        let hal_desc = hal::TextureViewDescriptor {
+            label: desc.label.borrow_option(),
+            format,
+            dimension: view_dim,
+            usage,
+            range: desc.range.clone(),
+        };
 
         let raw = unsafe {
             self.raw
-                .create_image_view(
-                    texture_raw,
-                    conv::map_texture_view_dimension(view_dim),
-                    conv::map_texture_format(format, self.private_features),
-                    hal::format::Swizzle::NO,
-                    // conservatively assume the same usage
-                    conv::map_texture_usage(texture.usage, aspects),
-                    range,
-                )
-                .or(Err(resource::CreateTextureViewError::OutOfMemory))?
+                .create_texture_view(texture_raw, &hal_desc)
+                .map_err(|_| resource::CreateTextureViewError::OutOfMemory)?
         };
 
         Ok(resource::TextureView {
-            inner: resource::TextureViewInner::Native {
-                raw,
-                source_id: Stored {
-                    value: id::Valid(texture_id),
-                    ref_count: texture.life_guard.add_ref(),
-                },
+            raw,
+            parent_id: Stored {
+                value: id::Valid(texture_id),
+                ref_count: texture.life_guard.add_ref(),
             },
-            aspects,
-            format: texture.format,
+            desc: resource::HalTextureViewDescriptor {
+                format: hal_desc.format,
+                dimension: hal_desc.dimension,
+                range: hal_desc.range,
+            },
             format_features: texture.format_features,
-            dimension: view_dim,
-            extent: wgt::Extent3d {
-                width: hal_extent.width,
-                height: hal_extent.height,
-                depth_or_array_layers: view_layer_count,
-            },
-            samples: texture.kind.num_samples(),
-            framebuffer_attachment: texture.framebuffer_attachment.clone(),
+            extent,
+            samples: texture.desc.sample_count,
             // once a storage - forever a storage
-            sampled_internal_use: if texture.usage.contains(wgt::TextureUsage::STORAGE) {
-                resource::TextureUse::SAMPLED | resource::TextureUse::STORAGE_LOAD
+            sampled_internal_use: if texture
+                .desc
+                .usage
+                .contains(wgt::TextureUsages::STORAGE_BINDING)
+            {
+                hal::TextureUses::RESOURCE | hal::TextureUses::STORAGE_READ
             } else {
-                resource::TextureUse::SAMPLED
+                hal::TextureUses::RESOURCE
             },
             selector,
             life_guard: LifeGuard::new(desc.label.borrow_or_default()),
         })
     }
 
     fn create_sampler(
         &self,
         self_id: id::DeviceId,
         desc: &resource::SamplerDescriptor,
-    ) -> Result<resource::Sampler<B>, resource::CreateSamplerError> {
+    ) -> Result<resource::Sampler<A>, resource::CreateSamplerError> {
         if desc
             .address_modes
             .iter()
             .any(|am| am == &wgt::AddressMode::ClampToBorder)
         {
             self.require_features(wgt::Features::ADDRESS_MODE_CLAMP_TO_BORDER)?;
         }
 
-        let actual_clamp = if let Some(clamp) = desc.anisotropy_clamp {
+        let lod_clamp = if desc.lod_min_clamp > 0.0 || desc.lod_max_clamp < 32.0 {
+            Some(desc.lod_min_clamp..desc.lod_max_clamp)
+        } else {
+            None
+        };
+
+        let anisotropy_clamp = if let Some(clamp) = desc.anisotropy_clamp {
             let clamp = clamp.get();
-            let valid_clamp = clamp <= MAX_ANISOTROPY && conv::is_power_of_two(clamp as u32);
+            let valid_clamp = clamp <= hal::MAX_ANISOTROPY && conv::is_power_of_two(clamp as u32);
             if !valid_clamp {
                 return Err(resource::CreateSamplerError::InvalidClamp(clamp));
             }
-            if self.private_features.anisotropic_filtering {
-                Some(clamp)
+            if self
+                .downlevel
+                .flags
+                .contains(wgt::DownlevelFlags::ANISOTROPIC_FILTERING)
+            {
+                std::num::NonZeroU8::new(clamp)
             } else {
                 None
             }
         } else {
             None
         };
 
-        let border = match desc.border_color {
-            None | Some(wgt::SamplerBorderColor::TransparentBlack) => {
-                hal::image::BorderColor::TransparentBlack
-            }
-            Some(wgt::SamplerBorderColor::OpaqueBlack) => hal::image::BorderColor::OpaqueBlack,
-            Some(wgt::SamplerBorderColor::OpaqueWhite) => hal::image::BorderColor::OpaqueWhite,
-        };
-
-        let filtering = [desc.min_filter, desc.mag_filter, desc.mipmap_filter]
-            .contains(&wgt::FilterMode::Linear);
-
-        let info = hal::image::SamplerDesc {
-            min_filter: conv::map_filter(desc.min_filter),
-            mag_filter: conv::map_filter(desc.mag_filter),
-            mip_filter: conv::map_filter(desc.mipmap_filter),
-            reduction_mode: hal::image::ReductionMode::WeightedAverage,
-            wrap_mode: (
-                conv::map_wrap(desc.address_modes[0]),
-                conv::map_wrap(desc.address_modes[1]),
-                conv::map_wrap(desc.address_modes[2]),
-            ),
-            lod_bias: hal::image::Lod(0.0),
-            lod_range: hal::image::Lod(desc.lod_min_clamp)..hal::image::Lod(desc.lod_max_clamp),
-            comparison: desc.compare.map(conv::map_compare_function),
-            border,
-            normalized: true,
-            anisotropy_clamp: actual_clamp,
+        //TODO: check for wgt::DownlevelFlags::COMPARISON_SAMPLERS
+
+        let hal_desc = hal::SamplerDescriptor {
+            label: desc.label.borrow_option(),
+            address_modes: desc.address_modes,
+            mag_filter: desc.mag_filter,
+            min_filter: desc.min_filter,
+            mipmap_filter: desc.mipmap_filter,
+            lod_clamp,
+            compare: desc.compare,
+            anisotropy_clamp,
+            border_color: desc.border_color,
         };
 
         let raw = unsafe {
-            self.raw.create_sampler(&info).map_err(|err| match err {
-                hal::device::AllocationError::OutOfMemory(_) => {
-                    resource::CreateSamplerError::Device(DeviceError::OutOfMemory)
-                }
-                hal::device::AllocationError::TooManyObjects => {
-                    resource::CreateSamplerError::TooManyObjects
-                }
-            })?
+            self.raw
+                .create_sampler(&hal_desc)
+                .map_err(DeviceError::from)?
         };
         Ok(resource::Sampler {
             raw,
             device_id: Stored {
                 value: id::Valid(self_id),
                 ref_count: self.life_guard.add_ref(),
             },
             life_guard: LifeGuard::new(desc.label.borrow_or_default()),
-            comparison: info.comparison.is_some(),
-            filtering,
+            comparison: desc.compare.is_some(),
+            filtering: desc.min_filter == wgt::FilterMode::Linear
+                || desc.mag_filter == wgt::FilterMode::Linear,
         })
     }
 
     fn create_shader_module<'a>(
         &self,
         self_id: id::DeviceId,
         desc: &pipeline::ShaderModuleDescriptor<'a>,
         source: pipeline::ShaderModuleSource<'a>,
-    ) -> Result<pipeline::ShaderModule<B>, pipeline::CreateShaderModuleError> {
-        // First, try to produce a Naga module.
-        let (spv, module) = match source {
-            pipeline::ShaderModuleSource::SpirV(spv) => {
-                profiling::scope!("naga::spv::parse");
-                // Parse the given shader code and store its representation.
-                let options = naga::front::spv::Options {
-                    adjust_coordinate_space: false, // we require NDC_Y_UP feature
-                    strict_capabilities: true,
-                    flow_graph_dump_prefix: None,
-                };
-                let parser = naga::front::spv::Parser::new(spv.iter().cloned(), &options);
-                let module = match parser.parse() {
-                    Ok(module) => Some(module),
-                    Err(err) => {
-                        log::warn!(
-                            "Failed to parse shader SPIR-V code for {:?}: {:?}",
-                            desc.label,
-                            err
-                        );
-                        if desc.flags.contains(wgt::ShaderFlags::VALIDATION) {
-                            return Err(pipeline::CreateShaderModuleError::Parsing);
-                        }
-                        log::warn!("\tProceeding unsafely without validation");
-                        None
-                    }
-                };
-                (Some(spv), module)
-            }
+    ) -> Result<pipeline::ShaderModule<A>, pipeline::CreateShaderModuleError> {
+        let module = match source {
             pipeline::ShaderModuleSource::Wgsl(code) => {
                 profiling::scope!("naga::wgsl::parse_str");
                 // TODO: refactor the corresponding Naga error to be owned, and then
                 // display it instead of unwrapping
                 match naga::front::wgsl::parse_str(&code) {
-                    Ok(module) => (None, Some(module)),
+                    Ok(module) => module,
                     Err(err) => {
                         log::error!("Failed to parse WGSL code for {:?}: {}", desc.label, err);
-                        return Err(pipeline::CreateShaderModuleError::Parsing);
+                        return Err(pipeline::CreateShaderModuleError::Parsing(
+                            pipeline::NagaParseError {
+                                shader_source: code.to_string(),
+                                error: err,
+                            },
+                        ));
                     }
                 }
             }
-            pipeline::ShaderModuleSource::Naga(module) => (None, Some(module)),
+            pipeline::ShaderModuleSource::Naga(module) => module,
         };
 
-        let (naga_result, interface) = match module {
-            // If succeeded, then validate it and attempt to give it to gfx-hal directly.
-            Some(module) if desc.flags.contains(wgt::ShaderFlags::VALIDATION) || spv.is_none() => {
-                use naga::valid::Capabilities as Caps;
-                profiling::scope!("naga::validate");
-
-                let mut caps = Caps::empty();
-                caps.set(
-                    Caps::PUSH_CONSTANT,
-                    self.features.contains(wgt::Features::PUSH_CONSTANTS),
-                );
-                caps.set(
-                    Caps::FLOAT64,
-                    self.features.contains(wgt::Features::SHADER_FLOAT64),
-                );
-                let info = naga::valid::Validator::new(naga::valid::ValidationFlags::all(), caps)
-                    .validate(&module)?;
-                let interface = validation::Interface::new(&module, &info);
-                let shader = hal::device::NagaShader { module, info };
-
-                let naga_result = if desc
-                    .flags
-                    .contains(wgt::ShaderFlags::EXPERIMENTAL_TRANSLATION)
-                    || !cfg!(feature = "cross")
-                {
-                    match unsafe { self.raw.create_shader_module_from_naga(shader) } {
-                        Ok(raw) => Ok(raw),
-                        Err((hal::device::ShaderError::CompilationFailed(msg), shader)) => {
-                            log::warn!("Shader module compilation failed: {}", msg);
-                            Err(Some(shader))
-                        }
-                        Err((_, shader)) => Err(Some(shader)),
+        use naga::valid::Capabilities as Caps;
+        profiling::scope!("naga::validate");
+
+        let mut caps = Caps::empty();
+        caps.set(
+            Caps::PUSH_CONSTANT,
+            self.features.contains(wgt::Features::PUSH_CONSTANTS),
+        );
+        caps.set(
+            Caps::FLOAT64,
+            self.features.contains(wgt::Features::SHADER_FLOAT64),
+        );
+        caps.set(
+            Caps::PRIMITIVE_INDEX,
+            self.features
+                .contains(wgt::Features::SHADER_PRIMITIVE_INDEX),
+        );
+        let info = naga::valid::Validator::new(naga::valid::ValidationFlags::all(), caps)
+            .validate(&module)?;
+        let interface = validation::Interface::new(&module, &info, self.features);
+        let hal_shader = hal::ShaderInput::Naga(hal::NagaShader { module, info });
+
+        let hal_desc = hal::ShaderModuleDescriptor {
+            label: desc.label.borrow_option(),
+        };
+        let raw = match unsafe { self.raw.create_shader_module(&hal_desc, hal_shader) } {
+            Ok(raw) => raw,
+            Err(error) => {
+                return Err(match error {
+                    hal::ShaderError::Device(error) => {
+                        pipeline::CreateShaderModuleError::Device(error.into())
                     }
-                } else {
-                    Err(Some(shader))
-                };
-                (naga_result, Some(interface))
-            }
-            _ => (Err(None), None),
-        };
-
-        // Otherwise, fall back to SPIR-V.
-        let spv_result = match naga_result {
-            Ok(raw) => Ok(raw),
-            Err(maybe_shader) => {
-                let spv = match spv {
-                    Some(data) => Ok(data),
-                    None => {
-                        // Produce a SPIR-V from the Naga module
-                        profiling::scope!("naga::wpv::write_vec");
-                        let shader = maybe_shader.unwrap();
-                        naga::back::spv::write_vec(&shader.module, &shader.info, &self.spv_options)
-                            .map(Cow::Owned)
+                    hal::ShaderError::Compilation(ref msg) => {
+                        log::error!("Shader error: {}", msg);
+                        pipeline::CreateShaderModuleError::Generation
                     }
-                };
-                match spv {
-                    Ok(data) => unsafe { self.raw.create_shader_module(&data) },
-                    Err(e) => Err(hal::device::ShaderError::CompilationFailed(format!(
-                        "{}",
-                        e
-                    ))),
-                }
+                })
             }
         };
 
         Ok(pipeline::ShaderModule {
-            raw: match spv_result {
-                Ok(raw) => raw,
-                Err(hal::device::ShaderError::OutOfMemory(_)) => {
-                    return Err(DeviceError::OutOfMemory.into());
-                }
-                Err(error) => {
-                    log::error!("Shader error: {}", error);
-                    return Err(pipeline::CreateShaderModuleError::Generation);
-                }
-            },
+            raw,
             device_id: Stored {
                 value: id::Valid(self_id),
                 ref_count: self.life_guard.add_ref(),
             },
-            interface,
+            interface: Some(interface),
             #[cfg(debug_assertions)]
-            label: desc.label.to_string_or_default(),
+            label: desc.label.borrow_or_default().to_string(),
         })
     }
 
-    /// Create a compatible render pass with a given key.
-    ///
-    /// This functions doesn't consider the following aspects for compatibility:
-    ///  - image layouts
-    ///  - resolve attachments
-    fn create_compatible_render_pass(
+    #[allow(unused_unsafe)]
+    unsafe fn create_shader_module_spirv<'a>(
         &self,
-        key: &RenderPassKey,
-    ) -> Result<B::RenderPass, hal::device::OutOfMemory> {
-        let mut color_ids = [(0, hal::image::Layout::ColorAttachmentOptimal); MAX_COLOR_TARGETS];
-        for (index, color) in color_ids[..key.colors.len()].iter_mut().enumerate() {
-            color.0 = index;
-        }
-        let depth_id = key.depth_stencil.as_ref().map(|_| {
-            (
-                key.colors.len(),
-                hal::image::Layout::DepthStencilAttachmentOptimal,
-            )
-        });
-
-        let subpass = hal::pass::SubpassDesc {
-            colors: &color_ids[..key.colors.len()],
-            depth_stencil: depth_id.as_ref(),
-            inputs: &[],
-            resolves: &[],
-            preserves: &[],
+        self_id: id::DeviceId,
+        desc: &pipeline::ShaderModuleDescriptor<'a>,
+        source: &'a [u32],
+    ) -> Result<pipeline::ShaderModule<A>, pipeline::CreateShaderModuleError> {
+        self.require_features(wgt::Features::SPIRV_SHADER_PASSTHROUGH)?;
+        let hal_desc = hal::ShaderModuleDescriptor {
+            label: desc.label.borrow_option(),
         };
-        let all = key.all().map(|&(ref at, _)| at.clone());
-
-        unsafe {
-            self.raw
-                .create_render_pass(all, iter::once(subpass), iter::empty())
-        }
+        let hal_shader = hal::ShaderInput::SpirV(source);
+        let raw = match unsafe { self.raw.create_shader_module(&hal_desc, hal_shader) } {
+            Ok(raw) => raw,
+            Err(error) => {
+                return Err(match error {
+                    hal::ShaderError::Device(error) => {
+                        pipeline::CreateShaderModuleError::Device(error.into())
+                    }
+                    hal::ShaderError::Compilation(ref msg) => {
+                        log::error!("Shader error: {}", msg);
+                        pipeline::CreateShaderModuleError::Generation
+                    }
+                })
+            }
+        };
+
+        Ok(pipeline::ShaderModule {
+            raw,
+            device_id: Stored {
+                value: id::Valid(self_id),
+                ref_count: self.life_guard.add_ref(),
+            },
+            interface: None,
+            #[cfg(debug_assertions)]
+            label: desc.label.borrow_or_default().to_string(),
+        })
     }
 
     fn deduplicate_bind_group_layout(
         self_id: id::DeviceId,
         entry_map: &binding_model::BindEntryMap,
-        guard: &Storage<binding_model::BindGroupLayout<B>, id::BindGroupLayoutId>,
+        guard: &Storage<binding_model::BindGroupLayout<A>, id::BindGroupLayoutId>,
     ) -> Option<id::BindGroupLayoutId> {
         guard
             .iter(self_id.backend())
-            .find(|&(_, ref bgl)| bgl.device_id.value.0 == self_id && bgl.entries == *entry_map)
+            .find(|&(_, bgl)| bgl.device_id.value.0 == self_id && bgl.entries == *entry_map)
             .map(|(id, value)| {
                 value.multi_ref_count.inc();
                 id
             })
     }
 
     fn get_introspection_bind_group_layouts<'a>(
-        pipeline_layout: &binding_model::PipelineLayout<B>,
-        bgl_guard: &'a Storage<binding_model::BindGroupLayout<B>, id::BindGroupLayoutId>,
-    ) -> ArrayVec<[&'a binding_model::BindEntryMap; MAX_BIND_GROUPS]> {
+        pipeline_layout: &binding_model::PipelineLayout<A>,
+        bgl_guard: &'a Storage<binding_model::BindGroupLayout<A>, id::BindGroupLayoutId>,
+    ) -> ArrayVec<&'a binding_model::BindEntryMap, { hal::MAX_BIND_GROUPS }> {
         pipeline_layout
             .bind_group_layout_ids
             .iter()
             .map(|&id| &bgl_guard[id].entries)
             .collect()
     }
 
     fn create_bind_group_layout(
         &self,
         self_id: id::DeviceId,
         label: Option<&str>,
         entry_map: binding_model::BindEntryMap,
-    ) -> Result<binding_model::BindGroupLayout<B>, binding_model::CreateBindGroupLayoutError> {
-        let mut desc_count = descriptor::DescriptorTotalCount::default();
+    ) -> Result<binding_model::BindGroupLayout<A>, binding_model::CreateBindGroupLayoutError> {
+        #[derive(PartialEq)]
+        enum WritableStorage {
+            Yes,
+            No,
+        }
+
         for entry in entry_map.values() {
             use wgt::BindingType as Bt;
 
             let mut required_features = wgt::Features::empty();
-            let (counter, array_feature, is_writable_storage) = match entry.ty {
+            let mut required_downlevel_flags = wgt::DownlevelFlags::empty();
+            let (array_feature, writable_storage) = match entry.ty {
                 Bt::Buffer {
                     ty: wgt::BufferBindingType::Uniform,
                     has_dynamic_offset: false,
                     min_binding_size: _,
                 } => (
-                    &mut desc_count.uniform_buffer,
                     Some(wgt::Features::BUFFER_BINDING_ARRAY),
-                    false,
+                    WritableStorage::No,
                 ),
                 Bt::Buffer {
                     ty: wgt::BufferBindingType::Uniform,
                     has_dynamic_offset: true,
                     min_binding_size: _,
                 } => (
-                    &mut desc_count.uniform_buffer_dynamic,
                     Some(wgt::Features::BUFFER_BINDING_ARRAY),
-                    false,
+                    WritableStorage::No,
                 ),
                 Bt::Buffer {
                     ty: wgt::BufferBindingType::Storage { read_only },
-                    has_dynamic_offset,
-                    min_binding_size: _,
+                    ..
                 } => (
-                    if has_dynamic_offset {
-                        &mut desc_count.storage_buffer_dynamic
-                    } else {
-                        &mut desc_count.storage_buffer
-                    },
-                    Some(wgt::Features::BUFFER_BINDING_ARRAY),
-                    !read_only,
-                ),
-                Bt::Sampler { .. } => (&mut desc_count.sampler, None, false),
-                Bt::Texture { .. } => (
-                    &mut desc_count.sampled_image,
-                    Some(wgt::Features::SAMPLED_TEXTURE_BINDING_ARRAY),
-                    false,
-                ),
-                Bt::StorageTexture { access, .. } => (
-                    &mut desc_count.storage_image,
-                    None,
-                    match access {
-                        wgt::StorageTextureAccess::ReadOnly => false,
-                        wgt::StorageTextureAccess::WriteOnly => true,
-                        wgt::StorageTextureAccess::ReadWrite => {
-                            required_features |=
-                                wgt::Features::TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES;
-                            true
-                        }
+                    Some(
+                        wgt::Features::BUFFER_BINDING_ARRAY
+                            | wgt::Features::STORAGE_RESOURCE_BINDING_ARRAY,
+                    ),
+                    match read_only {
+                        true => WritableStorage::No,
+                        false => WritableStorage::Yes,
                     },
                 ),
+                Bt::Sampler { .. } => (None, WritableStorage::No),
+                Bt::Texture { .. } => (
+                    Some(wgt::Features::TEXTURE_BINDING_ARRAY),
+                    WritableStorage::No,
+                ),
+                Bt::StorageTexture {
+                    access,
+                    view_dimension,
+                    format: _,
+                } => {
+                    match view_dimension {
+                        wgt::TextureViewDimension::Cube | wgt::TextureViewDimension::CubeArray => {
+                            return Err(binding_model::CreateBindGroupLayoutError::Entry {
+                                binding: entry.binding,
+                                error: binding_model::BindGroupLayoutEntryError::StorageTextureCube,
+                            })
+                        }
+                        _ => (),
+                    }
+                    (
+                        Some(
+                            wgt::Features::TEXTURE_BINDING_ARRAY
+                                | wgt::Features::STORAGE_RESOURCE_BINDING_ARRAY,
+                        ),
+                        match access {
+                            wgt::StorageTextureAccess::WriteOnly => WritableStorage::Yes,
+                            wgt::StorageTextureAccess::ReadOnly => {
+                                required_features |=
+                                    wgt::Features::TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES;
+                                WritableStorage::No
+                            }
+                            wgt::StorageTextureAccess::ReadWrite => {
+                                required_features |=
+                                    wgt::Features::TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES;
+                                WritableStorage::Yes
+                            }
+                        },
+                    )
+                }
             };
 
-            *counter += match entry.count {
-                // Validate the count parameter
-                Some(count) => {
-                    required_features |= array_feature
-                        .ok_or(binding_model::BindGroupLayoutEntryError::ArrayUnsupported)
-                        .map_err(|error| binding_model::CreateBindGroupLayoutError::Entry {
-                            binding: entry.binding,
-                            error,
-                        })?;
-                    count.get()
+            // Validate the count parameter
+            if entry.count.is_some() {
+                required_features |= array_feature
+                    .ok_or(binding_model::BindGroupLayoutEntryError::ArrayUnsupported)
+                    .map_err(|error| binding_model::CreateBindGroupLayoutError::Entry {
+                        binding: entry.binding,
+                        error,
+                    })?;
+            }
+            if entry.visibility.contains(wgt::ShaderStages::VERTEX) {
+                if writable_storage == WritableStorage::Yes {
+                    required_features |= wgt::Features::VERTEX_WRITABLE_STORAGE;
                 }
-                None => 1,
-            };
-            if is_writable_storage && entry.visibility.contains(wgt::ShaderStage::VERTEX) {
-                required_features |= wgt::Features::VERTEX_WRITABLE_STORAGE;
+                if let Bt::Buffer {
+                    ty: wgt::BufferBindingType::Storage { .. },
+                    ..
+                } = entry.ty
+                {
+                    required_downlevel_flags |= wgt::DownlevelFlags::VERTEX_STORAGE;
+                }
+            }
+            if writable_storage == WritableStorage::Yes
+                && entry.visibility.contains(wgt::ShaderStages::FRAGMENT)
+            {
+                required_downlevel_flags |= wgt::DownlevelFlags::FRAGMENT_WRITABLE_STORAGE;
             }
 
             self.require_features(required_features)
                 .map_err(binding_model::BindGroupLayoutEntryError::MissingFeatures)
                 .map_err(|error| binding_model::CreateBindGroupLayoutError::Entry {
                     binding: entry.binding,
                     error,
                 })?;
+            self.require_downlevel_flags(required_downlevel_flags)
+                .map_err(binding_model::BindGroupLayoutEntryError::MissingDownlevelFlags)
+                .map_err(|error| binding_model::CreateBindGroupLayoutError::Entry {
+                    binding: entry.binding,
+                    error,
+                })?;
         }
 
-        let raw_bindings = entry_map
-            .values()
-            .map(|entry| hal::pso::DescriptorSetLayoutBinding {
-                binding: entry.binding,
-                ty: conv::map_binding_type(entry),
-                count: entry
-                    .count
-                    .map_or(1, |v| v.get() as hal::pso::DescriptorArrayIndex), //TODO: consolidate
-                stage_flags: conv::map_shader_stage_flags(entry.visibility),
-                immutable_samplers: false, // TODO
-            });
+        let mut hal_bindings = entry_map.values().cloned().collect::<Vec<_>>();
+        hal_bindings.sort_by_key(|b| b.binding);
+        let hal_desc = hal::BindGroupLayoutDescriptor {
+            label,
+            entries: &hal_bindings,
+        };
         let raw = unsafe {
-            let mut raw_layout = self
-                .raw
-                .create_descriptor_set_layout(raw_bindings, iter::empty())
-                .or(Err(DeviceError::OutOfMemory))?;
-            if let Some(label) = label {
-                self.raw
-                    .set_descriptor_set_layout_name(&mut raw_layout, label);
-            }
-            raw_layout
+            self.raw
+                .create_bind_group_layout(&hal_desc)
+                .map_err(DeviceError::from)?
         };
 
         let mut count_validator = binding_model::BindingTypeMaxCountValidator::default();
         for entry in entry_map.values() {
             count_validator.add_binding(entry);
         }
         // If a single bind group layout violates limits, the pipeline layout is definitely
         // going to violate limits too, lets catch it now.
@@ -1320,39 +1198,38 @@ impl<B: GfxBackend> Device<B> {
 
         Ok(binding_model::BindGroupLayout {
             raw,
             device_id: Stored {
                 value: id::Valid(self_id),
                 ref_count: self.life_guard.add_ref(),
             },
             multi_ref_count: MultiRefCount::new(),
-            desc_count,
             dynamic_count: entry_map
                 .values()
                 .filter(|b| b.ty.has_dynamic_offset())
                 .count(),
             count_validator,
             entries: entry_map,
             #[cfg(debug_assertions)]
             label: label.unwrap_or("").to_string(),
         })
     }
 
     #[allow(clippy::too_many_arguments)]
-    fn create_buffer_descriptor<'a>(
+    fn create_buffer_binding<'a>(
         bb: &binding_model::BufferBinding,
         binding: u32,
         decl: &wgt::BindGroupLayoutEntry,
-        used_buffer_ranges: &mut Vec<MemoryInitTrackerAction<id::BufferId>>,
+        used_buffer_ranges: &mut Vec<BufferInitTrackerAction>,
         dynamic_binding_info: &mut Vec<binding_model::BindGroupDynamicBindingData>,
         used: &mut TrackerSet,
-        storage: &'a Storage<resource::Buffer<B>, id::BufferId>,
+        storage: &'a Storage<resource::Buffer<A>, id::BufferId>,
         limits: &wgt::Limits,
-    ) -> Result<hal::pso::Descriptor<'a, B>, binding_model::CreateBindGroupError> {
+    ) -> Result<hal::BufferBinding<'a, A>, binding_model::CreateBindGroupError> {
         use crate::binding_model::CreateBindGroupError as Error;
 
         let (binding_ty, dynamic, min_size) = match decl.ty {
             wgt::BindingType::Buffer {
                 ty,
                 has_dynamic_offset,
                 min_binding_size,
             } => (ty, has_dynamic_offset, min_binding_size),
@@ -1361,41 +1238,41 @@ impl<B: GfxBackend> Device<B> {
                     binding,
                     actual: decl.ty,
                     expected: "UniformBuffer, StorageBuffer or ReadonlyStorageBuffer",
                 })
             }
         };
         let (pub_usage, internal_use, range_limit) = match binding_ty {
             wgt::BufferBindingType::Uniform => (
-                wgt::BufferUsage::UNIFORM,
-                resource::BufferUse::UNIFORM,
+                wgt::BufferUsages::UNIFORM,
+                hal::BufferUses::UNIFORM,
                 limits.max_uniform_buffer_binding_size,
             ),
             wgt::BufferBindingType::Storage { read_only } => (
-                wgt::BufferUsage::STORAGE,
+                wgt::BufferUsages::STORAGE,
                 if read_only {
-                    resource::BufferUse::STORAGE_LOAD
+                    hal::BufferUses::STORAGE_READ
                 } else {
-                    resource::BufferUse::STORAGE_STORE
+                    hal::BufferUses::STORAGE_WRITE
                 },
                 limits.max_storage_buffer_binding_size,
             ),
         };
 
         if bb.offset % wgt::BIND_BUFFER_ALIGNMENT != 0 {
             return Err(Error::UnalignedBufferOffset(bb.offset));
         }
 
         let buffer = used
             .buffers
             .use_extend(storage, bb.buffer_id, (), internal_use)
             .map_err(|_| Error::InvalidBuffer(bb.buffer_id))?;
         check_buffer_usage(buffer.usage, pub_usage)?;
-        let &(ref buffer_raw, _) = buffer
+        let raw_buffer = buffer
             .raw
             .as_ref()
             .ok_or(Error::InvalidBuffer(bb.buffer_id))?;
 
         let (bind_size, bind_end) = match bb.size {
             Some(size) => {
                 let end = bb.offset + size.get();
                 if end > buffer.size {
@@ -1433,113 +1310,117 @@ impl<B: GfxBackend> Device<B> {
                     actual: bind_size,
                     min: min_size,
                 });
             }
         } else if bind_size == 0 {
             return Err(Error::BindingZeroSize(bb.buffer_id));
         }
 
-        used_buffer_ranges.push(MemoryInitTrackerAction {
-            id: bb.buffer_id,
-            range: bb.offset..(bb.offset + bind_size),
-            kind: MemoryInitKind::NeedsInitializedMemory,
-        });
-
-        let sub_range = hal::buffer::SubRange {
+        used_buffer_ranges.extend(buffer.initialization_status.create_action(
+            bb.buffer_id,
+            bb.offset..(bb.offset + bind_size),
+            MemoryInitKind::NeedsInitializedMemory,
+        ));
+
+        Ok(hal::BufferBinding {
+            buffer: raw_buffer,
             offset: bb.offset,
-            size: Some(bind_size),
-        };
-        Ok(hal::pso::Descriptor::Buffer(buffer_raw, sub_range))
+            size: bb.size,
+        })
     }
 
     fn create_bind_group<G: GlobalIdentityHandlerFactory>(
         &self,
         self_id: id::DeviceId,
-        layout: &binding_model::BindGroupLayout<B>,
+        layout: &binding_model::BindGroupLayout<A>,
         desc: &binding_model::BindGroupDescriptor,
-        hub: &Hub<B, G>,
-        token: &mut Token<binding_model::BindGroupLayout<B>>,
-    ) -> Result<binding_model::BindGroup<B>, binding_model::CreateBindGroupError> {
+        hub: &Hub<A, G>,
+        token: &mut Token<binding_model::BindGroupLayout<A>>,
+    ) -> Result<binding_model::BindGroup<A>, binding_model::CreateBindGroupError> {
         use crate::binding_model::{BindingResource as Br, CreateBindGroupError as Error};
         {
             // Check that the number of entries in the descriptor matches
             // the number of entries in the layout.
             let actual = desc.entries.len();
             let expected = layout.entries.len();
             if actual != expected {
                 return Err(Error::BindingsNumMismatch { expected, actual });
             }
         }
 
         // TODO: arrayvec/smallvec
         // Record binding info for dynamic offset validation
         let mut dynamic_binding_info = Vec::new();
         // fill out the descriptors
-        let mut used = TrackerSet::new(B::VARIANT);
+        let mut used = TrackerSet::new(A::VARIANT);
 
         let (buffer_guard, mut token) = hub.buffers.read(token);
         let (texture_guard, mut token) = hub.textures.read(&mut token); //skip token
         let (texture_view_guard, mut token) = hub.texture_views.read(&mut token);
         let (sampler_guard, _) = hub.samplers.read(&mut token);
 
-        // `BTreeMap` has ordered bindings as keys, which allows us to coalesce
-        // the descriptor writes into a single transaction.
-        let mut write_map = BTreeMap::new();
         let mut used_buffer_ranges = Vec::new();
+        let mut hal_entries = Vec::with_capacity(desc.entries.len());
+        let mut hal_buffers = Vec::new();
+        let mut hal_samplers = Vec::new();
+        let mut hal_textures = Vec::new();
         for entry in desc.entries.iter() {
             let binding = entry.binding;
             // Find the corresponding declaration in the layout
             let decl = layout
                 .entries
                 .get(&binding)
                 .ok_or(Error::MissingBindingDeclaration(binding))?;
-            let descriptors: SmallVec<[_; 1]> = match entry.resource {
+            let res_index = match entry.resource {
                 Br::Buffer(ref bb) => {
-                    let buffer_desc = Self::create_buffer_descriptor(
-                        &bb,
+                    let bb = Self::create_buffer_binding(
+                        bb,
                         binding,
-                        &decl,
+                        decl,
                         &mut used_buffer_ranges,
                         &mut dynamic_binding_info,
                         &mut used,
                         &*buffer_guard,
                         &self.limits,
                     )?;
-                    SmallVec::from([buffer_desc])
+
+                    let res_index = hal_buffers.len();
+                    hal_buffers.push(bb);
+                    res_index
                 }
                 Br::BufferArray(ref bindings_array) => {
                     if let Some(count) = decl.count {
                         let count = count.get() as usize;
                         let num_bindings = bindings_array.len();
                         if count != num_bindings {
                             return Err(Error::BindingArrayLengthMismatch {
                                 actual: num_bindings,
                                 expected: count,
                             });
                         }
                     } else {
                         return Err(Error::SingleBindingExpected);
                     }
 
-                    bindings_array
-                        .iter()
-                        .map(|bb| {
-                            Self::create_buffer_descriptor(
-                                &bb,
-                                binding,
-                                &decl,
-                                &mut used_buffer_ranges,
-                                &mut dynamic_binding_info,
-                                &mut used,
-                                &*buffer_guard,
-                                &self.limits,
-                            )
-                        })
-                        .collect::<Result<_, _>>()?
+                    let res_index = hal_buffers.len();
+                    for bb in bindings_array.iter() {
+                        let bb = Self::create_buffer_binding(
+                            bb,
+                            binding,
+                            decl,
+                            &mut used_buffer_ranges,
+                            &mut dynamic_binding_info,
+                            &mut used,
+                            &*buffer_guard,
+                            &self.limits,
+                        )?;
+                        hal_buffers.push(bb);
+                    }
+                    res_index
                 }
                 Br::Sampler(id) => {
                     match decl.ty {
                         wgt::BindingType::Sampler {
                             filtering,
                             comparison,
                         } => {
                             let sampler = used
@@ -1559,286 +1440,296 @@ impl<B: GfxBackend> Device<B> {
                             if sampler.filtering && !filtering {
                                 return Err(Error::WrongSamplerFiltering {
                                     binding,
                                     layout_flt: filtering,
                                     sampler_flt: sampler.filtering,
                                 });
                             }
 
-                            SmallVec::from([hal::pso::Descriptor::Sampler(&sampler.raw)])
+                            let res_index = hal_samplers.len();
+                            hal_samplers.push(&sampler.raw);
+                            res_index
                         }
                         _ => {
                             return Err(Error::WrongBindingType {
                                 binding,
                                 actual: decl.ty,
                                 expected: "Sampler",
                             })
                         }
                     }
                 }
                 Br::TextureView(id) => {
                     let view = used
                         .views
                         .use_extend(&*texture_view_guard, id, (), ())
                         .map_err(|_| Error::InvalidTextureView(id))?;
-                    let format_info = view.format.describe();
-                    let (pub_usage, internal_use) = match decl.ty {
-                        wgt::BindingType::Texture {
-                            sample_type,
-                            view_dimension,
-                            multisampled,
-                        } => {
-                            use wgt::TextureSampleType as Tst;
-                            if multisampled != (view.samples != 1) {
-                                return Err(Error::InvalidTextureMultisample {
-                                    binding,
-                                    layout_multisampled: multisampled,
-                                    view_samples: view.samples as u32,
-                                });
-                            }
-                            match (sample_type, format_info.sample_type, view.format_features.filterable ) {
-                                (Tst::Uint, Tst::Uint, ..) |
-                                (Tst::Sint, Tst::Sint, ..) |
-                                (Tst::Depth, Tst::Depth, ..) |
-                                // if we expect non-filterable, accept anything float
-                                (Tst::Float { filterable: false }, Tst::Float { .. }, ..) |
-                                // if we expect filterable, require it
-                                (Tst::Float { filterable: true }, Tst::Float { filterable: true }, ..) |
-                                // if we expect filterable, also accept Float that is defined as unfilterable if filterable feature is explicitly enabled
-                                // (only hit if wgt::Features::TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES is enabled)
-                                (Tst::Float { filterable: true }, Tst::Float { .. }, true) |
-                                // if we expect float, also accept depth
-                                (Tst::Float { .. }, Tst::Depth, ..) => {}
-                                _ => {
-                                    return Err(Error::InvalidTextureSampleType {
-                                    binding,
-                                    layout_sample_type: sample_type,
-                                    view_format: view.format,
-                                })
-                            },
-                            }
-                            if view_dimension != view.dimension {
-                                return Err(Error::InvalidTextureDimension {
-                                    binding,
-                                    layout_dimension: view_dimension,
-                                    view_dimension: view.dimension,
-                                });
-                            }
-                            (wgt::TextureUsage::SAMPLED, view.sampled_internal_use)
-                        }
-                        wgt::BindingType::StorageTexture {
-                            access,
-                            format,
-                            view_dimension,
-                        } => {
-                            if format != view.format {
-                                return Err(Error::InvalidStorageTextureFormat {
-                                    binding,
-                                    layout_format: format,
-                                    view_format: view.format,
-                                });
-                            }
-                            if view_dimension != view.dimension {
-                                return Err(Error::InvalidTextureDimension {
-                                    binding,
-                                    layout_dimension: view_dimension,
-                                    view_dimension: view.dimension,
-                                });
-                            }
-                            let internal_use = match access {
-                                wgt::StorageTextureAccess::ReadOnly => {
-                                    resource::TextureUse::STORAGE_LOAD
-                                }
-                                wgt::StorageTextureAccess::WriteOnly => {
-                                    resource::TextureUse::STORAGE_STORE
-                                }
-                                wgt::StorageTextureAccess::ReadWrite => {
-                                    if !view.format_features.flags.contains(
-                                        wgt::TextureFormatFeatureFlags::STORAGE_READ_WRITE,
-                                    ) {
-                                        return Err(Error::StorageReadWriteNotSupported(
-                                            view.format,
-                                        ));
-                                    }
-
-                                    resource::TextureUse::STORAGE_STORE
-                                        | resource::TextureUse::STORAGE_LOAD
-                                }
-                            };
-                            (wgt::TextureUsage::STORAGE, internal_use)
-                        }
-                        _ => return Err(Error::WrongBindingType {
-                            binding,
-                            actual: decl.ty,
-                            expected:
-                                "SampledTexture, ReadonlyStorageTexture or WriteonlyStorageTexture",
-                        }),
-                    };
-                    if view
-                        .aspects
-                        .contains(hal::format::Aspects::DEPTH | hal::format::Aspects::STENCIL)
-                    {
-                        return Err(Error::DepthStencilAspect);
-                    }
-                    match view.inner {
-                        resource::TextureViewInner::Native {
-                            ref raw,
-                            ref source_id,
-                        } => {
-                            // Careful here: the texture may no longer have its own ref count,
-                            // if it was deleted by the user.
-                            let texture = &texture_guard[source_id.value];
-                            used.textures
-                                .change_extend(
-                                    source_id.value,
-                                    &source_id.ref_count,
-                                    view.selector.clone(),
-                                    internal_use,
-                                )
-                                .map_err(UsageConflict::from)?;
-                            check_texture_usage(texture.usage, pub_usage)?;
-                            let image_layout =
-                                conv::map_texture_state(internal_use, view.aspects).1;
-                            SmallVec::from([hal::pso::Descriptor::Image(raw, image_layout)])
-                        }
-                        resource::TextureViewInner::SwapChain { .. } => {
-                            return Err(Error::SwapChainImage);
-                        }
-                    }
+                    let (pub_usage, internal_use) = Self::texture_use_parameters(
+                        binding,
+                        decl,
+                        view,
+                        "SampledTexture, ReadonlyStorageTexture or WriteonlyStorageTexture",
+                    )?;
+
+                    // Careful here: the texture may no longer have its own ref count,
+                    // if it was deleted by the user.
+                    used.textures
+                        .change_extend(
+                            view.parent_id.value,
+                            &view.parent_id.ref_count,
+                            view.selector.clone(),
+                            internal_use,
+                        )
+                        .map_err(UsageConflict::from)?;
+                    let texture = &texture_guard[view.parent_id.value];
+                    check_texture_usage(texture.desc.usage, pub_usage)?;
+
+                    let res_index = hal_textures.len();
+                    hal_textures.push(hal::TextureBinding {
+                        view: &view.raw,
+                        usage: internal_use,
+                    });
+                    res_index
                 }
                 Br::TextureViewArray(ref bindings_array) => {
                     if let Some(count) = decl.count {
                         let count = count.get() as usize;
                         let num_bindings = bindings_array.len();
                         if count != num_bindings {
                             return Err(Error::BindingArrayLengthMismatch {
                                 actual: num_bindings,
                                 expected: count,
                             });
                         }
                     } else {
                         return Err(Error::SingleBindingExpected);
                     }
 
-                    bindings_array
-                        .iter()
-                        .map(|&id| {
-                            let view = used
-                                .views
-                                .use_extend(&*texture_view_guard, id, (), ())
-                                .map_err(|_| Error::InvalidTextureView(id))?;
-                            let (pub_usage, internal_use) = match decl.ty {
-                                wgt::BindingType::Texture { .. } => {
-                                    (wgt::TextureUsage::SAMPLED, view.sampled_internal_use)
-                                }
-                                _ => {
-                                    return Err(Error::WrongBindingType {
-                                        binding,
-                                        actual: decl.ty,
-                                        expected: "SampledTextureArray",
-                                    })
-                                }
-                            };
-                            match view.inner {
-                                resource::TextureViewInner::Native {
-                                    ref raw,
-                                    ref source_id,
-                                } => {
-                                    // Careful here: the texture may no longer have its own ref count,
-                                    // if it was deleted by the user.
-                                    let texture = &texture_guard[source_id.value];
-                                    used.textures
-                                        .change_extend(
-                                            source_id.value,
-                                            &source_id.ref_count,
-                                            view.selector.clone(),
-                                            internal_use,
-                                        )
-                                        .map_err(UsageConflict::from)?;
-                                    check_texture_usage(texture.usage, pub_usage)?;
-                                    let image_layout =
-                                        conv::map_texture_state(internal_use, view.aspects).1;
-                                    Ok(hal::pso::Descriptor::Image(raw, image_layout))
-                                }
-                                resource::TextureViewInner::SwapChain { .. } => {
-                                    Err(Error::SwapChainImage)
-                                }
-                            }
-                        })
-                        .collect::<Result<_, _>>()?
+                    let res_index = hal_textures.len();
+                    for &id in bindings_array.iter() {
+                        let view = used
+                            .views
+                            .use_extend(&*texture_view_guard, id, (), ())
+                            .map_err(|_| Error::InvalidTextureView(id))?;
+                        let (pub_usage, internal_use) = Self::texture_use_parameters(
+                            binding, decl, view,
+                            "SampledTextureArray, ReadonlyStorageTextureArray or WriteonlyStorageTextureArray"
+                        )?;
+
+                        // Careful here: the texture may no longer have its own ref count,
+                        // if it was deleted by the user.
+                        used.textures
+                            .change_extend(
+                                view.parent_id.value,
+                                &view.parent_id.ref_count,
+                                view.selector.clone(),
+                                internal_use,
+                            )
+                            .map_err(UsageConflict::from)?;
+                        let texture = &texture_guard[view.parent_id.value];
+                        check_texture_usage(texture.desc.usage, pub_usage)?;
+
+                        hal_textures.push(hal::TextureBinding {
+                            view: &view.raw,
+                            usage: internal_use,
+                        });
+                    }
+
+                    res_index
                 }
             };
-            if write_map.insert(binding, descriptors).is_some() {
-                return Err(Error::DuplicateBinding(binding));
-            }
+
+            hal_entries.push(hal::BindGroupEntry {
+                binding,
+                resource_index: res_index as u32,
+            });
         }
 
-        let mut desc_sets =
-            self.desc_allocator
-                .lock()
-                .allocate(&self.raw, &layout.raw, &layout.desc_count, 1)?;
-        let mut desc_set = desc_sets.pop().unwrap();
-
-        // Set the descriptor set's label for easier debugging.
-        if let Some(label) = desc.label.as_ref() {
-            unsafe {
-                self.raw.set_descriptor_set_name(desc_set.raw_mut(), &label);
+        hal_entries.sort_by_key(|entry| entry.binding);
+        for (a, b) in hal_entries.iter().zip(hal_entries.iter().skip(1)) {
+            if a.binding == b.binding {
+                return Err(Error::DuplicateBinding(a.binding));
             }
         }
 
-        if let Some(start_binding) = write_map.keys().next().cloned() {
-            let descriptors = write_map.into_iter().flat_map(|(_, list)| list);
-            unsafe {
-                let write = hal::pso::DescriptorSetWrite {
-                    set: desc_set.raw_mut(),
-                    binding: start_binding,
-                    array_offset: 0,
-                    descriptors,
-                };
-                self.raw.write_descriptor_set(write);
-            }
-        }
+        let hal_desc = hal::BindGroupDescriptor {
+            label: desc.label.borrow_option(),
+            layout: &layout.raw,
+            entries: &hal_entries,
+            buffers: &hal_buffers,
+            samplers: &hal_samplers,
+            textures: &hal_textures,
+        };
+        let raw = unsafe {
+            self.raw
+                .create_bind_group(&hal_desc)
+                .map_err(DeviceError::from)?
+        };
 
         Ok(binding_model::BindGroup {
-            raw: desc_set,
+            raw,
             device_id: Stored {
                 value: id::Valid(self_id),
                 ref_count: self.life_guard.add_ref(),
             },
             layout_id: id::Valid(desc.layout),
             life_guard: LifeGuard::new(desc.label.borrow_or_default()),
             used,
             used_buffer_ranges,
             dynamic_binding_info,
         })
     }
 
+    fn texture_use_parameters(
+        binding: u32,
+        decl: &wgt::BindGroupLayoutEntry,
+        view: &crate::resource::TextureView<A>,
+        expected: &'static str,
+    ) -> Result<(wgt::TextureUsages, hal::TextureUses), binding_model::CreateBindGroupError> {
+        use crate::binding_model::CreateBindGroupError as Error;
+        if view
+            .desc
+            .aspects()
+            .contains(hal::FormatAspects::DEPTH | hal::FormatAspects::STENCIL)
+        {
+            return Err(Error::DepthStencilAspect);
+        }
+        let format_info = view.desc.format.describe();
+        match decl.ty {
+            wgt::BindingType::Texture {
+                sample_type,
+                view_dimension,
+                multisampled,
+            } => {
+                use wgt::TextureSampleType as Tst;
+                if multisampled != (view.samples != 1) {
+                    return Err(Error::InvalidTextureMultisample {
+                        binding,
+                        layout_multisampled: multisampled,
+                        view_samples: view.samples,
+                    });
+                }
+                match (sample_type, format_info.sample_type, view.format_features.filterable) {
+                    (Tst::Uint, Tst::Uint, ..) |
+                    (Tst::Sint, Tst::Sint, ..) |
+                    (Tst::Depth, Tst::Depth, ..) |
+                    // if we expect non-filterable, accept anything float
+                    (Tst::Float { filterable: false }, Tst::Float { .. }, ..) |
+                    // if we expect filterable, require it
+                    (Tst::Float { filterable: true }, Tst::Float { filterable: true }, ..) |
+                    // if we expect filterable, also accept Float that is defined as unfilterable if filterable feature is explicitly enabled
+                    // (only hit if wgt::Features::TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES is enabled)
+                    (Tst::Float { filterable: true }, Tst::Float { .. }, true) |
+                    // if we expect float, also accept depth
+                    (Tst::Float { .. }, Tst::Depth, ..) => {}
+                    _ => {
+                        return Err(Error::InvalidTextureSampleType {
+                            binding,
+                            layout_sample_type: sample_type,
+                            view_format: view.desc.format,
+                        })
+                    }
+                }
+                if view_dimension != view.desc.dimension {
+                    return Err(Error::InvalidTextureDimension {
+                        binding,
+                        layout_dimension: view_dimension,
+                        view_dimension: view.desc.dimension,
+                    });
+                }
+                Ok((
+                    wgt::TextureUsages::TEXTURE_BINDING,
+                    view.sampled_internal_use,
+                ))
+            }
+            wgt::BindingType::StorageTexture {
+                access,
+                format,
+                view_dimension,
+            } => {
+                if format != view.desc.format {
+                    return Err(Error::InvalidStorageTextureFormat {
+                        binding,
+                        layout_format: format,
+                        view_format: view.desc.format,
+                    });
+                }
+                if view_dimension != view.desc.dimension {
+                    return Err(Error::InvalidTextureDimension {
+                        binding,
+                        layout_dimension: view_dimension,
+                        view_dimension: view.desc.dimension,
+                    });
+                }
+
+                let mip_level_count = view.selector.levels.end - view.selector.levels.start;
+                if mip_level_count != 1 {
+                    return Err(Error::InvalidStorageTextureMipLevelCount {
+                        binding,
+                        mip_level_count,
+                    });
+                }
+
+                let internal_use = match access {
+                    wgt::StorageTextureAccess::WriteOnly => hal::TextureUses::STORAGE_WRITE,
+                    wgt::StorageTextureAccess::ReadOnly => {
+                        if !view
+                            .format_features
+                            .flags
+                            .contains(wgt::TextureFormatFeatureFlags::STORAGE_READ_WRITE)
+                        {
+                            return Err(Error::StorageReadNotSupported(view.desc.format));
+                        }
+                        hal::TextureUses::STORAGE_READ
+                    }
+                    wgt::StorageTextureAccess::ReadWrite => {
+                        if !view
+                            .format_features
+                            .flags
+                            .contains(wgt::TextureFormatFeatureFlags::STORAGE_READ_WRITE)
+                        {
+                            return Err(Error::StorageReadNotSupported(view.desc.format));
+                        }
+
+                        hal::TextureUses::STORAGE_WRITE | hal::TextureUses::STORAGE_READ
+                    }
+                };
+                Ok((wgt::TextureUsages::STORAGE_BINDING, internal_use))
+            }
+            _ => Err(Error::WrongBindingType {
+                binding,
+                actual: decl.ty,
+                expected,
+            }),
+        }
+    }
+
     fn create_pipeline_layout(
         &self,
         self_id: id::DeviceId,
         desc: &binding_model::PipelineLayoutDescriptor,
-        bgl_guard: &Storage<binding_model::BindGroupLayout<B>, id::BindGroupLayoutId>,
-    ) -> Result<binding_model::PipelineLayout<B>, binding_model::CreatePipelineLayoutError> {
+        bgl_guard: &Storage<binding_model::BindGroupLayout<A>, id::BindGroupLayoutId>,
+    ) -> Result<binding_model::PipelineLayout<A>, binding_model::CreatePipelineLayoutError> {
         use crate::binding_model::CreatePipelineLayoutError as Error;
 
         let bind_group_layouts_count = desc.bind_group_layouts.len();
         let device_max_bind_groups = self.limits.max_bind_groups as usize;
         if bind_group_layouts_count > device_max_bind_groups {
             return Err(Error::TooManyGroups {
                 actual: bind_group_layouts_count,
                 max: device_max_bind_groups,
             });
         }
 
         if !desc.push_constant_ranges.is_empty() {
             self.require_features(wgt::Features::PUSH_CONSTANTS)?;
         }
 
-        let mut used_stages = wgt::ShaderStage::empty();
+        let mut used_stages = wgt::ShaderStages::empty();
         for (index, pc) in desc.push_constant_ranges.iter().enumerate() {
             if pc.stages.intersects(used_stages) {
                 return Err(Error::MoreThanOnePushConstantRangePerStage {
                     index,
                     provided: pc.stages,
                     intersected: pc.stages & used_stages,
                 });
             }
@@ -1875,35 +1766,32 @@ impl<B: GfxBackend> Device<B> {
                 .get(id)
                 .map_err(|_| Error::InvalidBindGroupLayout(id))?;
             count_validator.merge(&bind_group_layout.count_validator);
         }
         count_validator
             .validate(&self.limits)
             .map_err(Error::TooManyBindings)?;
 
-        let descriptor_set_layouts = desc
+        let bgl_vec = desc
             .bind_group_layouts
             .iter()
-            .map(|&id| &bgl_guard.get(id).unwrap().raw);
-        let push_constants = desc
-            .push_constant_ranges
-            .iter()
-            .map(|pc| (conv::map_shader_stage_flags(pc.stages), pc.range.clone()));
+            .map(|&id| &bgl_guard.get(id).unwrap().raw)
+            .collect::<Vec<_>>();
+        let hal_desc = hal::PipelineLayoutDescriptor {
+            label: desc.label.borrow_option(),
+            flags: hal::PipelineLayoutFlags::BASE_VERTEX_INSTANCE,
+            bind_group_layouts: &bgl_vec,
+            push_constant_ranges: desc.push_constant_ranges.as_ref(),
+        };
 
         let raw = unsafe {
-            let raw_layout = self
-                .raw
-                .create_pipeline_layout(descriptor_set_layouts, push_constants)
-                .or(Err(DeviceError::OutOfMemory))?;
-            if let Some(_) = desc.label {
-                //TODO-0.6: needs gfx changes published
-                //self.raw.set_pipeline_layout_name(&mut raw_layout, label);
-            }
-            raw_layout
+            self.raw
+                .create_pipeline_layout(&hal_desc)
+                .map_err(DeviceError::from)?
         };
 
         Ok(binding_model::PipelineLayout {
             raw,
             device_id: Stored {
                 value: id::Valid(self_id),
                 ref_count: self.life_guard.add_ref(),
             },
@@ -1921,19 +1809,19 @@ impl<B: GfxBackend> Device<B> {
     }
 
     //TODO: refactor this. It's the only method of `Device` that registers new objects
     // (the pipeline layout).
     fn derive_pipeline_layout(
         &self,
         self_id: id::DeviceId,
         implicit_context: Option<ImplicitPipelineContext>,
-        mut derived_group_layouts: ArrayVec<[binding_model::BindEntryMap; MAX_BIND_GROUPS]>,
-        bgl_guard: &mut Storage<binding_model::BindGroupLayout<B>, id::BindGroupLayoutId>,
-        pipeline_layout_guard: &mut Storage<binding_model::PipelineLayout<B>, id::PipelineLayoutId>,
+        mut derived_group_layouts: ArrayVec<binding_model::BindEntryMap, { hal::MAX_BIND_GROUPS }>,
+        bgl_guard: &mut Storage<binding_model::BindGroupLayout<A>, id::BindGroupLayoutId>,
+        pipeline_layout_guard: &mut Storage<binding_model::PipelineLayout<A>, id::PipelineLayoutId>,
     ) -> Result<id::PipelineLayoutId, pipeline::ImplicitLayoutError> {
         while derived_group_layouts
             .last()
             .map_or(false, |map| map.is_empty())
         {
             derived_group_layouts.pop();
         }
         let mut ids = implicit_context.ok_or(pipeline::ImplicitLayoutError::MissingIds(0))?;
@@ -1969,126 +1857,108 @@ impl<B: GfxBackend> Device<B> {
         Ok(ids.root_id)
     }
 
     fn create_compute_pipeline<G: GlobalIdentityHandlerFactory>(
         &self,
         self_id: id::DeviceId,
         desc: &pipeline::ComputePipelineDescriptor,
         implicit_context: Option<ImplicitPipelineContext>,
-        hub: &Hub<B, G>,
+        hub: &Hub<A, G>,
         token: &mut Token<Self>,
-    ) -> Result<pipeline::ComputePipeline<B>, pipeline::CreateComputePipelineError> {
+    ) -> Result<pipeline::ComputePipeline<A>, pipeline::CreateComputePipelineError> {
         //TODO: only lock mutable if the layout is derived
         let (mut pipeline_layout_guard, mut token) = hub.pipeline_layouts.write(token);
         let (mut bgl_guard, mut token) = hub.bind_group_layouts.write(&mut token);
 
         // This has to be done first, or otherwise the IDs may be pointing to entries
         // that are not even in the storage.
         if let Some(ref ids) = implicit_context {
             pipeline_layout_guard.insert_error(ids.root_id, IMPLICIT_FAILURE);
             for &bgl_id in ids.group_ids.iter() {
                 bgl_guard.insert_error(bgl_id, IMPLICIT_FAILURE);
             }
         }
 
-        if !self
-            .downlevel
-            .flags
-            .contains(wgt::DownlevelFlags::COMPUTE_SHADERS)
-        {
-            return Err(pipeline::CreateComputePipelineError::ComputeShadersUnsupported);
-        }
+        self.require_downlevel_flags(wgt::DownlevelFlags::COMPUTE_SHADERS)?;
 
         let mut derived_group_layouts =
-            ArrayVec::<[binding_model::BindEntryMap; MAX_BIND_GROUPS]>::new();
+            ArrayVec::<binding_model::BindEntryMap, { hal::MAX_BIND_GROUPS }>::new();
 
         let io = validation::StageIo::default();
         let (shader_module_guard, _) = hub.shader_modules.read(&mut token);
 
-        let entry_point_name = &desc.stage.entry_point;
         let shader_module = shader_module_guard
             .get(desc.stage.module)
             .map_err(|_| validation::StageError::InvalidModule)?;
 
-        let flag = wgt::ShaderStage::COMPUTE;
-        if let Some(ref interface) = shader_module.interface {
+        {
+            let flag = wgt::ShaderStages::COMPUTE;
             let provided_layouts = match desc.layout {
                 Some(pipeline_layout_id) => Some(Device::get_introspection_bind_group_layouts(
                     pipeline_layout_guard
                         .get(pipeline_layout_id)
                         .map_err(|_| pipeline::CreateComputePipelineError::InvalidLayout)?,
                     &*bgl_guard,
                 )),
                 None => {
                     for _ in 0..self.limits.max_bind_groups {
                         derived_group_layouts.push(binding_model::BindEntryMap::default());
                     }
                     None
                 }
             };
-            let _ = interface.check_stage(
-                provided_layouts.as_ref().map(|p| p.as_slice()),
-                &mut derived_group_layouts,
-                &entry_point_name,
-                flag,
-                io,
-            )?;
-        } else if desc.layout.is_none() {
-            return Err(pipeline::ImplicitLayoutError::ReflectionError(flag).into());
+            if let Some(ref interface) = shader_module.interface {
+                let _ = interface.check_stage(
+                    provided_layouts.as_ref().map(|p| p.as_slice()),
+                    &mut derived_group_layouts,
+                    &desc.stage.entry_point,
+                    flag,
+                    io,
+                )?;
+            }
         }
 
-        let shader = hal::pso::EntryPoint::<B> {
-            entry: &entry_point_name, // TODO
-            module: &shader_module.raw,
-            specialization: hal::pso::Specialization::EMPTY,
-        };
-
-        // TODO
-        let flags = hal::pso::PipelineCreationFlags::empty();
-        // TODO
-        let parent = hal::pso::BasePipeline::None;
-
         let pipeline_layout_id = match desc.layout {
             Some(id) => id,
             None => self.derive_pipeline_layout(
                 self_id,
                 implicit_context,
                 derived_group_layouts,
                 &mut *bgl_guard,
                 &mut *pipeline_layout_guard,
             )?,
         };
         let layout = pipeline_layout_guard
             .get(pipeline_layout_id)
             .map_err(|_| pipeline::CreateComputePipelineError::InvalidLayout)?;
 
-        let pipeline_desc = hal::pso::ComputePipelineDesc {
-            label: desc.label.as_ref().map(AsRef::as_ref),
-            shader,
+        let pipeline_desc = hal::ComputePipelineDescriptor {
+            label: desc.label.borrow_option(),
             layout: &layout.raw,
-            flags,
-            parent,
+            stage: hal::ProgrammableStage {
+                entry_point: desc.stage.entry_point.as_ref(),
+                module: &shader_module.raw,
+            },
         };
 
         let raw =
-            unsafe { self.raw.create_compute_pipeline(&pipeline_desc, None) }.map_err(|err| {
-                match err {
-                    hal::pso::CreationError::OutOfMemory(_) => {
-                        pipeline::CreateComputePipelineError::Device(DeviceError::OutOfMemory)
+            unsafe { self.raw.create_compute_pipeline(&pipeline_desc) }.map_err(
+                |err| match err {
+                    hal::PipelineError::Device(error) => {
+                        pipeline::CreateComputePipelineError::Device(error.into())
                     }
-                    hal::pso::CreationError::ShaderCreationError(_, error) => {
-                        pipeline::CreateComputePipelineError::Internal(error)
+                    hal::PipelineError::Linkage(_stages, msg) => {
+                        pipeline::CreateComputePipelineError::Internal(msg)
                     }
-                    _ => {
-                        log::error!("failed to create compute pipeline: {}", err);
-                        pipeline::CreateComputePipelineError::Device(DeviceError::OutOfMemory)
+                    hal::PipelineError::EntryPoint(_stage) => {
+                        pipeline::CreateComputePipelineError::Internal(EP_FAILURE.to_string())
                     }
-                }
-            })?;
+                },
+            )?;
 
         let pipeline = pipeline::ComputePipeline {
             raw,
             layout_id: Stored {
                 value: id::Valid(pipeline_layout_id),
                 ref_count: layout.life_guard.add_ref(),
             },
             device_id: Stored {
@@ -2098,54 +1968,61 @@ impl<B: GfxBackend> Device<B> {
             life_guard: LifeGuard::new(desc.label.borrow_or_default()),
         };
         Ok(pipeline)
     }
 
     fn create_render_pipeline<G: GlobalIdentityHandlerFactory>(
         &self,
         self_id: id::DeviceId,
-        adapter: &crate::instance::Adapter<B>,
+        adapter: &crate::instance::Adapter<A>,
         desc: &pipeline::RenderPipelineDescriptor,
         implicit_context: Option<ImplicitPipelineContext>,
-        hub: &Hub<B, G>,
+        hub: &Hub<A, G>,
         token: &mut Token<Self>,
-    ) -> Result<pipeline::RenderPipeline<B>, pipeline::CreateRenderPipelineError> {
+    ) -> Result<pipeline::RenderPipeline<A>, pipeline::CreateRenderPipelineError> {
         //TODO: only lock mutable if the layout is derived
         let (mut pipeline_layout_guard, mut token) = hub.pipeline_layouts.write(token);
         let (mut bgl_guard, mut token) = hub.bind_group_layouts.write(&mut token);
 
         // This has to be done first, or otherwise the IDs may be pointing to entries
         // that are not even in the storage.
         if let Some(ref ids) = implicit_context {
             pipeline_layout_guard.insert_error(ids.root_id, IMPLICIT_FAILURE);
             for &bgl_id in ids.group_ids.iter() {
                 bgl_guard.insert_error(bgl_id, IMPLICIT_FAILURE);
             }
         }
 
         let mut derived_group_layouts =
-            ArrayVec::<[binding_model::BindEntryMap; MAX_BIND_GROUPS]>::new();
-
-        let color_states = desc
+            ArrayVec::<binding_model::BindEntryMap, { hal::MAX_BIND_GROUPS }>::new();
+
+        let color_targets = desc
             .fragment
             .as_ref()
             .map_or(&[][..], |fragment| &fragment.targets);
         let depth_stencil_state = desc.depth_stencil.as_ref();
-        let rasterizer =
-            conv::map_primitive_state_to_rasterizer(&desc.primitive, depth_stencil_state);
+
+        if !color_targets.is_empty() && {
+            let first = &color_targets[0];
+            color_targets[1..]
+                .iter()
+                .any(|ct| ct.write_mask != first.write_mask || ct.blend != first.blend)
+        } {
+            log::info!("Color targets: {:?}", color_targets);
+            self.require_downlevel_flags(wgt::DownlevelFlags::INDEPENDENT_BLENDING)?;
+        }
 
         let mut io = validation::StageIo::default();
-        let mut validated_stages = wgt::ShaderStage::empty();
-
-        let desc_vbs = &desc.vertex.buffers;
-        let mut vertex_strides = Vec::with_capacity(desc_vbs.len());
-        let mut vertex_buffers = Vec::with_capacity(desc_vbs.len());
-        let mut attributes = Vec::new();
-        for (i, vb_state) in desc_vbs.iter().enumerate() {
+        let mut validated_stages = wgt::ShaderStages::empty();
+
+        let mut vertex_strides = Vec::with_capacity(desc.vertex.buffers.len());
+        let mut vertex_buffers = Vec::with_capacity(desc.vertex.buffers.len());
+        let mut total_attributes = 0;
+        for (i, vb_state) in desc.vertex.buffers.iter().enumerate() {
             vertex_strides
                 .alloc()
                 .init((vb_state.array_stride, vb_state.step_mode));
             if vb_state.attributes.is_empty() {
                 continue;
             }
             if vb_state.array_stride > self.limits.max_vertex_buffer_array_stride as u64 {
                 return Err(pipeline::CreateRenderPipelineError::VertexStrideTooLarge {
@@ -2155,26 +2032,23 @@ impl<B: GfxBackend> Device<B> {
                 });
             }
             if vb_state.array_stride % wgt::VERTEX_STRIDE_ALIGNMENT != 0 {
                 return Err(pipeline::CreateRenderPipelineError::UnalignedVertexStride {
                     index: i as u32,
                     stride: vb_state.array_stride,
                 });
             }
-            vertex_buffers.alloc().init(hal::pso::VertexBufferDesc {
-                binding: i as u32,
-                stride: vb_state.array_stride as u32,
-                rate: match vb_state.step_mode {
-                    InputStepMode::Vertex => hal::pso::VertexInputRate::Vertex,
-                    InputStepMode::Instance => hal::pso::VertexInputRate::Instance(1),
-                },
+            vertex_buffers.alloc().init(hal::VertexBufferLayout {
+                array_stride: vb_state.array_stride,
+                step_mode: vb_state.step_mode,
+                attributes: vb_state.attributes.as_ref(),
             });
-            let desc_atts = &vb_state.attributes;
-            for attribute in desc_atts.iter() {
+
+            for attribute in vb_state.attributes.iter() {
                 if attribute.offset >= 0x10000000 {
                     return Err(
                         pipeline::CreateRenderPipelineError::InvalidVertexAttributeOffset {
                             location: attribute.shader_location,
                             offset: attribute.offset,
                         },
                     );
                 }
@@ -2182,260 +2056,184 @@ impl<B: GfxBackend> Device<B> {
                 if let wgt::VertexFormat::Float64
                 | wgt::VertexFormat::Float64x2
                 | wgt::VertexFormat::Float64x3
                 | wgt::VertexFormat::Float64x4 = attribute.format
                 {
                     self.require_features(wgt::Features::VERTEX_ATTRIBUTE_64BIT)?;
                 }
 
-                attributes.alloc().init(hal::pso::AttributeDesc {
-                    location: attribute.shader_location,
-                    binding: i as u32,
-                    element: hal::pso::Element {
-                        format: conv::map_vertex_format(attribute.format),
-                        offset: attribute.offset as u32,
-                    },
-                });
                 io.insert(
                     attribute.shader_location,
                     validation::InterfaceVar::vertex_attribute(attribute.format),
                 );
             }
+            total_attributes += vb_state.attributes.len();
         }
 
         if vertex_buffers.len() > self.limits.max_vertex_buffers as usize {
             return Err(pipeline::CreateRenderPipelineError::TooManyVertexBuffers {
                 given: vertex_buffers.len() as u32,
                 limit: self.limits.max_vertex_buffers,
             });
         }
-        if attributes.len() > self.limits.max_vertex_attributes as usize {
+        if total_attributes > self.limits.max_vertex_attributes as usize {
             return Err(
                 pipeline::CreateRenderPipelineError::TooManyVertexAttributes {
-                    given: attributes.len() as u32,
+                    given: total_attributes as u32,
                     limit: self.limits.max_vertex_attributes,
                 },
             );
         }
 
-        if desc.primitive.strip_index_format.is_some()
-            && desc.primitive.topology != wgt::PrimitiveTopology::LineStrip
-            && desc.primitive.topology != wgt::PrimitiveTopology::TriangleStrip
-        {
+        if desc.primitive.strip_index_format.is_some() && !desc.primitive.topology.is_strip() {
             return Err(
                 pipeline::CreateRenderPipelineError::StripIndexFormatForNonStripTopology {
                     strip_index_format: desc.primitive.strip_index_format,
                     topology: desc.primitive.topology,
                 },
             );
         }
 
         if desc.primitive.clamp_depth {
             self.require_features(wgt::Features::DEPTH_CLAMPING)?;
         }
-        if desc.primitive.polygon_mode != wgt::PolygonMode::Fill {
-            self.require_features(wgt::Features::NON_FILL_POLYGON_MODE)?;
+
+        if desc.primitive.polygon_mode == wgt::PolygonMode::Line {
+            self.require_features(wgt::Features::POLYGON_MODE_LINE)?;
+        }
+        if desc.primitive.polygon_mode == wgt::PolygonMode::Point {
+            self.require_features(wgt::Features::POLYGON_MODE_POINT)?;
         }
 
         if desc.primitive.conservative {
             self.require_features(wgt::Features::CONSERVATIVE_RASTERIZATION)?;
         }
 
         if desc.primitive.conservative && desc.primitive.polygon_mode != wgt::PolygonMode::Fill {
             return Err(
                 pipeline::CreateRenderPipelineError::ConservativeRasterizationNonFillPolygonMode,
             );
         }
 
-        let input_assembler = conv::map_primitive_state_to_input_assembler(&desc.primitive);
-
-        let mut blender = hal::pso::BlendDesc {
-            logic_op: None,
-            targets: Vec::with_capacity(color_states.len()),
-        };
-        for (i, cs) in color_states.iter().enumerate() {
+        for (i, cs) in color_targets.iter().enumerate() {
             let error = loop {
                 let format_features = self.describe_format_features(adapter, cs.format)?;
                 if !format_features
                     .allowed_usages
-                    .contains(wgt::TextureUsage::RENDER_ATTACHMENT)
+                    .contains(wgt::TextureUsages::RENDER_ATTACHMENT)
                 {
                     break Some(pipeline::ColorStateError::FormatNotRenderable(cs.format));
                 }
                 if cs.blend.is_some() && !format_features.filterable {
                     break Some(pipeline::ColorStateError::FormatNotBlendable(cs.format));
                 }
-                let hal_format = conv::map_texture_format(cs.format, self.private_features);
-                if !hal_format
-                    .surface_desc()
-                    .aspects
-                    .contains(hal::format::Aspects::COLOR)
-                {
+                if !hal::FormatAspects::from(cs.format).contains(hal::FormatAspects::COLOR) {
                     break Some(pipeline::ColorStateError::FormatNotColor(cs.format));
                 }
 
-                match conv::map_color_target_state(cs) {
-                    Ok(bt) => blender.targets.push(bt),
-                    Err(e) => break Some(e),
-                }
                 break None;
             };
             if let Some(e) = error {
                 return Err(pipeline::CreateRenderPipelineError::ColorState(i as u8, e));
             }
         }
 
         if let Some(ds) = depth_stencil_state {
             let error = loop {
                 if !self
                     .describe_format_features(adapter, ds.format)?
                     .allowed_usages
-                    .contains(wgt::TextureUsage::RENDER_ATTACHMENT)
+                    .contains(wgt::TextureUsages::RENDER_ATTACHMENT)
                 {
                     break Some(pipeline::DepthStencilStateError::FormatNotRenderable(
                         ds.format,
                     ));
                 }
-                let hal_format = conv::map_texture_format(ds.format, self.private_features);
-                let aspects = hal_format.surface_desc().aspects;
-                if ds.is_depth_enabled() && !aspects.contains(hal::format::Aspects::DEPTH) {
+                let aspect = hal::FormatAspects::from(ds.format);
+                if ds.is_depth_enabled() && !aspect.contains(hal::FormatAspects::DEPTH) {
                     break Some(pipeline::DepthStencilStateError::FormatNotDepth(ds.format));
                 }
-                if ds.stencil.is_enabled() && !aspects.contains(hal::format::Aspects::STENCIL) {
+                if ds.stencil.is_enabled() && !aspect.contains(hal::FormatAspects::STENCIL) {
                     break Some(pipeline::DepthStencilStateError::FormatNotStencil(
                         ds.format,
                     ));
                 }
                 break None;
             };
             if let Some(e) = error {
                 return Err(pipeline::CreateRenderPipelineError::DepthStencilState(e));
             }
         }
-        let depth_stencil = depth_stencil_state
-            .map(conv::map_depth_stencil_state)
-            .unwrap_or_default();
-
-        let baked_states = hal::pso::BakedStates {
-            viewport: None,
-            scissor: None,
-            blend_constants: None,
-            depth_bounds: None,
-        };
 
         if desc.layout.is_none() {
             for _ in 0..self.limits.max_bind_groups {
                 derived_group_layouts.push(binding_model::BindEntryMap::default());
             }
         }
 
         let samples = {
             let sc = desc.multisample.count;
             if sc == 0 || sc > 32 || !conv::is_power_of_two(sc) {
                 return Err(pipeline::CreateRenderPipelineError::InvalidSampleCount(sc));
             }
-            sc as u8
-        };
-        let multisampling = if samples == 1 {
-            None
-        } else {
-            Some(conv::map_multisample_state(&desc.multisample))
-        };
-
-        let rp_key = RenderPassKey {
-            colors: color_states
-                .iter()
-                .map(|state| {
-                    let at = hal::pass::Attachment {
-                        format: Some(conv::map_texture_format(
-                            state.format,
-                            self.private_features,
-                        )),
-                        samples,
-                        ops: hal::pass::AttachmentOps::PRESERVE,
-                        stencil_ops: hal::pass::AttachmentOps::DONT_CARE,
-                        layouts: hal::image::Layout::General..hal::image::Layout::General,
-                    };
-                    (at, hal::image::Layout::ColorAttachmentOptimal)
-                })
-                .collect(),
-            // We can ignore the resolves as the vulkan specs says:
-            // As an additional special case, if two render passes have a single subpass,
-            // they are compatible even if they have different resolve attachment references
-            // or depth/stencil resolve modes but satisfy the other compatibility conditions.
-            resolves: ArrayVec::new(),
-            depth_stencil: depth_stencil_state.map(|state| {
-                let at = hal::pass::Attachment {
-                    format: Some(conv::map_texture_format(
-                        state.format,
-                        self.private_features,
-                    )),
-                    samples,
-                    ops: hal::pass::AttachmentOps::PRESERVE,
-                    stencil_ops: hal::pass::AttachmentOps::PRESERVE,
-                    layouts: hal::image::Layout::General..hal::image::Layout::General,
-                };
-                (at, hal::image::Layout::DepthStencilAttachmentOptimal)
-            }),
+            sc
         };
 
         let (shader_module_guard, _) = hub.shader_modules.read(&mut token);
 
-        let vertex = {
+        let vertex_stage = {
             let stage = &desc.vertex.stage;
-            let flag = wgt::ShaderStage::VERTEX;
+            let flag = wgt::ShaderStages::VERTEX;
 
             let shader_module = shader_module_guard.get(stage.module).map_err(|_| {
                 pipeline::CreateRenderPipelineError::Stage {
                     stage: flag,
                     error: validation::StageError::InvalidModule,
                 }
             })?;
 
+            let provided_layouts = match desc.layout {
+                Some(pipeline_layout_id) => {
+                    let pipeline_layout = pipeline_layout_guard
+                        .get(pipeline_layout_id)
+                        .map_err(|_| pipeline::CreateRenderPipelineError::InvalidLayout)?;
+                    Some(Device::get_introspection_bind_group_layouts(
+                        pipeline_layout,
+                        &*bgl_guard,
+                    ))
+                }
+                None => None,
+            };
+
             if let Some(ref interface) = shader_module.interface {
-                let provided_layouts = match desc.layout {
-                    Some(pipeline_layout_id) => {
-                        let pipeline_layout = pipeline_layout_guard
-                            .get(pipeline_layout_id)
-                            .map_err(|_| pipeline::CreateRenderPipelineError::InvalidLayout)?;
-                        Some(Device::get_introspection_bind_group_layouts(
-                            pipeline_layout,
-                            &*bgl_guard,
-                        ))
-                    }
-                    None => None,
-                };
-
                 io = interface
                     .check_stage(
                         provided_layouts.as_ref().map(|p| p.as_slice()),
                         &mut derived_group_layouts,
                         &stage.entry_point,
                         flag,
                         io,
                     )
                     .map_err(|error| pipeline::CreateRenderPipelineError::Stage {
                         stage: flag,
                         error,
                     })?;
                 validated_stages |= flag;
             }
 
-            hal::pso::EntryPoint::<B> {
-                entry: &stage.entry_point,
+            hal::ProgrammableStage {
                 module: &shader_module.raw,
-                specialization: hal::pso::Specialization::EMPTY,
+                entry_point: stage.entry_point.as_ref(),
             }
         };
 
-        let fragment = match desc.fragment {
+        let fragment_stage = match desc.fragment {
             Some(ref fragment) => {
-                let entry_point_name = &fragment.stage.entry_point;
-                let flag = wgt::ShaderStage::FRAGMENT;
+                let flag = wgt::ShaderStages::FRAGMENT;
 
                 let shader_module =
                     shader_module_guard
                         .get(fragment.stage.module)
                         .map_err(|_| pipeline::CreateRenderPipelineError::Stage {
                             stage: flag,
                             error: validation::StageError::InvalidModule,
                         })?;
@@ -2445,47 +2243,46 @@ impl<B: GfxBackend> Device<B> {
                         pipeline_layout_guard
                             .get(pipeline_layout_id)
                             .map_err(|_| pipeline::CreateRenderPipelineError::InvalidLayout)?,
                         &*bgl_guard,
                     )),
                     None => None,
                 };
 
-                if validated_stages == wgt::ShaderStage::VERTEX {
+                if validated_stages == wgt::ShaderStages::VERTEX {
                     if let Some(ref interface) = shader_module.interface {
                         io = interface
                             .check_stage(
                                 provided_layouts.as_ref().map(|p| p.as_slice()),
                                 &mut derived_group_layouts,
-                                &entry_point_name,
+                                &fragment.stage.entry_point,
                                 flag,
                                 io,
                             )
                             .map_err(|error| pipeline::CreateRenderPipelineError::Stage {
                                 stage: flag,
                                 error,
                             })?;
                         validated_stages |= flag;
                     }
                 }
 
-                Some(hal::pso::EntryPoint::<B> {
-                    entry: &entry_point_name,
+                Some(hal::ProgrammableStage {
                     module: &shader_module.raw,
-                    specialization: hal::pso::Specialization::EMPTY,
+                    entry_point: fragment.stage.entry_point.as_ref(),
                 })
             }
             None => None,
         };
 
-        if validated_stages.contains(wgt::ShaderStage::FRAGMENT) {
-            for (i, state) in color_states.iter().enumerate() {
+        if validated_stages.contains(wgt::ShaderStages::FRAGMENT) {
+            for (i, state) in color_targets.iter().enumerate() {
                 match io.get(&(i as wgt::ShaderLocation)) {
-                    Some(ref output) => {
+                    Some(output) => {
                         validation::check_texture_format(state.format, &output.ty).map_err(
                             |pipeline| {
                                 pipeline::CreateRenderPipelineError::ColorState(
                                     i as u8,
                                     pipeline::ColorStateError::IncompatibleFormat {
                                         pipeline,
                                         shader: output.ty,
                                     },
@@ -2500,108 +2297,77 @@ impl<B: GfxBackend> Device<B> {
                             i as u8,
                             pipeline::ColorStateError::Missing,
                         ));
                     }
                 }
             }
         }
         let last_stage = match desc.fragment {
-            Some(_) => wgt::ShaderStage::FRAGMENT,
-            None => wgt::ShaderStage::VERTEX,
+            Some(_) => wgt::ShaderStages::FRAGMENT,
+            None => wgt::ShaderStages::VERTEX,
         };
         if desc.layout.is_none() && !validated_stages.contains(last_stage) {
             return Err(pipeline::ImplicitLayoutError::ReflectionError(last_stage).into());
         }
 
-        let primitive_assembler = hal::pso::PrimitiveAssemblerDesc::Vertex {
-            buffers: &vertex_buffers,
-            attributes: &attributes,
-            input_assembler,
-            vertex,
-            tessellation: None,
-            geometry: None,
-        };
-
-        // TODO
-        let flags = hal::pso::PipelineCreationFlags::empty();
-        // TODO
-        let parent = hal::pso::BasePipeline::None;
-
         let pipeline_layout_id = match desc.layout {
             Some(id) => id,
             None => self.derive_pipeline_layout(
                 self_id,
                 implicit_context,
                 derived_group_layouts,
                 &mut *bgl_guard,
                 &mut *pipeline_layout_guard,
             )?,
         };
         let layout = pipeline_layout_guard
             .get(pipeline_layout_id)
             .map_err(|_| pipeline::CreateRenderPipelineError::InvalidLayout)?;
 
-        let mut rp_lock = self.render_passes.lock();
-        let pipeline_desc = hal::pso::GraphicsPipelineDesc {
-            label: desc.label.as_ref().map(AsRef::as_ref),
-            primitive_assembler,
-            rasterizer,
-            fragment,
-            blender,
-            depth_stencil,
-            multisampling,
-            baked_states,
+        let pipeline_desc = hal::RenderPipelineDescriptor {
+            label: desc.label.borrow_option(),
             layout: &layout.raw,
-            subpass: hal::pass::Subpass {
-                index: 0,
-                main_pass: match rp_lock.render_passes.entry(rp_key) {
-                    Entry::Occupied(e) => e.into_mut(),
-                    Entry::Vacant(e) => {
-                        let pass = self
-                            .create_compatible_render_pass(e.key())
-                            .or(Err(DeviceError::OutOfMemory))?;
-                        e.insert(pass)
+            vertex_buffers: &vertex_buffers,
+            vertex_stage,
+            primitive: desc.primitive,
+            depth_stencil: desc.depth_stencil.clone(),
+            multisample: desc.multisample,
+            fragment_stage,
+            color_targets,
+        };
+        let raw =
+            unsafe { self.raw.create_render_pipeline(&pipeline_desc) }.map_err(
+                |err| match err {
+                    hal::PipelineError::Device(error) => {
+                        pipeline::CreateRenderPipelineError::Device(error.into())
+                    }
+                    hal::PipelineError::Linkage(stage, msg) => {
+                        pipeline::CreateRenderPipelineError::Internal { stage, error: msg }
+                    }
+                    hal::PipelineError::EntryPoint(stage) => {
+                        pipeline::CreateRenderPipelineError::Internal {
+                            stage: hal::auxil::map_naga_stage(stage),
+                            error: EP_FAILURE.to_string(),
+                        }
                     }
                 },
-            },
-            flags,
-            parent,
-        };
-        // TODO: cache
-        let raw =
-            unsafe { self.raw.create_graphics_pipeline(&pipeline_desc, None) }.map_err(|err| {
-                match err {
-                    hal::pso::CreationError::OutOfMemory(_) => {
-                        pipeline::CreateRenderPipelineError::Device(DeviceError::OutOfMemory)
-                    }
-                    hal::pso::CreationError::ShaderCreationError(stage, error) => {
-                        pipeline::CreateRenderPipelineError::Internal {
-                            stage: conv::map_hal_flags_to_shader_stage(stage),
-                            error,
-                        }
-                    }
-                    _ => {
-                        log::error!("failed to create graphics pipeline: {}", err);
-                        pipeline::CreateRenderPipelineError::Device(DeviceError::OutOfMemory)
-                    }
-                }
-            })?;
+            )?;
 
         let pass_context = RenderPassContext {
             attachments: AttachmentData {
-                colors: color_states.iter().map(|state| state.format).collect(),
+                colors: color_targets.iter().map(|state| state.format).collect(),
                 resolves: ArrayVec::new(),
                 depth_stencil: depth_stencil_state.as_ref().map(|state| state.format),
             },
             sample_count: samples,
         };
 
         let mut flags = pipeline::PipelineFlags::empty();
-        for state in color_states.iter() {
+        for state in color_targets.iter() {
             if let Some(ref bs) = state.blend {
                 if bs.color.uses_constant() | bs.alpha.uses_constant() {
                     flags |= pipeline::PipelineFlags::BLEND_CONSTANT;
                 }
             }
         }
         if let Some(ds) = depth_stencil_state.as_ref() {
             if ds.stencil.is_enabled() && ds.stencil.needs_ref_value() {
@@ -2628,17 +2394,17 @@ impl<B: GfxBackend> Device<B> {
             vertex_strides,
             life_guard: LifeGuard::new(desc.label.borrow_or_default()),
         };
         Ok(pipeline)
     }
 
     fn describe_format_features(
         &self,
-        adapter: &crate::instance::Adapter<B>,
+        adapter: &crate::instance::Adapter<A>,
         format: TextureFormat,
     ) -> Result<wgt::TextureFormatFeatures, MissingFeatures> {
         let format_desc = format.describe();
         self.require_features(format_desc.required_features)?;
 
         if self
             .features
             .contains(wgt::Features::TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES)
@@ -2649,34 +2415,48 @@ impl<B: GfxBackend> Device<B> {
         }
     }
 
     fn wait_for_submit(
         &self,
         submission_index: SubmissionIndex,
         token: &mut Token<Self>,
     ) -> Result<(), WaitIdleError> {
-        if self.last_completed_submission_index() <= submission_index {
+        let last_done_index = unsafe {
+            self.raw
+                .get_fence_value(&self.fence)
+                .map_err(DeviceError::from)?
+        };
+        if last_done_index < submission_index {
             log::info!("Waiting for submission {:?}", submission_index);
-            self.lock_life(token)
-                .triage_submissions(&self.raw, true)
-                .map(|_| ())
-        } else {
-            Ok(())
+            unsafe {
+                self.raw
+                    .wait(&self.fence, submission_index, !0)
+                    .map_err(DeviceError::from)?
+            };
+            let closures = self
+                .lock_life(token)
+                .triage_submissions(submission_index, &self.command_allocator);
+            assert!(
+                closures.is_empty(),
+                "wait_for_submit is not expected to work with closures"
+            );
         }
+        Ok(())
     }
 
     fn create_query_set(
         &self,
         self_id: id::DeviceId,
-        desc: &wgt::QuerySetDescriptor,
-    ) -> Result<resource::QuerySet<B>, resource::CreateQuerySetError> {
+        desc: &resource::QuerySetDescriptor,
+    ) -> Result<resource::QuerySet<A>, resource::CreateQuerySetError> {
         use resource::CreateQuerySetError as Error;
 
         match desc.ty {
+            wgt::QueryType::Occlusion => {}
             wgt::QueryType::Timestamp => {
                 self.require_features(wgt::Features::TIMESTAMP_QUERY)?;
             }
             wgt::QueryType::PipelineStatistics(..) => {
                 self.require_features(wgt::Features::PIPELINE_STATISTICS_QUERY)?;
             }
         }
 
@@ -2686,86 +2466,75 @@ impl<B: GfxBackend> Device<B> {
 
         if desc.count > wgt::QUERY_SET_MAX_QUERIES {
             return Err(Error::TooManyQueries {
                 count: desc.count,
                 maximum: wgt::QUERY_SET_MAX_QUERIES,
             });
         }
 
-        let (hal_type, elements) = conv::map_query_type(&desc.ty);
-
+        let hal_desc = desc.map_label(super::LabelHelpers::borrow_option);
         Ok(resource::QuerySet {
-            raw: unsafe { self.raw.create_query_pool(hal_type, desc.count).unwrap() },
+            raw: unsafe { self.raw.create_query_set(&hal_desc).unwrap() },
             device_id: Stored {
                 value: id::Valid(self_id),
                 ref_count: self.life_guard.add_ref(),
             },
             life_guard: LifeGuard::new(""),
-            desc: desc.clone(),
-            elements,
+            desc: desc.map_label(|_| ()),
         })
     }
 }
 
-impl<B: hal::Backend> Device<B> {
-    pub(crate) fn destroy_bind_group(&self, bind_group: binding_model::BindGroup<B>) {
-        self.desc_allocator
-            .lock()
-            .free(&self.raw, iter::once(bind_group.raw));
-    }
-
-    pub(crate) fn destroy_buffer(&self, buffer: resource::Buffer<B>) {
-        if let Some((raw, memory)) = buffer.raw {
+impl<A: hal::Api> Device<A> {
+    pub(crate) fn destroy_buffer(&self, buffer: resource::Buffer<A>) {
+        if let Some(raw) = buffer.raw {
             unsafe {
-                self.mem_allocator.lock().free(&self.raw, memory);
                 self.raw.destroy_buffer(raw);
             }
         }
     }
 
-    pub(crate) fn destroy_texture(&self, texture: resource::Texture<B>) {
-        if let Some((raw, memory)) = texture.raw {
-            unsafe {
-                self.mem_allocator.lock().free(&self.raw, memory);
-                self.raw.destroy_image(raw);
-            }
+    pub(crate) fn destroy_command_buffer(&self, cmd_buf: command::CommandBuffer<A>) {
+        let mut baked = cmd_buf.into_baked();
+        unsafe {
+            baked.encoder.reset_all(baked.list.into_iter());
+        }
+        unsafe {
+            self.raw.destroy_command_encoder(baked.encoder);
         }
     }
 
     /// Wait for idle and remove resources that we can, before we die.
     pub(crate) fn prepare_to_die(&mut self) {
+        self.pending_writes.deactivate();
         let mut life_tracker = self.life_tracker.lock();
-        if let Err(error) = life_tracker.triage_submissions(&self.raw, true) {
-            log::error!("failed to triage submissions: {}", error);
+        let current_index = self.active_submission_index;
+        if let Err(error) = unsafe { self.raw.wait(&self.fence, current_index, CLEANUP_WAIT_MS) } {
+            log::error!("failed to wait for the device: {:?}", error);
         }
-        life_tracker.cleanup(&self.raw, &self.mem_allocator, &self.desc_allocator);
+        let _ = life_tracker.triage_submissions(current_index, &self.command_allocator);
+        life_tracker.cleanup(&self.raw);
+        #[cfg(feature = "trace")]
+        {
+            self.trace = None;
+        }
     }
 
     pub(crate) fn dispose(self) {
-        let mut desc_alloc = self.desc_allocator.into_inner();
-        let mut mem_alloc = self.mem_allocator.into_inner();
-        self.pending_writes
-            .dispose(&self.raw, &self.cmd_allocator, &mut mem_alloc);
-        self.cmd_allocator.destroy(&self.raw);
+        self.pending_writes.dispose(&self.raw);
+        self.command_allocator.into_inner().dispose(&self.raw);
         unsafe {
-            desc_alloc.cleanup(&self.raw);
-            mem_alloc.clear(&self.raw);
-            let rps = self.render_passes.into_inner();
-            for (_, rp) in rps.render_passes {
-                self.raw.destroy_render_pass(rp);
-            }
-            for (_, fbo) in rps.framebuffers {
-                self.raw.destroy_framebuffer(fbo);
-            }
+            self.raw.destroy_fence(self.fence);
+            self.raw.exit(self.queue);
         }
     }
 }
 
-impl<B: hal::Backend> crate::hub::Resource for Device<B> {
+impl<A: hal::Api> crate::hub::Resource for Device<A> {
     const TYPE: &'static str = "Device";
 
     fn life_guard(&self) -> &LifeGuard {
         &self.life_guard
     }
 }
 
 #[derive(Clone, Debug, Error)]
@@ -2777,244 +2546,240 @@ pub enum DeviceError {
     #[error("parent device is invalid")]
     Invalid,
     #[error("parent device is lost")]
     Lost,
     #[error("not enough memory left")]
     OutOfMemory,
 }
 
-impl From<hal::device::WaitError> for DeviceError {
-    fn from(err: hal::device::WaitError) -> Self {
-        match err {
-            hal::device::WaitError::OutOfMemory(_) => Self::OutOfMemory,
-            hal::device::WaitError::DeviceLost(_) => Self::Lost,
-        }
-    }
-}
-
-impl From<gpu_alloc::MapError> for DeviceError {
-    fn from(err: gpu_alloc::MapError) -> Self {
-        match err {
-            gpu_alloc::MapError::OutOfDeviceMemory | gpu_alloc::MapError::OutOfHostMemory => {
-                DeviceError::OutOfMemory
-            }
-            _ => panic!("failed to map buffer: {}", err),
-        }
-    }
-}
-
-impl DeviceError {
-    fn from_bind(err: hal::device::BindError) -> Self {
-        match err {
-            hal::device::BindError::OutOfMemory(_) => Self::OutOfMemory,
-            _ => panic!("failed to bind memory: {}", err),
+impl From<hal::DeviceError> for DeviceError {
+    fn from(error: hal::DeviceError) -> Self {
+        match error {
+            hal::DeviceError::Lost => DeviceError::Lost,
+            hal::DeviceError::OutOfMemory => DeviceError::OutOfMemory,
         }
     }
 }
 
 #[derive(Clone, Debug, Error)]
 #[error("Features {0:?} are required but not enabled on the device")]
 pub struct MissingFeatures(pub wgt::Features);
 
+#[derive(Clone, Debug, Error)]
+#[error(
+    "Downlevel flags {0:?} are required but not supported on the device.\n{}",
+    DOWNLEVEL_ERROR_MESSAGE
+)]
+pub struct MissingDownlevelFlags(pub wgt::DownlevelFlags);
+
 #[derive(Clone, Debug)]
 #[cfg_attr(feature = "trace", derive(serde::Serialize))]
 #[cfg_attr(feature = "replay", derive(serde::Deserialize))]
 pub struct ImplicitPipelineContext {
     pub root_id: id::PipelineLayoutId,
-    pub group_ids: ArrayVec<[id::BindGroupLayoutId; MAX_BIND_GROUPS]>,
+    pub group_ids: ArrayVec<id::BindGroupLayoutId, { hal::MAX_BIND_GROUPS }>,
 }
 
 pub struct ImplicitPipelineIds<'a, G: GlobalIdentityHandlerFactory> {
     pub root_id: Input<G, id::PipelineLayoutId>,
     pub group_ids: &'a [Input<G, id::BindGroupLayoutId>],
 }
 
 impl<G: GlobalIdentityHandlerFactory> ImplicitPipelineIds<'_, G> {
-    fn prepare<B: hal::Backend>(self, hub: &Hub<B, G>) -> ImplicitPipelineContext {
+    fn prepare<A: hal::Api>(self, hub: &Hub<A, G>) -> ImplicitPipelineContext {
         ImplicitPipelineContext {
             root_id: hub.pipeline_layouts.prepare(self.root_id).into_id(),
             group_ids: self
                 .group_ids
                 .iter()
                 .map(|id_in| hub.bind_group_layouts.prepare(id_in.clone()).into_id())
                 .collect(),
         }
     }
 }
 
 impl<G: GlobalIdentityHandlerFactory> Global<G> {
-    pub fn adapter_get_swap_chain_preferred_format<B: GfxBackend>(
+    pub fn adapter_is_surface_supported<A: HalApi>(
         &self,
         adapter_id: id::AdapterId,
         surface_id: id::SurfaceId,
-    ) -> Result<TextureFormat, instance::GetSwapChainPreferredFormatError> {
-        let hub = B::hub(self);
+    ) -> Result<bool, instance::IsSurfaceSupportedError> {
+        let hub = A::hub(self);
         let mut token = Token::root();
 
-        let (mut surface_guard, mut token) = self.surfaces.write(&mut token);
+        let (surface_guard, mut token) = self.surfaces.read(&mut token);
         let (adapter_guard, mut _token) = hub.adapters.read(&mut token);
         let adapter = adapter_guard
             .get(adapter_id)
-            .map_err(|_| instance::GetSwapChainPreferredFormatError::InvalidAdapter)?;
+            .map_err(|_| instance::IsSurfaceSupportedError::InvalidAdapter)?;
         let surface = surface_guard
-            .get_mut(surface_id)
-            .map_err(|_| instance::GetSwapChainPreferredFormatError::InvalidSurface)?;
-
-        adapter.get_swap_chain_preferred_format(surface)
+            .get(surface_id)
+            .map_err(|_| instance::IsSurfaceSupportedError::InvalidSurface)?;
+        Ok(adapter.is_surface_supported(surface))
     }
-
-    pub fn device_features<B: GfxBackend>(
+    pub fn surface_get_preferred_format<A: HalApi>(
+        &self,
+        surface_id: id::SurfaceId,
+        adapter_id: id::AdapterId,
+    ) -> Result<TextureFormat, instance::GetSurfacePreferredFormatError> {
+        let hub = A::hub(self);
+        let mut token = Token::root();
+
+        let (surface_guard, mut token) = self.surfaces.read(&mut token);
+        let (adapter_guard, mut _token) = hub.adapters.read(&mut token);
+        let adapter = adapter_guard
+            .get(adapter_id)
+            .map_err(|_| instance::GetSurfacePreferredFormatError::InvalidAdapter)?;
+        let surface = surface_guard
+            .get(surface_id)
+            .map_err(|_| instance::GetSurfacePreferredFormatError::InvalidSurface)?;
+
+        surface.get_preferred_format(adapter)
+    }
+
+    pub fn device_features<A: HalApi>(
         &self,
         device_id: id::DeviceId,
     ) -> Result<wgt::Features, InvalidDevice> {
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, _) = hub.devices.read(&mut token);
         let device = device_guard.get(device_id).map_err(|_| InvalidDevice)?;
 
         Ok(device.features)
     }
 
-    pub fn device_limits<B: GfxBackend>(
+    pub fn device_limits<A: HalApi>(
         &self,
         device_id: id::DeviceId,
     ) -> Result<wgt::Limits, InvalidDevice> {
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, _) = hub.devices.read(&mut token);
         let device = device_guard.get(device_id).map_err(|_| InvalidDevice)?;
 
         Ok(device.limits.clone())
     }
 
-    pub fn device_downlevel_properties<B: GfxBackend>(
+    pub fn device_downlevel_properties<A: HalApi>(
         &self,
         device_id: id::DeviceId,
-    ) -> Result<wgt::DownlevelProperties, InvalidDevice> {
-        let hub = B::hub(self);
+    ) -> Result<wgt::DownlevelCapabilities, InvalidDevice> {
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, _) = hub.devices.read(&mut token);
         let device = device_guard.get(device_id).map_err(|_| InvalidDevice)?;
 
-        Ok(device.downlevel)
+        Ok(device.downlevel.clone())
     }
 
-    pub fn device_create_buffer<B: GfxBackend>(
+    pub fn device_create_buffer<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &resource::BufferDescriptor,
         id_in: Input<G, id::BufferId>,
     ) -> (id::BufferId, Option<resource::CreateBufferError>) {
         profiling::scope!("create_buffer", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.buffers.prepare(id_in);
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
                 Err(_) => break DeviceError::Invalid.into(),
             };
             #[cfg(feature = "trace")]
             if let Some(ref trace) = device.trace {
                 let mut desc = desc.clone();
                 let mapped_at_creation = mem::replace(&mut desc.mapped_at_creation, false);
-                if mapped_at_creation && !desc.usage.contains(wgt::BufferUsage::MAP_WRITE) {
-                    desc.usage |= wgt::BufferUsage::COPY_DST;
+                if mapped_at_creation && !desc.usage.contains(wgt::BufferUsages::MAP_WRITE) {
+                    desc.usage |= wgt::BufferUsages::COPY_DST;
                 }
                 trace
                     .lock()
                     .add(trace::Action::CreateBuffer(fid.id(), desc));
             }
 
             let mut buffer = match device.create_buffer(device_id, desc, false) {
                 Ok(buffer) => buffer,
                 Err(e) => break e,
             };
             let ref_count = buffer.life_guard.add_ref();
 
             let buffer_use = if !desc.mapped_at_creation {
-                resource::BufferUse::EMPTY
-            } else if desc.usage.contains(wgt::BufferUsage::MAP_WRITE) {
+                hal::BufferUses::empty()
+            } else if desc.usage.contains(wgt::BufferUsages::MAP_WRITE) {
                 // buffer is mappable, so we are just doing that at start
                 let map_size = buffer.size;
                 let ptr = match map_buffer(&device.raw, &mut buffer, 0, map_size, HostMap::Write) {
                     Ok(ptr) => ptr,
                     Err(e) => {
-                        let (raw, memory) = buffer.raw.unwrap();
-                        device.lock_life(&mut token).schedule_resource_destruction(
-                            queue::TempResource::Buffer(raw),
-                            memory,
-                            !0,
-                        );
+                        let raw = buffer.raw.unwrap();
+                        device
+                            .lock_life(&mut token)
+                            .schedule_resource_destruction(queue::TempResource::Buffer(raw), !0);
                         break e.into();
                     }
                 };
                 buffer.map_state = resource::BufferMapState::Active {
                     ptr,
-                    sub_range: hal::buffer::SubRange::WHOLE,
+                    range: 0..map_size,
                     host: HostMap::Write,
                 };
-                resource::BufferUse::MAP_WRITE
+                hal::BufferUses::MAP_WRITE
             } else {
                 // buffer needs staging area for initialization only
                 let stage_desc = wgt::BufferDescriptor {
                     label: Some(Cow::Borrowed("<init_buffer>")),
                     size: desc.size,
-                    usage: wgt::BufferUsage::MAP_WRITE | wgt::BufferUsage::COPY_SRC,
+                    usage: wgt::BufferUsages::MAP_WRITE | wgt::BufferUsages::COPY_SRC,
                     mapped_at_creation: false,
                 };
                 let mut stage = match device.create_buffer(device_id, &stage_desc, true) {
                     Ok(stage) => stage,
                     Err(e) => {
-                        let (raw, memory) = buffer.raw.unwrap();
-                        device.lock_life(&mut token).schedule_resource_destruction(
-                            queue::TempResource::Buffer(raw),
-                            memory,
-                            !0,
-                        );
+                        let raw = buffer.raw.unwrap();
+                        device
+                            .lock_life(&mut token)
+                            .schedule_resource_destruction(queue::TempResource::Buffer(raw), !0);
                         break e;
                     }
                 };
-                let (stage_buffer, mut stage_memory) = stage.raw.unwrap();
-                let ptr = match stage_memory.map(&device.raw, 0, stage.size) {
-                    Ok(ptr) => ptr,
+                let stage_buffer = stage.raw.unwrap();
+                let mapping = match unsafe { device.raw.map_buffer(&stage_buffer, 0..stage.size) } {
+                    Ok(mapping) => mapping,
                     Err(e) => {
-                        let (raw, memory) = buffer.raw.unwrap();
+                        let raw = buffer.raw.unwrap();
                         let mut life_lock = device.lock_life(&mut token);
+                        life_lock
+                            .schedule_resource_destruction(queue::TempResource::Buffer(raw), !0);
                         life_lock.schedule_resource_destruction(
-                            queue::TempResource::Buffer(raw),
-                            memory,
+                            queue::TempResource::Buffer(stage_buffer),
                             !0,
                         );
-                        life_lock.schedule_resource_destruction(
-                            queue::TempResource::Buffer(stage_buffer),
-                            stage_memory,
-                            !0,
-                        );
-                        break e.into();
+                        break DeviceError::from(e).into();
                     }
                 };
 
                 // Zero initialize memory and then mark both staging and buffer as initialized
                 // (it's guaranteed that this is the case by the time the buffer is usable)
-                unsafe { ptr::write_bytes(ptr.as_ptr(), 0, buffer.size as usize) };
+                unsafe { ptr::write_bytes(mapping.ptr.as_ptr(), 0, buffer.size as usize) };
                 buffer.initialization_status.clear(0..buffer.size);
                 stage.initialization_status.clear(0..buffer.size);
 
                 buffer.map_state = resource::BufferMapState::Init {
-                    ptr,
-                    needs_flush: !stage_memory.is_coherent(),
+                    ptr: mapping.ptr,
+                    needs_flush: !mapping.is_coherent,
                     stage_buffer,
-                    stage_memory,
                 };
-                resource::BufferUse::COPY_DST
+                hal::BufferUses::COPY_DST
             };
 
             let id = fid.assign(buffer, &mut token);
             log::info!("Created buffer {:?} with {:?}", id, desc);
 
             device
                 .trackers
                 .lock()
@@ -3024,127 +2789,152 @@ impl<G: GlobalIdentityHandlerFactory> Gl
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
     #[cfg(feature = "replay")]
-    pub fn device_wait_for_buffer<B: GfxBackend>(
+    pub fn device_wait_for_buffer<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         buffer_id: id::BufferId,
     ) -> Result<(), WaitIdleError> {
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let last_submission = {
             let (buffer_guard, _) = hub.buffers.write(&mut token);
             match buffer_guard.get(buffer_id) {
                 Ok(buffer) => buffer.life_guard.submission_index.load(Ordering::Acquire),
                 Err(_) => return Ok(()),
             }
         };
 
         device_guard
             .get(device_id)
             .map_err(|_| DeviceError::Invalid)?
             .wait_for_submit(last_submission, &mut token)
     }
 
-    pub fn device_set_buffer_sub_data<B: GfxBackend>(
+    #[doc(hidden)]
+    pub fn device_set_buffer_sub_data<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         buffer_id: id::BufferId,
         offset: BufferAddress,
         data: &[u8],
     ) -> Result<(), resource::BufferAccessError> {
         profiling::scope!("set_buffer_sub_data", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let (mut buffer_guard, _) = hub.buffers.write(&mut token);
         let device = device_guard
             .get(device_id)
             .map_err(|_| DeviceError::Invalid)?;
         let buffer = buffer_guard
             .get_mut(buffer_id)
             .map_err(|_| resource::BufferAccessError::Invalid)?;
-        check_buffer_usage(buffer.usage, wgt::BufferUsage::MAP_WRITE)?;
+        check_buffer_usage(buffer.usage, wgt::BufferUsages::MAP_WRITE)?;
         //assert!(buffer isn't used by the GPU);
 
         #[cfg(feature = "trace")]
         if let Some(ref trace) = device.trace {
             let mut trace = trace.lock();
             let data_path = trace.make_binary("bin", data);
             trace.add(trace::Action::WriteBuffer {
                 id: buffer_id,
                 data: data_path,
                 range: offset..offset + data.len() as BufferAddress,
                 queued: false,
             });
         }
 
-        buffer
-            .raw
-            .as_mut()
-            .unwrap()
-            .1
-            .write_bytes(&device.raw, offset, data)?;
+        let raw_buf = buffer.raw.as_ref().unwrap();
+        unsafe {
+            let mapping = device
+                .raw
+                .map_buffer(raw_buf, offset..offset + data.len() as u64)
+                .map_err(DeviceError::from)?;
+            ptr::copy_nonoverlapping(data.as_ptr(), mapping.ptr.as_ptr(), data.len());
+            if !mapping.is_coherent {
+                device
+                    .raw
+                    .flush_mapped_ranges(raw_buf, iter::once(offset..offset + data.len() as u64));
+            }
+            device
+                .raw
+                .unmap_buffer(raw_buf)
+                .map_err(DeviceError::from)?;
+        }
 
         Ok(())
     }
 
-    pub fn device_get_buffer_sub_data<B: GfxBackend>(
+    #[doc(hidden)]
+    pub fn device_get_buffer_sub_data<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         buffer_id: id::BufferId,
         offset: BufferAddress,
         data: &mut [u8],
     ) -> Result<(), resource::BufferAccessError> {
         profiling::scope!("get_buffer_sub_data", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let (mut buffer_guard, _) = hub.buffers.write(&mut token);
         let device = device_guard
             .get(device_id)
             .map_err(|_| DeviceError::Invalid)?;
         let buffer = buffer_guard
             .get_mut(buffer_id)
             .map_err(|_| resource::BufferAccessError::Invalid)?;
-        check_buffer_usage(buffer.usage, wgt::BufferUsage::MAP_READ)?;
+        check_buffer_usage(buffer.usage, wgt::BufferUsages::MAP_READ)?;
         //assert!(buffer isn't used by the GPU);
 
-        buffer
-            .raw
-            .as_mut()
-            .unwrap()
-            .1
-            .read_bytes(&device.raw, offset, data)?;
+        let raw_buf = buffer.raw.as_ref().unwrap();
+        unsafe {
+            let mapping = device
+                .raw
+                .map_buffer(raw_buf, offset..offset + data.len() as u64)
+                .map_err(DeviceError::from)?;
+            if !mapping.is_coherent {
+                device.raw.invalidate_mapped_ranges(
+                    raw_buf,
+                    iter::once(offset..offset + data.len() as u64),
+                );
+            }
+            ptr::copy_nonoverlapping(mapping.ptr.as_ptr(), data.as_mut_ptr(), data.len());
+            device
+                .raw
+                .unmap_buffer(raw_buf)
+                .map_err(DeviceError::from)?;
+        }
 
         Ok(())
     }
 
-    pub fn buffer_label<B: GfxBackend>(&self, id: id::BufferId) -> String {
-        B::hub(self).buffers.label_for_resource(id)
+    pub fn buffer_label<A: HalApi>(&self, id: id::BufferId) -> String {
+        A::hub(self).buffers.label_for_resource(id)
     }
 
-    pub fn buffer_destroy<B: GfxBackend>(
+    pub fn buffer_destroy<A: HalApi>(
         &self,
         buffer_id: id::BufferId,
     ) -> Result<(), resource::DestroyError> {
         profiling::scope!("destroy", "Buffer");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         //TODO: lock pending writes separately, keep the device read-only
         let (mut device_guard, mut token) = hub.devices.write(&mut token);
 
         log::info!("Buffer {:?} is destroyed", buffer_id);
         let (mut buffer_guard, _) = hub.buffers.write(&mut token);
         let buffer = buffer_guard
@@ -3153,41 +2943,39 @@ impl<G: GlobalIdentityHandlerFactory> Gl
 
         let device = &mut device_guard[buffer.device_id.value];
 
         #[cfg(feature = "trace")]
         if let Some(ref trace) = device.trace {
             trace.lock().add(trace::Action::FreeBuffer(buffer_id));
         }
 
-        let (raw, memory) = buffer
+        let raw = buffer
             .raw
             .take()
             .ok_or(resource::DestroyError::AlreadyDestroyed)?;
         let temp = queue::TempResource::Buffer(raw);
 
         if device.pending_writes.dst_buffers.contains(&buffer_id) {
-            device.pending_writes.temp_resources.push((temp, memory));
+            device.pending_writes.temp_resources.push(temp);
         } else {
             let last_submit_index = buffer.life_guard.submission_index.load(Ordering::Acquire);
             drop(buffer_guard);
-            device.lock_life(&mut token).schedule_resource_destruction(
-                temp,
-                memory,
-                last_submit_index,
-            );
+            device
+                .lock_life(&mut token)
+                .schedule_resource_destruction(temp, last_submit_index);
         }
 
         Ok(())
     }
 
-    pub fn buffer_drop<B: GfxBackend>(&self, buffer_id: id::BufferId, wait: bool) {
+    pub fn buffer_drop<A: HalApi>(&self, buffer_id: id::BufferId, wait: bool) {
         profiling::scope!("drop", "Buffer");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         log::info!("Buffer {:?} is dropped", buffer_id);
         let (ref_count, last_submit_index, device_id) = {
             let (mut buffer_guard, _) = hub.buffers.write(&mut token);
             match buffer_guard.get_mut(buffer_id) {
                 Ok(buffer) => {
                     let ref_count = buffer.life_guard.ref_count.take().unwrap();
@@ -3223,25 +3011,25 @@ impl<G: GlobalIdentityHandlerFactory> Gl
         if wait {
             match device.wait_for_submit(last_submit_index, &mut token) {
                 Ok(()) => (),
                 Err(e) => log::error!("Failed to wait for buffer {:?}: {:?}", buffer_id, e),
             }
         }
     }
 
-    pub fn device_create_texture<B: GfxBackend>(
+    pub fn device_create_texture<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &resource::TextureDescriptor,
         id_in: Input<G, id::TextureId>,
     ) -> (id::TextureId, Option<resource::CreateTextureError>) {
         profiling::scope!("create_texture", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.textures.prepare(id_in);
 
         let (adapter_guard, mut token) = hub.adapters.read(&mut token);
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
@@ -3274,27 +3062,86 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                 .unwrap();
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
-    pub fn texture_label<B: GfxBackend>(&self, id: id::TextureId) -> String {
-        B::hub(self).textures.label_for_resource(id)
+    /// # Safety
+    ///
+    /// - `hal_texture` must be created from `device_id` corresponding raw handle.
+    /// - `hal_texture` must be created respecting `desc`
+    pub unsafe fn create_texture_from_hal<A: HalApi>(
+        &self,
+        hal_texture: A::Texture,
+        device_id: id::DeviceId,
+        desc: &resource::TextureDescriptor,
+        id_in: Input<G, id::TextureId>,
+    ) -> (id::TextureId, Option<resource::CreateTextureError>) {
+        profiling::scope!("create_texture", "Device");
+
+        let hub = A::hub(self);
+        let mut token = Token::root();
+        let fid = hub.textures.prepare(id_in);
+
+        let (adapter_guard, mut token) = hub.adapters.read(&mut token);
+        let (device_guard, mut token) = hub.devices.read(&mut token);
+        let error = loop {
+            let device = match device_guard.get(device_id) {
+                Ok(device) => device,
+                Err(_) => break DeviceError::Invalid.into(),
+            };
+
+            // NB: Any change done through the raw texture handle will not be recorded in the replay
+            #[cfg(feature = "trace")]
+            if let Some(ref trace) = device.trace {
+                trace
+                    .lock()
+                    .add(trace::Action::CreateTexture(fid.id(), desc.clone()));
+            }
+
+            let adapter = &adapter_guard[device.adapter_id.value];
+            let texture =
+                match device.create_texture_from_hal(hal_texture, device_id, adapter, desc) {
+                    Ok(texture) => texture,
+                    Err(error) => break error,
+                };
+            let num_levels = texture.full_range.levels.end;
+            let num_layers = texture.full_range.layers.end;
+            let ref_count = texture.life_guard.add_ref();
+
+            let id = fid.assign(texture, &mut token);
+            log::info!("Created texture {:?} with {:?}", id, desc);
+
+            device
+                .trackers
+                .lock()
+                .textures
+                .init(id, ref_count, TextureState::new(num_levels, num_layers))
+                .unwrap();
+            return (id.0, None);
+        };
+
+        let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
+        (id, Some(error))
     }
 
-    pub fn texture_destroy<B: GfxBackend>(
+    pub fn texture_label<A: HalApi>(&self, id: id::TextureId) -> String {
+        A::hub(self).textures.label_for_resource(id)
+    }
+
+    pub fn texture_destroy<A: HalApi>(
         &self,
         texture_id: id::TextureId,
     ) -> Result<(), resource::DestroyError> {
         profiling::scope!("destroy", "Texture");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         //TODO: lock pending writes separately, keep the device read-only
         let (mut device_guard, mut token) = hub.devices.write(&mut token);
 
         log::info!("Buffer {:?} is destroyed", texture_id);
         let (mut texture_guard, _) = hub.textures.write(&mut token);
         let texture = texture_guard
@@ -3303,41 +3150,42 @@ impl<G: GlobalIdentityHandlerFactory> Gl
 
         let device = &mut device_guard[texture.device_id.value];
 
         #[cfg(feature = "trace")]
         if let Some(ref trace) = device.trace {
             trace.lock().add(trace::Action::FreeTexture(texture_id));
         }
 
-        let (raw, memory) = texture
-            .raw
-            .take()
-            .ok_or(resource::DestroyError::AlreadyDestroyed)?;
-        let temp = queue::TempResource::Image(raw);
-
-        if device.pending_writes.dst_textures.contains(&texture_id) {
-            device.pending_writes.temp_resources.push((temp, memory));
-        } else {
-            let last_submit_index = texture.life_guard.submission_index.load(Ordering::Acquire);
-            drop(texture_guard);
-            device.lock_life(&mut token).schedule_resource_destruction(
-                temp,
-                memory,
-                last_submit_index,
-            );
+        match texture.inner {
+            resource::TextureInner::Native { ref mut raw } => {
+                let raw = raw.take().ok_or(resource::DestroyError::AlreadyDestroyed)?;
+                let temp = queue::TempResource::Texture(raw);
+
+                if device.pending_writes.dst_textures.contains(&texture_id) {
+                    device.pending_writes.temp_resources.push(temp);
+                } else {
+                    let last_submit_index =
+                        texture.life_guard.submission_index.load(Ordering::Acquire);
+                    drop(texture_guard);
+                    device
+                        .lock_life(&mut token)
+                        .schedule_resource_destruction(temp, last_submit_index);
+                }
+            }
+            resource::TextureInner::Surface { .. } => {} //TODO
         }
 
         Ok(())
     }
 
-    pub fn texture_drop<B: GfxBackend>(&self, texture_id: id::TextureId, wait: bool) {
+    pub fn texture_drop<A: HalApi>(&self, texture_id: id::TextureId, wait: bool) {
         profiling::scope!("drop", "Texture");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let (ref_count, last_submit_index, device_id) = {
             let (mut texture_guard, _) = hub.textures.write(&mut token);
             match texture_guard.get_mut(texture_id) {
                 Ok(texture) => {
                     let ref_count = texture.life_guard.ref_count.take().unwrap();
                     let last_submit_index =
@@ -3373,25 +3221,25 @@ impl<G: GlobalIdentityHandlerFactory> Gl
         if wait {
             match device.wait_for_submit(last_submit_index, &mut token) {
                 Ok(()) => (),
                 Err(e) => log::error!("Failed to wait for texture {:?}: {:?}", texture_id, e),
             }
         }
     }
 
-    pub fn texture_create_view<B: GfxBackend>(
+    pub fn texture_create_view<A: HalApi>(
         &self,
         texture_id: id::TextureId,
         desc: &resource::TextureViewDescriptor,
         id_in: Input<G, id::TextureViewId>,
     ) -> (id::TextureViewId, Option<resource::CreateTextureViewError>) {
         profiling::scope!("create_view", "Texture");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.texture_views.prepare(id_in);
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let (texture_guard, mut token) = hub.textures.read(&mut token);
         let error = loop {
             let texture = match texture_guard.get(texture_id) {
                 Ok(texture) => texture,
@@ -3422,47 +3270,40 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                 .unwrap();
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
-    pub fn texture_view_label<B: GfxBackend>(&self, id: id::TextureViewId) -> String {
-        B::hub(self).texture_views.label_for_resource(id)
+    pub fn texture_view_label<A: HalApi>(&self, id: id::TextureViewId) -> String {
+        A::hub(self).texture_views.label_for_resource(id)
     }
 
-    pub fn texture_view_drop<B: GfxBackend>(
+    pub fn texture_view_drop<A: HalApi>(
         &self,
         texture_view_id: id::TextureViewId,
         wait: bool,
     ) -> Result<(), resource::TextureViewDestroyError> {
         profiling::scope!("drop", "TextureView");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let (last_submit_index, device_id) = {
             let (texture_guard, mut token) = hub.textures.read(&mut token);
             let (mut texture_view_guard, _) = hub.texture_views.write(&mut token);
 
             match texture_view_guard.get_mut(texture_view_id) {
                 Ok(view) => {
                     let _ref_count = view.life_guard.ref_count.take();
                     let last_submit_index =
                         view.life_guard.submission_index.load(Ordering::Acquire);
-                    let device_id = match view.inner {
-                        resource::TextureViewInner::Native { ref source_id, .. } => {
-                            texture_guard[source_id.value].device_id.value
-                        }
-                        resource::TextureViewInner::SwapChain { .. } => {
-                            return Err(resource::TextureViewDestroyError::SwapChainImage)
-                        }
-                    };
+                    let device_id = texture_guard[view.parent_id.value].device_id.value;
                     (last_submit_index, device_id)
                 }
                 Err(InvalidId) => {
                     hub.texture_views
                         .unregister_locked(texture_view_id, &mut *texture_view_guard);
                     return Ok(());
                 }
             }
@@ -3484,25 +3325,25 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                     texture_view_id,
                     e
                 ),
             }
         }
         Ok(())
     }
 
-    pub fn device_create_sampler<B: GfxBackend>(
+    pub fn device_create_sampler<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &resource::SamplerDescriptor,
         id_in: Input<G, id::SamplerId>,
     ) -> (id::SamplerId, Option<resource::CreateSamplerError>) {
         profiling::scope!("create_sampler", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.samplers.prepare(id_in);
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
                 Err(_) => break DeviceError::Invalid.into(),
@@ -3529,24 +3370,24 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                 .unwrap();
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
-    pub fn sampler_label<B: GfxBackend>(&self, id: id::SamplerId) -> String {
-        B::hub(self).samplers.label_for_resource(id)
+    pub fn sampler_label<A: HalApi>(&self, id: id::SamplerId) -> String {
+        A::hub(self).samplers.label_for_resource(id)
     }
 
-    pub fn sampler_drop<B: GfxBackend>(&self, sampler_id: id::SamplerId) {
+    pub fn sampler_drop<A: HalApi>(&self, sampler_id: id::SamplerId) {
         profiling::scope!("drop", "Sampler");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let device_id = {
             let (mut sampler_guard, _) = hub.samplers.write(&mut token);
             match sampler_guard.get_mut(sampler_id) {
                 Ok(sampler) => {
                     sampler.life_guard.ref_count.take();
                     sampler.device_id.value
@@ -3562,29 +3403,29 @@ impl<G: GlobalIdentityHandlerFactory> Gl
         let (device_guard, mut token) = hub.devices.read(&mut token);
         device_guard[device_id]
             .lock_life(&mut token)
             .suspected_resources
             .samplers
             .push(id::Valid(sampler_id));
     }
 
-    pub fn device_create_bind_group_layout<B: GfxBackend>(
+    pub fn device_create_bind_group_layout<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &binding_model::BindGroupLayoutDescriptor,
         id_in: Input<G, id::BindGroupLayoutId>,
     ) -> (
         id::BindGroupLayoutId,
         Option<binding_model::CreateBindGroupLayoutError>,
     ) {
         profiling::scope!("create_bind_group_layout", "Device");
 
         let mut token = Token::root();
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let fid = hub.bind_group_layouts.prepare(id_in);
 
         let error = 'outer: loop {
             let (device_guard, mut token) = hub.devices.read(&mut token);
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
                 Err(_) => break DeviceError::Invalid.into(),
             };
@@ -3613,42 +3454,39 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                     Device::deduplicate_bind_group_layout(device_id, &entry_map, &*bgl_guard)
                 {
                     return (id, None);
                 }
             }
 
             let layout = match device.create_bind_group_layout(
                 device_id,
-                desc.label.as_ref().map(|cow| cow.as_ref()),
+                desc.label.borrow_option(),
                 entry_map,
             ) {
                 Ok(layout) => layout,
                 Err(e) => break e,
             };
 
             let id = fid.assign(layout, &mut token);
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
-    pub fn bind_group_layout_label<B: GfxBackend>(&self, id: id::BindGroupLayoutId) -> String {
-        B::hub(self).bind_group_layouts.label_for_resource(id)
+    pub fn bind_group_layout_label<A: HalApi>(&self, id: id::BindGroupLayoutId) -> String {
+        A::hub(self).bind_group_layouts.label_for_resource(id)
     }
 
-    pub fn bind_group_layout_drop<B: GfxBackend>(
-        &self,
-        bind_group_layout_id: id::BindGroupLayoutId,
-    ) {
+    pub fn bind_group_layout_drop<A: HalApi>(&self, bind_group_layout_id: id::BindGroupLayoutId) {
         profiling::scope!("drop", "BindGroupLayout");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let device_id = {
             let (mut bind_group_layout_guard, _) = hub.bind_group_layouts.write(&mut token);
             match bind_group_layout_guard.get_mut(bind_group_layout_id) {
                 Ok(layout) => layout.device_id.value,
                 Err(InvalidId) => {
                     hub.bind_group_layouts
                         .unregister_locked(bind_group_layout_id, &mut *bind_group_layout_guard);
@@ -3660,28 +3498,28 @@ impl<G: GlobalIdentityHandlerFactory> Gl
         let (device_guard, mut token) = hub.devices.read(&mut token);
         device_guard[device_id]
             .lock_life(&mut token)
             .suspected_resources
             .bind_group_layouts
             .push(id::Valid(bind_group_layout_id));
     }
 
-    pub fn device_create_pipeline_layout<B: GfxBackend>(
+    pub fn device_create_pipeline_layout<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &binding_model::PipelineLayoutDescriptor,
         id_in: Input<G, id::PipelineLayoutId>,
     ) -> (
         id::PipelineLayoutId,
         Option<binding_model::CreatePipelineLayoutError>,
     ) {
         profiling::scope!("create_pipeline_layout", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.pipeline_layouts.prepare(id_in);
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
                 Err(_) => break DeviceError::Invalid.into(),
@@ -3704,24 +3542,24 @@ impl<G: GlobalIdentityHandlerFactory> Gl
             let id = fid.assign(layout, &mut token);
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
-    pub fn pipeline_layout_label<B: GfxBackend>(&self, id: id::PipelineLayoutId) -> String {
-        B::hub(self).pipeline_layouts.label_for_resource(id)
+    pub fn pipeline_layout_label<A: HalApi>(&self, id: id::PipelineLayoutId) -> String {
+        A::hub(self).pipeline_layouts.label_for_resource(id)
     }
 
-    pub fn pipeline_layout_drop<B: GfxBackend>(&self, pipeline_layout_id: id::PipelineLayoutId) {
+    pub fn pipeline_layout_drop<A: HalApi>(&self, pipeline_layout_id: id::PipelineLayoutId) {
         profiling::scope!("drop", "PipelineLayout");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_id, ref_count) = {
             let (mut pipeline_layout_guard, _) = hub.pipeline_layouts.write(&mut token);
             match pipeline_layout_guard.get_mut(pipeline_layout_id) {
                 Ok(layout) => (
                     layout.device_id.value,
                     layout.life_guard.ref_count.take().unwrap(),
                 ),
@@ -3739,25 +3577,25 @@ impl<G: GlobalIdentityHandlerFactory> Gl
             .suspected_resources
             .pipeline_layouts
             .push(Stored {
                 value: id::Valid(pipeline_layout_id),
                 ref_count,
             });
     }
 
-    pub fn device_create_bind_group<B: GfxBackend>(
+    pub fn device_create_bind_group<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &binding_model::BindGroupDescriptor,
         id_in: Input<G, id::BindGroupId>,
     ) -> (id::BindGroupId, Option<binding_model::CreateBindGroupError>) {
         profiling::scope!("create_bind_group", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.bind_groups.prepare(id_in);
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let (bind_group_layout_guard, mut token) = hub.bind_group_layouts.read(&mut token);
 
         let error = loop {
             let device = match device_guard.get(device_id) {
@@ -3770,26 +3608,22 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                     .lock()
                     .add(trace::Action::CreateBindGroup(fid.id(), desc.clone()));
             }
 
             let bind_group_layout = match bind_group_layout_guard.get(desc.layout) {
                 Ok(layout) => layout,
                 Err(_) => break binding_model::CreateBindGroupError::InvalidLayout,
             };
-            let bind_group = match device.create_bind_group(
-                device_id,
-                bind_group_layout,
-                desc,
-                &hub,
-                &mut token,
-            ) {
-                Ok(bind_group) => bind_group,
-                Err(e) => break e,
-            };
+            let bind_group =
+                match device.create_bind_group(device_id, bind_group_layout, desc, hub, &mut token)
+                {
+                    Ok(bind_group) => bind_group,
+                    Err(e) => break e,
+                };
             let ref_count = bind_group.life_guard.add_ref();
 
             let id = fid.assign(bind_group, &mut token);
             log::debug!(
                 "Bind group {:?} {:#?}",
                 id,
                 hub.bind_groups.read(&mut token).0[id].used
             );
@@ -3802,24 +3636,24 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                 .unwrap();
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
-    pub fn bind_group_label<B: GfxBackend>(&self, id: id::BindGroupId) -> String {
-        B::hub(self).bind_groups.label_for_resource(id)
+    pub fn bind_group_label<A: HalApi>(&self, id: id::BindGroupId) -> String {
+        A::hub(self).bind_groups.label_for_resource(id)
     }
 
-    pub fn bind_group_drop<B: GfxBackend>(&self, bind_group_id: id::BindGroupId) {
+    pub fn bind_group_drop<A: HalApi>(&self, bind_group_id: id::BindGroupId) {
         profiling::scope!("drop", "BindGroup");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let device_id = {
             let (mut bind_group_guard, _) = hub.bind_groups.write(&mut token);
             match bind_group_guard.get_mut(bind_group_id) {
                 Ok(bind_group) => {
                     bind_group.life_guard.ref_count.take();
                     bind_group.device_id.value
@@ -3835,53 +3669,50 @@ impl<G: GlobalIdentityHandlerFactory> Gl
         let (device_guard, mut token) = hub.devices.read(&mut token);
         device_guard[device_id]
             .lock_life(&mut token)
             .suspected_resources
             .bind_groups
             .push(id::Valid(bind_group_id));
     }
 
-    pub fn device_create_shader_module<B: GfxBackend>(
+    pub fn device_create_shader_module<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &pipeline::ShaderModuleDescriptor,
         source: pipeline::ShaderModuleSource,
         id_in: Input<G, id::ShaderModuleId>,
     ) -> (
         id::ShaderModuleId,
         Option<pipeline::CreateShaderModuleError>,
     ) {
         profiling::scope!("create_shader_module", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.shader_modules.prepare(id_in);
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
                 Err(_) => break DeviceError::Invalid.into(),
             };
             #[cfg(feature = "trace")]
             if let Some(ref trace) = device.trace {
                 let mut trace = trace.lock();
                 let data = match source {
-                    pipeline::ShaderModuleSource::SpirV(ref spv) => {
-                        trace.make_binary("spv", unsafe {
-                            std::slice::from_raw_parts(spv.as_ptr() as *const u8, spv.len() * 4)
-                        })
-                    }
                     pipeline::ShaderModuleSource::Wgsl(ref code) => {
                         trace.make_binary("wgsl", code.as_bytes())
                     }
-                    pipeline::ShaderModuleSource::Naga(_) => {
-                        // we don't want to enable Naga serialization just for this alone
-                        trace.make_binary("ron", &[])
+                    pipeline::ShaderModuleSource::Naga(ref module) => {
+                        let string =
+                            ron::ser::to_string_pretty(module, ron::ser::PrettyConfig::default())
+                                .unwrap();
+                        trace.make_binary("ron", string.as_bytes())
                     }
                 };
                 trace.add(trace::Action::CreateShaderModule {
                     id: fid.id(),
                     desc: desc.clone(),
                     data,
                 });
             };
@@ -3893,24 +3724,76 @@ impl<G: GlobalIdentityHandlerFactory> Gl
             let id = fid.assign(shader, &mut token);
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
-    pub fn shader_module_label<B: GfxBackend>(&self, id: id::ShaderModuleId) -> String {
-        B::hub(self).shader_modules.label_for_resource(id)
+    #[allow(unused_unsafe)] // Unsafe-ness of internal calls has little to do with unsafe-ness of this.
+    /// # Safety
+    ///
+    /// This function passes SPIR-V binary to the backend as-is and can potentially result in a
+    /// driver crash.
+    pub unsafe fn device_create_shader_module_spirv<A: HalApi>(
+        &self,
+        device_id: id::DeviceId,
+        desc: &pipeline::ShaderModuleDescriptor,
+        source: Cow<[u32]>,
+        id_in: Input<G, id::ShaderModuleId>,
+    ) -> (
+        id::ShaderModuleId,
+        Option<pipeline::CreateShaderModuleError>,
+    ) {
+        profiling::scope!("create_shader_module", "Device");
+
+        let hub = A::hub(self);
+        let mut token = Token::root();
+        let fid = hub.shader_modules.prepare(id_in);
+
+        let (device_guard, mut token) = hub.devices.read(&mut token);
+        let error = loop {
+            let device = match device_guard.get(device_id) {
+                Ok(device) => device,
+                Err(_) => break DeviceError::Invalid.into(),
+            };
+            #[cfg(feature = "trace")]
+            if let Some(ref trace) = device.trace {
+                let mut trace = trace.lock();
+                let data = trace.make_binary("spv", unsafe {
+                    std::slice::from_raw_parts(source.as_ptr() as *const u8, source.len() * 4)
+                });
+                trace.add(trace::Action::CreateShaderModule {
+                    id: fid.id(),
+                    desc: desc.clone(),
+                    data,
+                });
+            };
+
+            let shader = match device.create_shader_module_spirv(device_id, desc, &source) {
+                Ok(shader) => shader,
+                Err(e) => break e,
+            };
+            let id = fid.assign(shader, &mut token);
+            return (id.0, None);
+        };
+
+        let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
+        (id, Some(error))
     }
 
-    pub fn shader_module_drop<B: GfxBackend>(&self, shader_module_id: id::ShaderModuleId) {
+    pub fn shader_module_label<A: HalApi>(&self, id: id::ShaderModuleId) -> String {
+        A::hub(self).shader_modules.label_for_resource(id)
+    }
+
+    pub fn shader_module_drop<A: HalApi>(&self, shader_module_id: id::ShaderModuleId) {
         profiling::scope!("drop", "ShaderModule");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let (module, _) = hub.shader_modules.unregister(shader_module_id, &mut token);
         if let Some(module) = module {
             let device = &device_guard[module.device_id.value];
             #[cfg(feature = "trace")]
             if let Some(ref trace) = device.trace {
                 trace
@@ -3918,95 +3801,88 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                     .add(trace::Action::DestroyShaderModule(shader_module_id));
             }
             unsafe {
                 device.raw.destroy_shader_module(module.raw);
             }
         }
     }
 
-    pub fn device_create_command_encoder<B: GfxBackend>(
+    pub fn device_create_command_encoder<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &wgt::CommandEncoderDescriptor<Label>,
         id_in: Input<G, id::CommandEncoderId>,
-    ) -> (id::CommandEncoderId, Option<command::CommandAllocatorError>) {
+    ) -> (id::CommandEncoderId, Option<DeviceError>) {
         profiling::scope!("create_command_encoder", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.command_buffers.prepare(id_in);
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
-                Err(_) => break DeviceError::Invalid.into(),
+                Err(_) => break DeviceError::Invalid,
             };
-
             let dev_stored = Stored {
                 value: id::Valid(device_id),
                 ref_count: device.life_guard.add_ref(),
             };
-
-            let mut command_buffer = match device.cmd_allocator.allocate(
+            let encoder = match device
+                .command_allocator
+                .lock()
+                .acquire_encoder(&device.raw, &device.queue)
+            {
+                Ok(raw) => raw,
+                Err(_) => break DeviceError::OutOfMemory,
+            };
+            let command_buffer = command::CommandBuffer::new(
+                encoder,
                 dev_stored,
-                &device.raw,
                 device.limits.clone(),
-                device.downlevel,
+                device.downlevel.clone(),
                 device.features,
-                device.private_features,
-                &desc.label,
                 #[cfg(feature = "trace")]
                 device.trace.is_some(),
-            ) {
-                Ok(cmd_buf) => cmd_buf,
-                Err(e) => break e,
-            };
-
-            let mut raw = command_buffer.raw.first_mut().unwrap();
-            unsafe {
-                if let Some(ref label) = desc.label {
-                    device.raw.set_command_buffer_name(&mut raw, label);
-                }
-                raw.begin_primary(hal::command::CommandBufferFlags::ONE_TIME_SUBMIT);
-            }
+                &desc.label,
+            );
 
             let id = fid.assign(command_buffer, &mut token);
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
-    pub fn command_buffer_label<B: GfxBackend>(&self, id: id::CommandBufferId) -> String {
-        B::hub(self).command_buffers.label_for_resource(id)
+    pub fn command_buffer_label<A: HalApi>(&self, id: id::CommandBufferId) -> String {
+        A::hub(self).command_buffers.label_for_resource(id)
     }
 
-    pub fn command_encoder_drop<B: GfxBackend>(&self, command_encoder_id: id::CommandEncoderId) {
+    pub fn command_encoder_drop<A: HalApi>(&self, command_encoder_id: id::CommandEncoderId) {
         profiling::scope!("drop", "CommandEncoder");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let (mut device_guard, mut token) = hub.devices.write(&mut token);
         let (cmdbuf, _) = hub
             .command_buffers
             .unregister(command_encoder_id, &mut token);
         if let Some(cmdbuf) = cmdbuf {
             let device = &mut device_guard[cmdbuf.device_id.value];
-            device.untrack::<G>(&hub, &cmdbuf.trackers, &mut token);
-            device.cmd_allocator.discard(cmdbuf);
+            device.untrack::<G>(hub, &cmdbuf.trackers, &mut token);
         }
     }
 
-    pub fn command_buffer_drop<B: GfxBackend>(&self, command_buffer_id: id::CommandBufferId) {
+    pub fn command_buffer_drop<A: HalApi>(&self, command_buffer_id: id::CommandBufferId) {
         profiling::scope!("drop", "CommandBuffer");
-        self.command_encoder_drop::<B>(command_buffer_id)
+        self.command_encoder_drop::<A>(command_buffer_id)
     }
 
     pub fn device_create_render_bundle_encoder(
         &self,
         device_id: id::DeviceId,
         desc: &command::RenderBundleEncoderDescriptor,
     ) -> (
         id::RenderBundleEncoderId,
@@ -4015,47 +3891,48 @@ impl<G: GlobalIdentityHandlerFactory> Gl
         profiling::scope!("create_render_bundle_encoder", "Device");
         let (encoder, error) = match command::RenderBundleEncoder::new(desc, device_id, None) {
             Ok(encoder) => (encoder, None),
             Err(e) => (command::RenderBundleEncoder::dummy(device_id), Some(e)),
         };
         (Box::into_raw(Box::new(encoder)), error)
     }
 
-    pub fn render_bundle_encoder_finish<B: GfxBackend>(
+    pub fn render_bundle_encoder_finish<A: HalApi>(
         &self,
         bundle_encoder: command::RenderBundleEncoder,
         desc: &command::RenderBundleDescriptor,
         id_in: Input<G, id::RenderBundleId>,
     ) -> (id::RenderBundleId, Option<command::RenderBundleError>) {
         profiling::scope!("finish", "RenderBundleEncoder");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.render_bundles.prepare(id_in);
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(bundle_encoder.parent()) {
                 Ok(device) => device,
                 Err(_) => break command::RenderBundleError::INVALID_DEVICE,
             };
             #[cfg(feature = "trace")]
             if let Some(ref trace) = device.trace {
                 trace.lock().add(trace::Action::CreateRenderBundle {
                     id: fid.id(),
                     desc: trace::new_render_bundle_encoder_descriptor(
                         desc.label.clone(),
                         &bundle_encoder.context,
+                        bundle_encoder.is_ds_read_only,
                     ),
                     base: bundle_encoder.to_base_pass(),
                 });
             }
 
-            let render_bundle = match bundle_encoder.finish(desc, device, &hub, &mut token) {
+            let render_bundle = match bundle_encoder.finish(desc, device, hub, &mut token) {
                 Ok(bundle) => bundle,
                 Err(e) => break e,
             };
 
             log::debug!("Render bundle {:#?}", render_bundle.used);
             let ref_count = render_bundle.life_guard.add_ref();
             let id = fid.assign(render_bundle, &mut token);
 
@@ -4067,23 +3944,23 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                 .unwrap();
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
-    pub fn render_bundle_label<B: GfxBackend>(&self, id: id::RenderBundleId) -> String {
-        B::hub(self).render_bundles.label_for_resource(id)
+    pub fn render_bundle_label<A: HalApi>(&self, id: id::RenderBundleId) -> String {
+        A::hub(self).render_bundles.label_for_resource(id)
     }
 
-    pub fn render_bundle_drop<B: GfxBackend>(&self, render_bundle_id: id::RenderBundleId) {
+    pub fn render_bundle_drop<A: HalApi>(&self, render_bundle_id: id::RenderBundleId) {
         profiling::scope!("drop", "RenderBundle");
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let device_id = {
             let (mut bundle_guard, _) = hub.render_bundles.write(&mut token);
             match bundle_guard.get_mut(render_bundle_id) {
                 Ok(bundle) => {
                     bundle.life_guard.ref_count.take();
@@ -4099,25 +3976,25 @@ impl<G: GlobalIdentityHandlerFactory> Gl
 
         device_guard[device_id]
             .lock_life(&mut token)
             .suspected_resources
             .render_bundles
             .push(id::Valid(render_bundle_id));
     }
 
-    pub fn device_create_query_set<B: GfxBackend>(
+    pub fn device_create_query_set<A: HalApi>(
         &self,
         device_id: id::DeviceId,
-        desc: &wgt::QuerySetDescriptor,
+        desc: &resource::QuerySetDescriptor,
         id_in: Input<G, id::QuerySetId>,
     ) -> (id::QuerySetId, Option<resource::CreateQuerySetError>) {
         profiling::scope!("create_query_set", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let fid = hub.query_sets.prepare(id_in);
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
                 Err(_) => break DeviceError::Invalid.into(),
@@ -4147,20 +4024,20 @@ impl<G: GlobalIdentityHandlerFactory> Gl
 
             return (id.0, None);
         };
 
         let id = fid.assign_error("", &mut token);
         (id, Some(error))
     }
 
-    pub fn query_set_drop<B: GfxBackend>(&self, query_set_id: id::QuerySetId) {
+    pub fn query_set_drop<A: HalApi>(&self, query_set_id: id::QuerySetId) {
         profiling::scope!("drop", "QuerySet");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let device_id = {
             let (mut query_set_guard, _) = hub.query_sets.write(&mut token);
             let query_set = query_set_guard.get_mut(query_set_id).unwrap();
             query_set.life_guard.ref_count.take();
             query_set.device_id.value
         };
@@ -4177,33 +4054,33 @@ impl<G: GlobalIdentityHandlerFactory> Gl
 
         device
             .lock_life(&mut token)
             .suspected_resources
             .query_sets
             .push(id::Valid(query_set_id));
     }
 
-    pub fn device_create_render_pipeline<B: GfxBackend>(
+    pub fn device_create_render_pipeline<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &pipeline::RenderPipelineDescriptor,
         id_in: Input<G, id::RenderPipelineId>,
         implicit_pipeline_ids: Option<ImplicitPipelineIds<G>>,
     ) -> (
         id::RenderPipelineId,
         Option<pipeline::CreateRenderPipelineError>,
     ) {
         profiling::scope!("create_render_pipeline", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let fid = hub.render_pipelines.prepare(id_in);
-        let implicit_context = implicit_pipeline_ids.map(|ipi| ipi.prepare(&hub));
+        let implicit_context = implicit_pipeline_ids.map(|ipi| ipi.prepare(hub));
 
         let (adapter_guard, mut token) = hub.adapters.read(&mut token);
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
                 Err(_) => break DeviceError::Invalid.into(),
             };
@@ -4217,43 +4094,43 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                 });
             }
 
             let pipeline = match device.create_render_pipeline(
                 device_id,
                 adapter,
                 desc,
                 implicit_context,
-                &hub,
+                hub,
                 &mut token,
             ) {
                 Ok(pair) => pair,
                 Err(e) => break e,
             };
 
             let id = fid.assign(pipeline, &mut token);
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
     /// Get an ID of one of the bind group layouts. The ID adds a refcount,
     /// which needs to be released by calling `bind_group_layout_drop`.
-    pub fn render_pipeline_get_bind_group_layout<B: GfxBackend>(
+    pub fn render_pipeline_get_bind_group_layout<A: HalApi>(
         &self,
         pipeline_id: id::RenderPipelineId,
         index: u32,
         id_in: Input<G, id::BindGroupLayoutId>,
     ) -> (
         id::BindGroupLayoutId,
         Option<binding_model::GetBindGroupLayoutError>,
     ) {
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (pipeline_layout_guard, mut token) = hub.pipeline_layouts.read(&mut token);
 
         let error = loop {
             let (bgl_guard, mut token) = hub.bind_group_layouts.read(&mut token);
             let (_, mut token) = hub.bind_groups.read(&mut token);
             let (pipeline_guard, _) = hub.render_pipelines.read(&mut token);
 
@@ -4275,23 +4152,23 @@ impl<G: GlobalIdentityHandlerFactory> Gl
 
         let id = hub
             .bind_group_layouts
             .prepare(id_in)
             .assign_error("<derived>", &mut token);
         (id, Some(error))
     }
 
-    pub fn render_pipeline_label<B: GfxBackend>(&self, id: id::RenderPipelineId) -> String {
-        B::hub(self).render_pipelines.label_for_resource(id)
+    pub fn render_pipeline_label<A: HalApi>(&self, id: id::RenderPipelineId) -> String {
+        A::hub(self).render_pipelines.label_for_resource(id)
     }
 
-    pub fn render_pipeline_drop<B: GfxBackend>(&self, render_pipeline_id: id::RenderPipelineId) {
+    pub fn render_pipeline_drop<A: HalApi>(&self, render_pipeline_id: id::RenderPipelineId) {
         profiling::scope!("drop", "RenderPipeline");
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, mut token) = hub.devices.read(&mut token);
 
         let (device_id, layout_id) = {
             let (mut pipeline_guard, _) = hub.render_pipelines.write(&mut token);
             match pipeline_guard.get_mut(render_pipeline_id) {
                 Ok(pipeline) => {
                     pipeline.life_guard.ref_count.take();
@@ -4311,33 +4188,33 @@ impl<G: GlobalIdentityHandlerFactory> Gl
             .render_pipelines
             .push(id::Valid(render_pipeline_id));
         life_lock
             .suspected_resources
             .pipeline_layouts
             .push(layout_id);
     }
 
-    pub fn device_create_compute_pipeline<B: GfxBackend>(
+    pub fn device_create_compute_pipeline<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         desc: &pipeline::ComputePipelineDescriptor,
         id_in: Input<G, id::ComputePipelineId>,
         implicit_pipeline_ids: Option<ImplicitPipelineIds<G>>,
     ) -> (
         id::ComputePipelineId,
         Option<pipeline::CreateComputePipelineError>,
     ) {
         profiling::scope!("create_compute_pipeline", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let fid = hub.compute_pipelines.prepare(id_in);
-        let implicit_context = implicit_pipeline_ids.map(|ipi| ipi.prepare(&hub));
+        let implicit_context = implicit_pipeline_ids.map(|ipi| ipi.prepare(hub));
 
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
                 Err(_) => break DeviceError::Invalid.into(),
             };
             #[cfg(feature = "trace")]
@@ -4348,43 +4225,43 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                     implicit_context: implicit_context.clone(),
                 });
             }
 
             let pipeline = match device.create_compute_pipeline(
                 device_id,
                 desc,
                 implicit_context,
-                &hub,
+                hub,
                 &mut token,
             ) {
                 Ok(pair) => pair,
                 Err(e) => break e,
             };
 
             let id = fid.assign(pipeline, &mut token);
             return (id.0, None);
         };
 
         let id = fid.assign_error(desc.label.borrow_or_default(), &mut token);
         (id, Some(error))
     }
 
     /// Get an ID of one of the bind group layouts. The ID adds a refcount,
     /// which needs to be released by calling `bind_group_layout_drop`.
-    pub fn compute_pipeline_get_bind_group_layout<B: GfxBackend>(
+    pub fn compute_pipeline_get_bind_group_layout<A: HalApi>(
         &self,
         pipeline_id: id::ComputePipelineId,
         index: u32,
         id_in: Input<G, id::BindGroupLayoutId>,
     ) -> (
         id::BindGroupLayoutId,
         Option<binding_model::GetBindGroupLayoutError>,
     ) {
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (pipeline_layout_guard, mut token) = hub.pipeline_layouts.read(&mut token);
 
         let error = loop {
             let (bgl_guard, mut token) = hub.bind_group_layouts.read(&mut token);
             let (_, mut token) = hub.bind_groups.read(&mut token);
             let (pipeline_guard, _) = hub.compute_pipelines.read(&mut token);
 
@@ -4406,23 +4283,23 @@ impl<G: GlobalIdentityHandlerFactory> Gl
 
         let id = hub
             .bind_group_layouts
             .prepare(id_in)
             .assign_error("<derived>", &mut token);
         (id, Some(error))
     }
 
-    pub fn compute_pipeline_label<B: GfxBackend>(&self, id: id::ComputePipelineId) -> String {
-        B::hub(self).compute_pipelines.label_for_resource(id)
+    pub fn compute_pipeline_label<A: HalApi>(&self, id: id::ComputePipelineId) -> String {
+        A::hub(self).compute_pipelines.label_for_resource(id)
     }
 
-    pub fn compute_pipeline_drop<B: GfxBackend>(&self, compute_pipeline_id: id::ComputePipelineId) {
+    pub fn compute_pipeline_drop<A: HalApi>(&self, compute_pipeline_id: id::ComputePipelineId) {
         profiling::scope!("drop", "ComputePipeline");
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, mut token) = hub.devices.read(&mut token);
 
         let (device_id, layout_id) = {
             let (mut pipeline_guard, _) = hub.compute_pipelines.write(&mut token);
             match pipeline_guard.get_mut(compute_pipeline_id) {
                 Ok(pipeline) => {
                     pipeline.life_guard.ref_count.take();
@@ -4442,298 +4319,302 @@ impl<G: GlobalIdentityHandlerFactory> Gl
             .compute_pipelines
             .push(id::Valid(compute_pipeline_id));
         life_lock
             .suspected_resources
             .pipeline_layouts
             .push(layout_id);
     }
 
-    pub fn device_create_swap_chain<B: GfxBackend>(
+    pub fn surface_configure<A: HalApi>(
         &self,
-        device_id: id::DeviceId,
         surface_id: id::SurfaceId,
-        desc: &wgt::SwapChainDescriptor,
-    ) -> (id::SwapChainId, Option<swap_chain::CreateSwapChainError>) {
-        profiling::scope!("create_swap_chain", "Device");
-
-        fn validate_swap_chain_descriptor(
-            config: &mut hal::window::SwapchainConfig,
-            caps: &hal::window::SurfaceCapabilities,
-        ) -> Result<(), swap_chain::CreateSwapChainError> {
+        device_id: id::DeviceId,
+        config: &wgt::SurfaceConfiguration,
+    ) -> Option<present::ConfigureSurfaceError> {
+        use hal::{Adapter as _, Surface as _};
+        use present::ConfigureSurfaceError as E;
+        profiling::scope!("surface_configure");
+
+        fn validate_surface_configuraiton(
+            config: &mut hal::SurfaceConfiguration,
+            caps: &hal::SurfaceCapabilities,
+        ) -> Result<(), E> {
             let width = config.extent.width;
             let height = config.extent.height;
             if width < caps.extents.start().width
                 || width > caps.extents.end().width
                 || height < caps.extents.start().height
                 || height > caps.extents.end().height
             {
                 log::warn!(
                     "Requested size {}x{} is outside of the supported range: {:?}",
                     width,
                     height,
                     caps.extents
                 );
             }
-            if !caps.present_modes.contains(config.present_mode) {
+            if !caps.present_modes.contains(&config.present_mode) {
                 log::warn!(
-                    "Surface does not support present mode: {:?}, falling back to {:?}",
+                    "Surface does not support present mode: {:?}, falling back to FIFO",
                     config.present_mode,
-                    hal::window::PresentMode::FIFO
                 );
-                config.present_mode = hal::window::PresentMode::FIFO;
+                config.present_mode = wgt::PresentMode::Fifo;
+            }
+            if !caps.formats.contains(&config.format) {
+                return Err(E::UnsupportedFormat {
+                    requested: config.format,
+                    available: caps.formats.clone(),
+                });
+            }
+            if !caps.usage.contains(config.usage) {
+                return Err(E::UnsupportedUsage);
             }
             if width == 0 || height == 0 {
-                return Err(swap_chain::CreateSwapChainError::ZeroArea);
+                return Err(E::ZeroArea);
             }
             Ok(())
         }
 
-        log::info!("creating swap chain {:?}", desc);
-        let sc_id = surface_id.to_swap_chain_id(B::VARIANT);
-        let hub = B::hub(self);
+        log::info!("configuring surface with {:?}", config);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let (mut surface_guard, mut token) = self.surfaces.write(&mut token);
         let (adapter_guard, mut token) = hub.adapters.read(&mut token);
-        let (device_guard, mut token) = hub.devices.read(&mut token);
-        let (mut swap_chain_guard, _) = hub.swap_chains.write(&mut token);
+        let (device_guard, _token) = hub.devices.read(&mut token);
 
         let error = loop {
             let device = match device_guard.get(device_id) {
                 Ok(device) => device,
                 Err(_) => break DeviceError::Invalid.into(),
             };
             #[cfg(feature = "trace")]
             if let Some(ref trace) = device.trace {
                 trace
                     .lock()
-                    .add(trace::Action::CreateSwapChain(sc_id, desc.clone()));
+                    .add(trace::Action::ConfigureSurface(surface_id, config.clone()));
             }
 
             let surface = match surface_guard.get_mut(surface_id) {
                 Ok(surface) => surface,
-                Err(_) => break swap_chain::CreateSwapChainError::InvalidSurface,
+                Err(_) => break E::InvalidSurface,
             };
 
-            let (caps, formats) = {
-                let surface = B::get_surface_mut(surface);
+            let caps = unsafe {
+                let suf = A::get_surface(surface);
                 let adapter = &adapter_guard[device.adapter_id.value];
-                let queue_family = &adapter.raw.queue_families[0];
-                if !surface.supports_queue_family(queue_family) {
-                    break swap_chain::CreateSwapChainError::UnsupportedQueueFamily;
+                match adapter.raw.adapter.surface_capabilities(&suf.raw) {
+                    Some(caps) => caps,
+                    None => break E::UnsupportedQueueFamily,
                 }
-                let formats = surface.supported_formats(&adapter.raw.physical_device);
-                let caps = surface.capabilities(&adapter.raw.physical_device);
-                (caps, formats)
             };
 
-            let num_frames = swap_chain::DESIRED_NUM_FRAMES
-                .max(*caps.image_count.start())
-                .min(*caps.image_count.end());
-            let mut config = swap_chain::swap_chain_descriptor_to_hal(
-                &desc,
-                num_frames,
-                device.private_features,
-            );
-            if let Some(formats) = formats {
-                if !formats.contains(&config.format) {
-                    break swap_chain::CreateSwapChainError::UnsupportedFormat {
-                        requested: config.format,
-                        available: formats,
-                    };
+            let num_frames = present::DESIRED_NUM_FRAMES
+                .max(*caps.swap_chain_sizes.start())
+                .min(*caps.swap_chain_sizes.end());
+            let mut hal_config = hal::SurfaceConfiguration {
+                swap_chain_size: num_frames,
+                present_mode: config.present_mode,
+                composite_alpha_mode: hal::CompositeAlphaMode::Opaque,
+                format: config.format,
+                extent: wgt::Extent3d {
+                    width: config.width,
+                    height: config.height,
+                    depth_or_array_layers: 1,
+                },
+                usage: conv::map_texture_usage(config.usage, hal::FormatAspects::COLOR),
+            };
+
+            if let Err(error) = validate_surface_configuraiton(&mut hal_config, &caps) {
+                break error;
+            }
+
+            match unsafe {
+                A::get_surface_mut(surface)
+                    .raw
+                    .configure(&device.raw, &hal_config)
+            } {
+                Ok(()) => (),
+                Err(error) => {
+                    break match error {
+                        hal::SurfaceError::Outdated | hal::SurfaceError::Lost => E::InvalidSurface,
+                        hal::SurfaceError::Device(error) => E::Device(error.into()),
+                        hal::SurfaceError::Other(message) => {
+                            log::error!("surface configuration failed: {}", message);
+                            E::InvalidSurface
+                        }
+                    }
                 }
             }
-            if let Err(error) = validate_swap_chain_descriptor(&mut config, &caps) {
-                break error;
-            }
-            let framebuffer_attachment = config.framebuffer_attachment();
-
-            match unsafe { B::get_surface_mut(surface).configure_swapchain(&device.raw, config) } {
-                Ok(()) => (),
-                Err(hal::window::SwapchainError::OutOfMemory(_)) => {
-                    break DeviceError::OutOfMemory.into()
-                }
-                Err(hal::window::SwapchainError::DeviceLost(_)) => break DeviceError::Lost.into(),
-                Err(err) => panic!("failed to configure swap chain on creation: {}", err),
-            }
-
-            if let Some(sc) = swap_chain_guard.try_remove(sc_id) {
-                if sc.acquired_view_id.is_some() {
-                    break swap_chain::CreateSwapChainError::SwapChainOutputExists;
-                }
-                unsafe {
-                    device.raw.destroy_semaphore(sc.semaphore);
+
+            if let Some(present) = surface.presentation.take() {
+                if present.acquired_texture.is_some() {
+                    break E::PreviousOutputExists;
                 }
             }
 
-            let swap_chain = swap_chain::SwapChain {
-                life_guard: LifeGuard::new("<SwapChain>"),
+            surface.presentation = Some(present::Presentation {
                 device_id: Stored {
                     value: id::Valid(device_id),
                     ref_count: device.life_guard.add_ref(),
                 },
-                desc: desc.clone(),
+                config: config.clone(),
                 num_frames,
-                semaphore: match device.raw.create_semaphore() {
-                    Ok(sem) => sem,
-                    Err(_) => break DeviceError::OutOfMemory.into(),
-                },
-                acquired_view_id: None,
-                active_submission_index: 0,
-                framebuffer_attachment,
-            };
-            swap_chain_guard.insert(sc_id, swap_chain);
-
-            return (sc_id, None);
+                acquired_texture: None,
+            });
+
+            return None;
         };
 
-        swap_chain_guard.insert_error(sc_id, "");
-        (sc_id, Some(error))
+        Some(error)
     }
 
     #[cfg(feature = "replay")]
     /// Only triange suspected resource IDs. This helps us to avoid ID collisions
     /// upon creating new resources when re-playing a trace.
-    pub fn device_maintain_ids<B: GfxBackend>(
+    pub fn device_maintain_ids<A: HalApi>(
         &self,
         device_id: id::DeviceId,
     ) -> Result<(), InvalidDevice> {
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let device = device_guard.get(device_id).map_err(|_| InvalidDevice)?;
         device.lock_life(&mut token).triage_suspected(
-            &hub,
+            hub,
             &device.trackers,
             #[cfg(feature = "trace")]
             None,
             &mut token,
         );
         Ok(())
     }
 
-    pub fn device_poll<B: GfxBackend>(
+    pub fn device_poll<A: HalApi>(
         &self,
         device_id: id::DeviceId,
         force_wait: bool,
     ) -> Result<(), WaitIdleError> {
-        let hub = B::hub(self);
-        let mut token = Token::root();
-        let callbacks = {
+        let closures = {
+            let hub = A::hub(self);
+            let mut token = Token::root();
             let (device_guard, mut token) = hub.devices.read(&mut token);
             device_guard
                 .get(device_id)
                 .map_err(|_| DeviceError::Invalid)?
-                .maintain(&hub, force_wait, &mut token)?
+                .maintain(hub, force_wait, &mut token)?
         };
-        fire_map_callbacks(callbacks);
+        unsafe {
+            closures.fire();
+        }
         Ok(())
     }
 
-    fn poll_devices<B: GfxBackend>(
+    fn poll_devices<A: HalApi>(
         &self,
         force_wait: bool,
-        callbacks: &mut Vec<BufferMapPendingCallback>,
+        closures: &mut UserClosures,
     ) -> Result<(), WaitIdleError> {
         profiling::scope!("poll_devices");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, mut token) = hub.devices.read(&mut token);
-        for (_, device) in device_guard.iter(B::VARIANT) {
-            let cbs = device.maintain(&hub, force_wait, &mut token)?;
-            callbacks.extend(cbs);
+        for (_, device) in device_guard.iter(A::VARIANT) {
+            let cbs = device.maintain(hub, force_wait, &mut token)?;
+            closures.extend(cbs);
         }
         Ok(())
     }
 
     pub fn poll_all_devices(&self, force_wait: bool) -> Result<(), WaitIdleError> {
-        use crate::backend;
-        let mut callbacks = Vec::new();
+        let mut closures = UserClosures::default();
 
         #[cfg(vulkan)]
         {
-            self.poll_devices::<backend::Vulkan>(force_wait, &mut callbacks)?;
+            self.poll_devices::<hal::api::Vulkan>(force_wait, &mut closures)?;
         }
         #[cfg(metal)]
         {
-            self.poll_devices::<backend::Metal>(force_wait, &mut callbacks)?;
+            self.poll_devices::<hal::api::Metal>(force_wait, &mut closures)?;
         }
         #[cfg(dx12)]
         {
-            self.poll_devices::<backend::Dx12>(force_wait, &mut callbacks)?;
+            self.poll_devices::<hal::api::Dx12>(force_wait, &mut closures)?;
         }
         #[cfg(dx11)]
         {
-            self.poll_devices::<backend::Dx11>(force_wait, &mut callbacks)?;
+            self.poll_devices::<hal::api::Dx11>(force_wait, &mut closures)?;
         }
 
-        fire_map_callbacks(callbacks);
+        unsafe {
+            closures.fire();
+        }
 
         Ok(())
     }
 
-    pub fn device_label<B: GfxBackend>(&self, id: id::DeviceId) -> String {
-        B::hub(self).devices.label_for_resource(id)
+    pub fn device_label<A: HalApi>(&self, id: id::DeviceId) -> String {
+        A::hub(self).devices.label_for_resource(id)
     }
 
-    pub fn device_start_capture<B: GfxBackend>(&self, id: id::DeviceId) {
-        let hub = B::hub(self);
+    pub fn device_start_capture<A: HalApi>(&self, id: id::DeviceId) {
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, _) = hub.devices.read(&mut token);
         if let Ok(device) = device_guard.get(id) {
-            device.raw.start_capture();
+            unsafe { device.raw.start_capture() };
         }
     }
 
-    pub fn device_stop_capture<B: GfxBackend>(&self, id: id::DeviceId) {
-        let hub = B::hub(self);
+    pub fn device_stop_capture<A: HalApi>(&self, id: id::DeviceId) {
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, _) = hub.devices.read(&mut token);
         if let Ok(device) = device_guard.get(id) {
-            device.raw.stop_capture();
+            unsafe { device.raw.stop_capture() };
         }
     }
 
-    pub fn device_drop<B: GfxBackend>(&self, device_id: id::DeviceId) {
+    pub fn device_drop<A: HalApi>(&self, device_id: id::DeviceId) {
         profiling::scope!("drop", "Device");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device, _) = hub.devices.unregister(device_id, &mut token);
         if let Some(mut device) = device {
             device.prepare_to_die();
 
             // Adapter is only referenced by the device and itself.
             // This isn't a robust way to destroy them, we should find a better one.
             if device.adapter_id.ref_count.load() == 1 {
-                let (_adapter, _) = hub
+                let _ = hub
                     .adapters
                     .unregister(device.adapter_id.value.0, &mut token);
             }
 
             device.dispose();
         }
     }
 
-    pub fn buffer_map_async<B: GfxBackend>(
+    pub fn buffer_map_async<A: HalApi>(
         &self,
         buffer_id: id::BufferId,
         range: Range<BufferAddress>,
         op: resource::BufferMapOperation,
     ) -> Result<(), resource::BufferAccessError> {
         profiling::scope!("map_async", "Buffer");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (device_guard, mut token) = hub.devices.read(&mut token);
         let (pub_usage, internal_use) = match op.host {
-            HostMap::Read => (wgt::BufferUsage::MAP_READ, resource::BufferUse::MAP_READ),
-            HostMap::Write => (wgt::BufferUsage::MAP_WRITE, resource::BufferUse::MAP_WRITE),
+            HostMap::Read => (wgt::BufferUsages::MAP_READ, hal::BufferUses::MAP_READ),
+            HostMap::Write => (wgt::BufferUsages::MAP_WRITE, hal::BufferUses::MAP_WRITE),
         };
 
         if range.start % wgt::MAP_ALIGNMENT != 0 || range.end % wgt::COPY_BUFFER_ALIGNMENT != 0 {
             return Err(resource::BufferAccessError::UnalignedRange);
         }
 
         let (device_id, ref_count) = {
             let (mut buffer_guard, _) = hub.buffers.write(&mut token);
@@ -4773,25 +4654,25 @@ impl<G: GlobalIdentityHandlerFactory> Gl
 
         device
             .lock_life(&mut token)
             .map(id::Valid(buffer_id), ref_count);
 
         Ok(())
     }
 
-    pub fn buffer_get_mapped_range<B: GfxBackend>(
+    pub fn buffer_get_mapped_range<A: HalApi>(
         &self,
         buffer_id: id::BufferId,
         offset: BufferAddress,
         size: Option<BufferAddress>,
     ) -> Result<(*mut u8, u64), resource::BufferAccessError> {
         profiling::scope!("get_mapped_range", "Buffer");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
         let (buffer_guard, _) = hub.buffers.read(&mut token);
         let buffer = buffer_guard
             .get(buffer_id)
             .map_err(|_| resource::BufferAccessError::Invalid)?;
 
         let range_size = if let Some(size) = size {
             size
@@ -4814,160 +4695,156 @@ impl<G: GlobalIdentityHandlerFactory> Gl
                 if offset + range_size > buffer.size {
                     return Err(resource::BufferAccessError::OutOfBoundsOverrun {
                         index: offset + range_size - 1,
                         max: buffer.size,
                     });
                 }
                 unsafe { Ok((ptr.as_ptr().offset(offset as isize), range_size)) }
             }
-            resource::BufferMapState::Active {
-                ptr, ref sub_range, ..
-            } => {
-                if offset < sub_range.offset {
+            resource::BufferMapState::Active { ptr, ref range, .. } => {
+                if offset < range.start {
                     return Err(resource::BufferAccessError::OutOfBoundsUnderrun {
                         index: offset,
-                        min: sub_range.offset,
+                        min: range.start,
                     });
                 }
-                let range_end_offset = sub_range
-                    .size
-                    .map(|size| size + sub_range.offset)
-                    .unwrap_or(buffer.size);
-                if offset + range_size > range_end_offset {
+                if offset + range_size > range.end {
                     return Err(resource::BufferAccessError::OutOfBoundsOverrun {
                         index: offset + range_size - 1,
-                        max: range_end_offset,
+                        max: range.end,
                     });
                 }
                 unsafe { Ok((ptr.as_ptr().offset(offset as isize), range_size)) }
             }
             resource::BufferMapState::Idle | resource::BufferMapState::Waiting(_) => {
                 Err(resource::BufferAccessError::NotMapped)
             }
         }
     }
 
-    fn buffer_unmap_inner<B: GfxBackend>(
+    fn buffer_unmap_inner<A: HalApi>(
         &self,
         buffer_id: id::BufferId,
-    ) -> Result<Option<BufferMapPendingCallback>, resource::BufferAccessError> {
+    ) -> Result<Option<BufferMapPendingClosure>, resource::BufferAccessError> {
         profiling::scope!("unmap", "Buffer");
 
-        let hub = B::hub(self);
+        let hub = A::hub(self);
         let mut token = Token::root();
 
         let (mut device_guard, mut token) = hub.devices.write(&mut token);
         let (mut buffer_guard, _) = hub.buffers.write(&mut token);
         let buffer = buffer_guard
             .get_mut(buffer_id)
             .map_err(|_| resource::BufferAccessError::Invalid)?;
         let device = &mut device_guard[buffer.device_id.value];
 
         log::debug!("Buffer {:?} map state -> Idle", buffer_id);
         match mem::replace(&mut buffer.map_state, resource::BufferMapState::Idle) {
             resource::BufferMapState::Init {
                 ptr,
                 stage_buffer,
-                stage_memory,
                 needs_flush,
             } => {
                 #[cfg(feature = "trace")]
                 if let Some(ref trace) = device.trace {
                     let mut trace = trace.lock();
                     let data = trace.make_binary("bin", unsafe {
                         std::slice::from_raw_parts(ptr.as_ptr(), buffer.size as usize)
                     });
                     trace.add(trace::Action::WriteBuffer {
                         id: buffer_id,
                         data,
                         range: 0..buffer.size,
                         queued: true,
                     });
                 }
                 let _ = ptr;
-
                 if needs_flush {
-                    stage_memory.flush_range(&device.raw, 0, None)?;
+                    unsafe {
+                        device
+                            .raw
+                            .flush_mapped_ranges(&stage_buffer, iter::once(0..buffer.size));
+                    }
                 }
 
-                let &(ref buf_raw, _) = buffer
+                let raw_buf = buffer
                     .raw
                     .as_ref()
                     .ok_or(resource::BufferAccessError::Destroyed)?;
 
                 buffer.life_guard.use_at(device.active_submission_index + 1);
-                let region = hal::command::BufferCopy {
-                    src: 0,
-                    dst: 0,
-                    size: buffer.size,
-                };
-                let transition_src = hal::memory::Barrier::Buffer {
-                    states: hal::buffer::Access::HOST_WRITE..hal::buffer::Access::TRANSFER_READ,
-                    target: &stage_buffer,
-                    range: hal::buffer::SubRange::WHOLE,
-                    families: None,
+                let region = wgt::BufferSize::new(buffer.size).map(|size| hal::BufferCopy {
+                    src_offset: 0,
+                    dst_offset: 0,
+                    size,
+                });
+                let transition_src = hal::BufferBarrier {
+                    buffer: &stage_buffer,
+                    usage: hal::BufferUses::MAP_WRITE..hal::BufferUses::COPY_SRC,
                 };
-                let transition_dst = hal::memory::Barrier::Buffer {
-                    states: hal::buffer::Access::empty()..hal::buffer::Access::TRANSFER_WRITE,
-                    target: buf_raw,
-                    range: hal::buffer::SubRange::WHOLE,
-                    families: None,
+                let transition_dst = hal::BufferBarrier {
+                    buffer: raw_buf,
+                    usage: hal::BufferUses::empty()..hal::BufferUses::COPY_DST,
                 };
+                let encoder = device.pending_writes.activate();
                 unsafe {
-                    let cmdbuf = device.borrow_pending_writes();
-                    cmdbuf.pipeline_barrier(
-                        hal::pso::PipelineStage::HOST..hal::pso::PipelineStage::TRANSFER,
-                        hal::memory::Dependencies::empty(),
+                    encoder.transition_buffers(
                         iter::once(transition_src).chain(iter::once(transition_dst)),
                     );
                     if buffer.size > 0 {
-                        cmdbuf.copy_buffer(&stage_buffer, buf_raw, iter::once(region));
+                        encoder.copy_buffer_to_buffer(&stage_buffer, raw_buf, region.into_iter());
                     }
                 }
                 device
                     .pending_writes
-                    .consume_temp(queue::TempResource::Buffer(stage_buffer), stage_memory);
+                    .consume_temp(queue::TempResource::Buffer(stage_buffer));
                 device.pending_writes.dst_buffers.insert(buffer_id);
             }
             resource::BufferMapState::Idle => {
                 return Err(resource::BufferAccessError::NotMapped);
             }
             resource::BufferMapState::Waiting(pending) => {
                 return Ok(Some((pending.op, resource::BufferMapAsyncStatus::Aborted)));
             }
-            resource::BufferMapState::Active {
-                ptr,
-                sub_range,
-                host,
-            } => {
+            resource::BufferMapState::Active { ptr, range, host } => {
                 if host == HostMap::Write {
                     #[cfg(feature = "trace")]
                     if let Some(ref trace) = device.trace {
                         let mut trace = trace.lock();
-                        let size = sub_range.size_to(buffer.size);
+                        let size = range.end - range.start;
                         let data = trace.make_binary("bin", unsafe {
                             std::slice::from_raw_parts(ptr.as_ptr(), size as usize)
                         });
                         trace.add(trace::Action::WriteBuffer {
                             id: buffer_id,
                             data,
-                            range: sub_range.offset..sub_range.offset + size,
+                            range: range.clone(),
                             queued: false,
                         });
                     }
-                    let _ = (ptr, sub_range);
+                    let _ = (ptr, range);
                 }
-                unmap_buffer(&device.raw, buffer)?;
+                unsafe {
+                    device
+                        .raw
+                        .unmap_buffer(buffer.raw.as_ref().unwrap())
+                        .map_err(DeviceError::from)?
+                };
             }
         }
         Ok(None)
     }
 
-    pub fn buffer_unmap<B: GfxBackend>(
+    pub fn buffer_unmap<A: HalApi>(
         &self,
         buffer_id: id::BufferId,
     ) -> Result<(), resource::BufferAccessError> {
-        self.buffer_unmap_inner::<B>(buffer_id)
-            //Note: outside inner function so no locks are held when calling the callback
-            .map(|pending_callback| fire_map_callbacks(pending_callback.into_iter()))
+        //Note: outside inner function so no locks are held when calling the callback
+        let closure = self.buffer_unmap_inner::<A>(buffer_id)?;
+        if let Some((operation, status)) = closure {
+            unsafe {
+                (operation.callback)(status, operation.user_data);
+            }
+        }
+        Ok(())
     }
 }