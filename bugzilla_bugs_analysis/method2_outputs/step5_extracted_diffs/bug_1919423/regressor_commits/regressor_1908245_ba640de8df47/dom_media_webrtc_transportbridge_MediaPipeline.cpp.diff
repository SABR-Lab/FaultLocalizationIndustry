# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: dom/media/webrtc/transportbridge/MediaPipeline.cpp
# Commit: ba640de8df47
# Full Hash: ba640de8df47d54906c708eb78f2d000d6f6d7f6
# Author: Andrew Osmond <aosmond@mozilla.com>
# Date: 2024-09-13 21:45:07
# Regressor Bug: 1908245
# File Overlap Count: 1
# Description:
#   Bug 1908245 - Add support for missing requestVideoFrameCallback parameters. r=media-playback-reviewers,webidl,padenot,saschanaz
#   
#   This patch adds support for exposing the capture time, receive time,
#   processing duration and RTP timestamp parameters on the callback for
#   rVFC for WebRTC related elements.
# ==============================================================================

diff -r 2f4f9cce6f7d -r ba640de8df47 dom/media/webrtc/transportbridge/MediaPipeline.cpp
--- a/dom/media/webrtc/transportbridge/MediaPipeline.cpp	Fri Sep 13 18:15:18 2024 +0000
+++ b/dom/media/webrtc/transportbridge/MediaPipeline.cpp	Fri Sep 13 18:19:57 2024 +0000
@@ -1421,8 +1421,7 @@
     mForceDropFrames = false;
   }
 
-  void RenderVideoFrame(const webrtc::VideoFrameBuffer& aBuffer,
-                        uint32_t aTimeStamp, int64_t aRenderTime) {
+  void RenderVideoFrame(const webrtc::VideoFrame& aVideoFrame) {
     PrincipalHandle principal;
     {
       MutexAutoLock lock(mMutex);
@@ -1432,16 +1431,16 @@
       principal = mPrincipalHandle;
     }
     RefPtr<Image> image;
-    if (aBuffer.type() == webrtc::VideoFrameBuffer::Type::kNative) {
+    const webrtc::VideoFrameBuffer& buffer = *aVideoFrame.video_frame_buffer();
+    if (buffer.type() == webrtc::VideoFrameBuffer::Type::kNative) {
       // We assume that only native handles are used with the
       // WebrtcMediaDataCodec decoder.
-      const ImageBuffer* imageBuffer =
-          static_cast<const ImageBuffer*>(&aBuffer);
+      const ImageBuffer* imageBuffer = static_cast<const ImageBuffer*>(&buffer);
       image = imageBuffer->GetNativeImage();
     } else {
-      MOZ_ASSERT(aBuffer.type() == webrtc::VideoFrameBuffer::Type::kI420);
+      MOZ_ASSERT(buffer.type() == webrtc::VideoFrameBuffer::Type::kI420);
       rtc::scoped_refptr<const webrtc::I420BufferInterface> i420(
-          aBuffer.GetI420());
+          buffer.GetI420());
 
       MOZ_ASSERT(i420->DataY());
       // Create a video frame using |buffer|.
@@ -1475,9 +1474,25 @@
       image = std::move(yuvImage);
     }
 
+    Maybe<webrtc::Timestamp> receiveTime;
+    for (const auto& packet : aVideoFrame.packet_infos()) {
+      if (!receiveTime || *receiveTime < packet.receive_time()) {
+        receiveTime = Some(packet.receive_time());
+      }
+    }
+
     VideoSegment segment;
     auto size = image->GetSize();
-    segment.AppendFrame(image.forget(), size, principal);
+    auto processingDuration =
+        aVideoFrame.processing_time()
+            ? media::TimeUnit::FromMicroseconds(
+                  aVideoFrame.processing_time()->Elapsed().us())
+            : media::TimeUnit::Invalid();
+    segment.AppendWebrtcRemoteFrame(
+        image.forget(), size, principal,
+        /* aForceBlack */ false, TimeStamp::Now(), processingDuration,
+        aVideoFrame.rtp_timestamp(), aVideoFrame.ntp_time_ms(),
+        receiveTime ? receiveTime->us() : 0);
     mSource->AppendData(&segment);
   }
 
@@ -1501,9 +1516,8 @@
 
   // Implement VideoRenderer
   void FrameSizeChange(unsigned int aWidth, unsigned int aHeight) override {}
-  void RenderVideoFrame(const webrtc::VideoFrameBuffer& aBuffer,
-                        uint32_t aTimeStamp, int64_t aRenderTime) override {
-    mPipeline->mListener->RenderVideoFrame(aBuffer, aTimeStamp, aRenderTime);
+  void RenderVideoFrame(const webrtc::VideoFrame& aVideoFrame) override {
+    mPipeline->mListener->RenderVideoFrame(aVideoFrame);
   }
 
  private: