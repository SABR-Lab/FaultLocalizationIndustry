# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: js/src/wasm/WasmCode.cpp
# Commit: 4c22ce111c01
# Full Hash: 4c22ce111c012805515250b63074235b5578caa6
# Author: Ryan Hunt <rhunt@eqrion.net>
# Date: 2024-10-31 04:51:12
# Regressor Bug: 1913119
# File Overlap Count: 1
# Description:
#   Bug 1913119 - wasm: Re-use pool mechanism for lazy stubs and simplify logic. r=jseward
#   
#   Lazy func export stubs used some of the mechanism for allocating from a pool
#   of code segments, but not all. This caused some code duplication and layering
#   issues.
# ==============================================================================

diff -r 7593ec9fa478 -r 4c22ce111c01 js/src/wasm/WasmCode.cpp
--- a/js/src/wasm/WasmCode.cpp	Wed Oct 30 21:54:24 2024 +0000
+++ b/js/src/wasm/WasmCode.cpp	Wed Oct 30 21:54:24 2024 +0000
@@ -315,15 +315,14 @@
 
 bool CodeSegment::linkAndMakeExecutableSubRange(
     jit::AutoMarkJitCodeWritableForThread& writable, const LinkData& linkData,
-    const Code* maybeCode, uint8_t* pageStart, uint8_t* codeStart,
-    uint32_t codeLength) {
-  // See ASCII art at CodeSegment::createFromMasmWithBumpAlloc (implementation)
-  // for the meaning of pageStart/codeStart/fuzz/codeLength.
-  MOZ_ASSERT(CodeSegment::IsAligned(uintptr_t(pageStart)));
-  MOZ_ASSERT(codeStart >= pageStart);
-  MOZ_ASSERT(codeStart - pageStart <
-             ptrdiff_t(CodeSegment::AllocationAlignment()));
-  uint32_t fuzz = codeStart - pageStart;
+    const Code* maybeCode, uint8_t* allocationStart, uint8_t* codeStart,
+    uint32_t allocationLength) {
+  MOZ_ASSERT(CodeSegment::IsAligned(uintptr_t(allocationStart)));
+  MOZ_ASSERT(codeStart >= allocationStart);
+  MOZ_ASSERT_IF(JitOptions.writeProtectCode,
+                uintptr_t(allocationStart) % gc::SystemPageSize() == 0 &&
+                    allocationLength % gc::SystemPageSize() == 0);
+
   if (!StaticallyLink(writable, codeStart, linkData, maybeCode)) {
     return false;
   }
@@ -331,8 +330,29 @@
   // Optimized compilation finishes on a background thread, so we must make sure
   // to flush the icaches of all the executing threads.
   // Reprotect the whole region to avoid having separate RW and RX mappings.
-  return ExecutableAllocator::makeExecutableAndFlushICache(pageStart,
-                                                           fuzz + codeLength);
+  return ExecutableAllocator::makeExecutableAndFlushICache(allocationStart,
+                                                           allocationLength);
+}
+
+bool CodeSegment::linkAndMakeExecutableSubRange(
+    jit::AutoMarkJitCodeWritableForThread& writable, jit::MacroAssembler& masm,
+    uint8_t* allocationStart, uint8_t* codeStart, uint32_t allocationLength) {
+  MOZ_ASSERT(CodeSegment::IsAligned(uintptr_t(allocationStart)));
+  MOZ_ASSERT(codeStart >= allocationStart);
+  MOZ_ASSERT_IF(JitOptions.writeProtectCode,
+                uintptr_t(allocationStart) % gc::SystemPageSize() == 0 &&
+                    allocationLength % gc::SystemPageSize() == 0);
+
+  PatchDebugSymbolicAccesses(codeStart, masm);
+  for (const CodeLabel& label : masm.codeLabels()) {
+    Assembler::Bind(codeStart, label);
+  }
+
+  // Optimized compilation finishes on a background thread, so we must make sure
+  // to flush the icaches of all the executing threads.
+  // Reprotect the whole region to avoid having separate RW and RX mappings.
+  return ExecutableAllocator::makeExecutableAndFlushICache(allocationStart,
+                                                           allocationLength);
 }
 
 bool CodeSegment::linkAndMakeExecutable(
@@ -341,8 +361,8 @@
   MOZ_ASSERT(base() == bytes_.get());
   return linkAndMakeExecutableSubRange(
       writable, linkData, maybeCode,
-      /*pageStart=*/base(), /*codeStart=*/base(),
-      /*codeLength=*/RoundupExecutableCodePageSize(lengthBytes()));
+      /*allocationStart=*/base(), /*codeStart=*/base(),
+      /*allocationLength=*/RoundupExecutableCodePageSize(lengthBytes()));
 }
 
 /* static */
@@ -419,154 +439,80 @@
   return segment;
 }
 
-// Helper for Code::createManyLazyEntryStubs
-// and CodeSegment::createFromMasmWithBumpAlloc
-SharedCodeSegment js::wasm::AllocateCodePagesFrom(
-    SharedCodeSegmentVector& lazySegments, uint32_t bytesNeeded,
-    bool allowLastDitchGC, size_t* offsetInSegment,
-    size_t* roundedUpAllocationSize) {
-  size_t codeLength = CodeSegment::AlignAllocationBytes(bytesNeeded);
+// When allocating a single stub to a page, we should not always place the stub
+// at the beginning of the page as the stubs will tend to thrash the icache by
+// creating conflicts (everything ends up in the same cache set).  Instead,
+// locate stubs at different line offsets up to 3/4 the system page size (the
+// code allocation quantum).
+//
+// This may be called on background threads, hence the atomic.
+static uint32_t RandomPaddingForCodeLength(uint32_t codeLength) {
+  // The counter serves only to spread the code out, it has no other meaning and
+  // can wrap around.
+  static mozilla::Atomic<uint32_t, mozilla::MemoryOrdering::ReleaseAcquire>
+      counter(0);
+  // We assume that the icache line size is 64 bytes, which is close to
+  // universally true.
+  const size_t cacheLineSize = 64;
+  const size_t systemPageSize = gc::SystemPageSize();
+
+  // Don't add more than 3/4 of a page of padding
+  size_t maxPadBytes = ((systemPageSize * 3) / 4);
+  size_t maxPadLines = maxPadBytes / cacheLineSize;
+
+  // If code length is close to a page boundary, avoid pushing it to a new page
+  size_t remainingBytesInPage =
+      AlignBytes(codeLength, systemPageSize) - codeLength;
+  size_t remainingLinesInPage = remainingBytesInPage / cacheLineSize;
 
-  if (lazySegments.length() == 0 ||
-      !lazySegments[lazySegments.length() - 1]->hasSpace(codeLength)) {
+  // Limit padding to the smallest of the above
+  size_t padLinesAvailable = std::min(maxPadLines, remainingLinesInPage);
+
+  uint32_t random = counter++;
+  uint32_t padding = (random % padLinesAvailable) * cacheLineSize;
+  // "adding on the padding area doesn't change the total number of pages
+  //  required"
+  MOZ_ASSERT(AlignBytes(codeLength + padding, systemPageSize) ==
+             AlignBytes(codeLength, systemPageSize));
+  return padding;
+}
+
+/* static */
+SharedCodeSegment CodeSegment::claimSpaceFromPool(
+    uint32_t codeLength, SharedCodeSegmentVector* segmentPool,
+    bool allowLastDitchGC, uint8_t** allocationStartOut, uint8_t** codeStartOut,
+    uint32_t* allocationLengthOut) {
+  uint32_t paddingLength = RandomPaddingForCodeLength(codeLength);
+  uint32_t allocationLength =
+      CodeSegment::AlignAllocationBytes(paddingLength + codeLength);
+
+  // Find a CodeSegment that has enough space. We just check the last code
+  // segment in the pool for simplicity.
+  if (segmentPool->length() == 0 ||
+      !(*segmentPool)[segmentPool->length() - 1]->hasSpace(allocationLength)) {
     SharedCodeSegment newSegment =
-        CodeSegment::createEmpty(codeLength, allowLastDitchGC);
+        CodeSegment::createEmpty(allocationLength, allowLastDitchGC);
     if (!newSegment) {
       return nullptr;
     }
-    if (!lazySegments.emplaceBack(std::move(newSegment))) {
+    if (!segmentPool->emplaceBack(std::move(newSegment))) {
       return nullptr;
     }
   }
 
-  MOZ_ASSERT(lazySegments.length() > 0);
-  CodeSegment* segment = lazySegments[lazySegments.length() - 1].get();
-
-  uint8_t* codePtr = nullptr;
-  segment->claimSpace(codeLength, &codePtr);
-  *offsetInSegment = codePtr - segment->base();
-  if (roundedUpAllocationSize) {
-    *roundedUpAllocationSize = codeLength;
-  }
-  return segment;
-}
+  MOZ_ASSERT(segmentPool->length() > 0);
+  SharedCodeSegment segment = (*segmentPool)[segmentPool->length() - 1].get();
 
-// Note, this allocates from `code->lazyFuncSegments` only, not from
-// `code->lazyStubSegments`.
-/* static */
-SharedCodeSegment CodeSegment::createFromMasmWithBumpAlloc(
-    jit::MacroAssembler& masm, const LinkData& linkData, const Code* code,
-    bool allowLastDitchGC, uint8_t** codeStartOut, uint32_t* codeLengthOut,
-    uint32_t* metadataBiasOut) {
-  // Here's a picture that illustrates the relationship of the various
-  // variables.  This is an example for a machine with a 4KB page size, for an
-  // allocation ("CODE") which requires more than one page but less than two,
-  // in a Segment where the first page is already allocated.
-  //
-  // segment->base() (aligned at 4K = hardware page size)
-  // :
-  // :                      +4k                     +8k                    +12k
-  // :                       :                       :                       :
-  // +-----------------------+         +---------------------------------+   :
-  // |        IN USE         |         |   CODE              CODE        |   :
-  // +-----------------------+---------+---------------------------------+---+
-  // .                       .         .                                 .
-  // :    offsetInSegment    :  fuzz   :           codeLength            :
-  // :<--------------------->:<------->:<------------------------------->:
-  // :                       :         :                                 :
-  // :                       :         :     requestLength               :
-  // :                       :<----------------------------------------->:
-  // :                       :         :
-  // :          metadataBias           :
-  // :<------------------------------->:
-  //                         :         :
-  //                         :         codeStart
-  //                         :
-  //                         pageStart
-
-  // Values to be computed
-  SharedCodeSegment segment;
-  uint32_t requestLength;
-  uint8_t* pageStart;
-  uint8_t* codeStart;
-
-  // We have to allocate an integral number of hardware pages.  Hence it's very
-  // likely there will be space left over at the end of the last page, in which
-  // case we can move the real start point of the code forward a bit, so as to
-  // spread it out over more icache sets.  We'll compute the required movement
-  // into `fuzz`.
-  uint32_t fuzz = 0;
-
-  // The number of bytes that we need, really.
-  uint32_t codeLength = masm.bytesNeeded();
-
-  // The rounded-up allocation size -- only needed for stats.
-  size_t roundedUpAllocationSize = 0;
+  uint8_t* allocationStart = nullptr;
+  segment->claimSpace(allocationLength, &allocationStart);
+  uint8_t* codeStart = allocationStart + paddingLength;
 
-  {
-    auto guard = code->data().writeLock();
-
-    // Figure out the maximum number of instruction cache lines the allocation
-    // can be moved forwards from the start of a page, whilst not pushing the
-    // end of it into a new page.  Then choose `fuzz` pseudo-randomly on that
-    // basis.  We assume that the icache line size is 64 bytes, which is close
-    // to universally true.
-    const uint32_t cacheLineSize = 64;
-    int32_t bytesUnusedAtEndOfPage =
-        int32_t(CodeSegment::AlignAllocationBytes(codeLength) - codeLength);
-    MOZ_RELEASE_ASSERT(bytesUnusedAtEndOfPage >= 0 &&
-                       bytesUnusedAtEndOfPage <
-                           int32_t(CodeSegment::AllocationAlignment()));
-    uint32_t fuzzLinesAvailable =
-        uint32_t(bytesUnusedAtEndOfPage) / cacheLineSize;
-    // But don't overdo it (important if hardware page size is > 4k)
-    if (fuzzLinesAvailable > 63) {
-      fuzzLinesAvailable = 63;
-    }
-    // And so choose `fuzz` accordingly.
-    fuzz = guard->simplePRNG.get11RandomBits() % (fuzzLinesAvailable + 1);
-    fuzz *= cacheLineSize;
-
-    requestLength = fuzz + codeLength;
-    // "adding on the fuzz area doesn't change the total number of pages
-    //  required"
-    MOZ_RELEASE_ASSERT(CodeSegment::AlignAllocationBytes(requestLength) ==
-                       CodeSegment::AlignAllocationBytes(codeLength));
+  MOZ_ASSERT(CodeSegment::IsAligned(uintptr_t(segment->base())));
+  MOZ_ASSERT(CodeSegment::IsAligned(allocationStart - segment->base()));
 
-    // Find a CodeSegment that has enough space
-    size_t offsetInSegment = 0;
-    segment = AllocateCodePagesFrom(guard->lazyFuncSegments, requestLength,
-                                    allowLastDitchGC, &offsetInSegment,
-                                    &roundedUpAllocationSize);
-    if (!segment) {
-      return nullptr;
-    }
-    MOZ_ASSERT(CodeSegment::IsAligned(uintptr_t(segment->base())));
-    MOZ_ASSERT(CodeSegment::IsAligned(offsetInSegment));
-
-    pageStart = segment->base() + offsetInSegment;
-    codeStart = pageStart + fuzz;
-  }
-
-  // Update allocation statistics.
-  {
-    auto guard = code->codeMeta().stats.writeLock();
-    guard->partialCodeBytesMapped += roundedUpAllocationSize;
-    guard->partialCodeBytesUsed += codeLength;
-  }
-
-  Maybe<AutoMarkJitCodeWritableForThread> writable;
-  writable.emplace();
-
-  masm.executableCopy(codeStart);
-  if (!segment->linkAndMakeExecutableSubRange(
-          *writable, linkData, code, pageStart, codeStart, codeLength)) {
-    return nullptr;
-  }
-
+  *allocationStartOut = allocationStart;
   *codeStartOut = codeStart;
-  *codeLengthOut = codeLength;
-  *metadataBiasOut = codeStart - segment->base();
+  *allocationLengthOut = allocationLength;
   return segment;
 }
 
@@ -580,30 +526,6 @@
   return mallocSizeOf(get());
 }
 
-// When allocating a single stub to a page, we should not always place the stub
-// at the beginning of the page as the stubs will tend to thrash the icache by
-// creating conflicts (everything ends up in the same cache set).  Instead,
-// locate stubs at different line offsets up to 3/4 the system page size (the
-// code allocation quantum).
-//
-// This may be called on background threads, hence the atomic.
-
-static void PadCodeForSingleStub(MacroAssembler& masm) {
-  // Assume 64B icache line size
-  static uint8_t zeroes[64];
-
-  // The counter serves only to spread the code out, it has no other meaning and
-  // can wrap around.
-  static mozilla::Atomic<uint32_t, mozilla::MemoryOrdering::ReleaseAcquire>
-      counter(0);
-
-  uint32_t maxPadLines = ((gc::SystemPageSize() * 3) / 4) / sizeof(zeroes);
-  uint32_t padLines = counter++ % maxPadLines;
-  for (uint32_t i = 0; i < padLines; i++) {
-    masm.appendRawCode(zeroes, sizeof(zeroes));
-  }
-}
-
 static constexpr unsigned LAZY_STUB_LIFO_DEFAULT_CHUNK_SIZE = 8 * 1024;
 
 bool Code::createManyLazyEntryStubs(const WriteGuard& guard,
@@ -617,10 +539,6 @@
   JitContext jitContext;
   WasmMacroAssembler masm(alloc);
 
-  if (funcExportIndices.length() == 1) {
-    PadCodeForSingleStub(masm);
-  }
-
   const FuncExportVector& funcExports = tierCodeBlock.funcExports;
   uint8_t* segmentBase = tierCodeBlock.segment->base();
 
@@ -655,44 +573,43 @@
     return false;
   }
 
-  size_t offsetInSegment = 0;
-  size_t codeLength = 0;
-  CodeSegment* segment =
-      AllocateCodePagesFrom(guard->lazyStubSegments, masm.bytesNeeded(),
-                            /* allowLastDitchGC = */ true, &offsetInSegment,
-                            &codeLength)
-          .get();
-  if (!segment) {
-    return false;
-  }
-  uint8_t* codePtr = segment->base() + offsetInSegment;
-  MOZ_ASSERT(CodeSegment::IsAligned(codeLength));
-
   UniqueCodeBlock stubCodeBlock =
       MakeUnique<CodeBlock>(CodeBlockKind::LazyStubs);
   if (!stubCodeBlock) {
     return false;
   }
-  stubCodeBlock->segment = segment;
-  stubCodeBlock->codeBase = codePtr;
+
+  // Allocate space in a code segment we can use
+  uint32_t codeLength = masm.bytesNeeded();
+  uint8_t* allocationStart;
+  uint8_t* codeStart;
+  uint32_t allocationLength;
+  stubCodeBlock->segment = CodeSegment::claimSpaceFromPool(
+      codeLength, &guard->lazyStubSegments,
+      /* allowLastDitchGC = */ true, &allocationStart, &codeStart,
+      &allocationLength);
+  if (!stubCodeBlock->segment) {
+    return false;
+  }
+
+  // Copy, link, and make the code executable
+  {
+    AutoMarkJitCodeWritableForThread writable;
+
+    masm.executableCopy(codeStart);
+    memset(codeStart + codeLength, 0, allocationLength - codeLength);
+
+    if (!stubCodeBlock->segment->linkAndMakeExecutableSubRange(
+            writable, masm, allocationStart, codeStart, allocationLength)) {
+      return false;
+    }
+  }
+
+  stubCodeBlock->codeBase = codeStart;
   stubCodeBlock->codeLength = codeLength;
   stubCodeBlock->codeRanges = std::move(codeRanges);
 
-  {
-    AutoMarkJitCodeWritableForThread writable;
-    masm.executableCopy(codePtr);
-    PatchDebugSymbolicAccesses(codePtr, masm);
-    memset(codePtr + masm.bytesNeeded(), 0, codeLength - masm.bytesNeeded());
-
-    for (const CodeLabel& label : masm.codeLabels()) {
-      Assembler::Bind(codePtr, label);
-    }
-  }
-
-  if (!ExecutableAllocator::makeExecutableAndFlushICache(codePtr, codeLength)) {
-    return false;
-  }
-
+  uint32_t offsetInSegment = codeStart - stubCodeBlock->segment->base();
   *stubBlockIndex = guard->blocks.length();
 
   uint32_t codeRangeIndex = 0;
@@ -1013,6 +930,49 @@
          blockPtr->initialize(*this, codeBlockIndex);
 }
 
+SharedCodeSegment Code::createFuncCodeSegmentFromPool(
+    jit::MacroAssembler& masm, const LinkData& linkData, bool allowLastDitchGC,
+    uint8_t** codeStartOut, uint32_t* codeLengthOut) const {
+  uint32_t codeLength = masm.bytesNeeded();
+
+  // Allocate space in a code segment we can use
+  uint8_t* allocationStart;
+  uint8_t* codeStart;
+  uint32_t allocationLength;
+  SharedCodeSegment segment;
+  {
+    auto guard = data_.writeLock();
+    segment = CodeSegment::claimSpaceFromPool(
+        codeLength, &guard->lazyFuncSegments, allowLastDitchGC,
+        &allocationStart, &codeStart, &allocationLength);
+    if (!segment) {
+      return nullptr;
+    }
+  }
+
+  // Update allocation statistics
+  {
+    auto guard = codeMeta().stats.writeLock();
+    guard->partialCodeBytesMapped += allocationLength;
+    guard->partialCodeBytesUsed += codeLength;
+  }
+
+  // Copy and link the function code
+  Maybe<AutoMarkJitCodeWritableForThread> writable;
+  writable.emplace();
+
+  masm.executableCopy(codeStart);
+  if (!segment->linkAndMakeExecutableSubRange(*writable, linkData, this,
+                                              allocationStart, codeStart,
+                                              allocationLength)) {
+    return nullptr;
+  }
+
+  *codeStartOut = codeStart;
+  *codeLengthOut = codeLength;
+  return segment;
+}
+
 const LazyFuncExport* Code::lookupLazyFuncExport(const WriteGuard& guard,
                                                  uint32_t funcIndex) const {
   size_t match;