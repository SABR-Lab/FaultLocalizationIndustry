# ==============================================================================
# REGRESSOR COMMIT DIFF
# ==============================================================================
# File: js/src/wasm/WasmCode.cpp
# Commit: 1a3bc285395f
# Full Hash: 1a3bc285395ffcc5d9281dd45d4f6c2bb918bf43
# Author: Julien Pages <jpages@mozilla.com>
# Date: 2025-03-04 04:58:00
# Regressor Bug: 1861078
# File Overlap Count: 1
# Description:
#   Bug 1861078 - wasm: make metadata offsets relative to CodeBlock->codeBase. r=rhunt
#   
#   Metadata used offsets to the beginning of code segment. Change that to use
#   offsets relative to the beginning of CodeBlock. This allows us to avoid offseting
#   metadata later on and prepare the work for simplification of stackmaps.
# ==============================================================================

diff -r 66d70b730ea6 -r 1a3bc285395f js/src/wasm/WasmCode.cpp
--- a/js/src/wasm/WasmCode.cpp	Mon Mar 03 16:15:17 2025 +0000
+++ b/js/src/wasm/WasmCode.cpp	Mon Mar 03 16:37:47 2025 +0000
@@ -143,7 +143,7 @@
     const CodeBlock& bestBlock = maybeCode->funcCodeBlock(far.targetFuncIndex);
     uint32_t stubRangeIndex = bestBlock.funcToCodeRange[far.targetFuncIndex];
     const CodeRange& stubRange = bestBlock.codeRanges[stubRangeIndex];
-    uint8_t* stubBase = bestBlock.segment->base();
+    uint8_t* stubBase = bestBlock.base();
     MacroAssembler::patchFarJump(base + far.jumpOffset,
                                  stubBase + stubRange.funcUncheckedCallEntry());
   }
@@ -537,7 +537,6 @@
   stubCodeBlock->codeLength = codeLength;
   stubCodeBlock->codeRanges = std::move(codeRanges);
 
-  uint32_t offsetInSegment = codeStart - stubCodeBlock->segment->base();
   *stubBlockIndex = guard->blocks.length();
 
   uint32_t codeRangeIndex = 0;
@@ -548,22 +547,10 @@
     LazyFuncExport lazyExport(fe.funcIndex(), *stubBlockIndex, codeRangeIndex,
                               tierCodeBlock.kind);
 
-    // Offset the code range for the interp entry to where it landed in the
-    // segment.
-    CodeRange& interpRange = stubCodeBlock->codeRanges[codeRangeIndex];
-    MOZ_ASSERT(interpRange.isInterpEntry());
-    MOZ_ASSERT(interpRange.funcIndex() == fe.funcIndex());
-    interpRange.offsetBy(offsetInSegment);
     codeRangeIndex += 1;
 
-    // Offset the code range for the jit entry (if any) to where it landed in
-    // the segment.
     if (funcType.canHaveJitEntry()) {
-      CodeRange& jitRange = stubCodeBlock->codeRanges[codeRangeIndex];
-      MOZ_ASSERT(jitRange.isJitEntry());
-      MOZ_ASSERT(jitRange.funcIndex() == fe.funcIndex());
       codeRangeIndex += 1;
-      jitRange.offsetBy(offsetInSegment);
     }
 
     size_t exportIndex;
@@ -609,7 +596,6 @@
   }
 
   const CodeBlock& block = *guard->blocks[stubBlockIndex];
-  const CodeSegment& segment = *block.segment;
   const CodeRangeVector& codeRanges = block.codeRanges;
 
   const FuncExport& fe = tierCodeBlock.funcExports[funcExportIndex];
@@ -623,7 +609,7 @@
   const CodeRange& interpRange =
       codeRanges[codeRanges.length() - funcEntryRanges];
   MOZ_ASSERT(interpRange.isInterpEntry());
-  *interpEntry = segment.base() + interpRange.begin();
+  *interpEntry = block.base() + interpRange.begin();
 
   // The second created range is the jit entry
   if (funcType.canHaveJitEntry()) {
@@ -631,7 +617,7 @@
         codeRanges[codeRanges.length() - funcEntryRanges + 1];
     MOZ_ASSERT(jitRange.isJitEntry());
     jumpTables_.setJitEntry(jitRange.funcIndex(),
-                            segment.base() + jitRange.begin());
+                            block.base() + jitRange.begin());
   }
   return true;
 }
@@ -645,7 +631,7 @@
 
   const FuncExport& fe = **funcExport;
   if (fe.hasEagerStubs()) {
-    *interpEntry = codeBlock.segment->base() + fe.eagerInterpEntryOffset();
+    *interpEntry = codeBlock.base() + fe.eagerInterpEntryOffset();
     return true;
   }
 
@@ -818,12 +804,11 @@
     // Update jump vectors with pointers to tier-2 lazy entry stubs, if any.
     if (stub2Index) {
       const CodeBlock& block = *guard->blocks[*stub2Index];
-      const CodeSegment& segment = *block.segment;
       for (const CodeRange& cr : block.codeRanges) {
         if (!cr.isJitEntry()) {
           continue;
         }
-        jumpTables_.setJitEntry(cr.funcIndex(), segment.base() + cr.begin());
+        jumpTables_.setJitEntry(cr.funcIndex(), block.base() + cr.begin());
       }
     }
   }
@@ -831,7 +816,7 @@
   // And we update the jump vectors with pointers to tier-2 functions and eager
   // stubs.  Callers will continue to invoke tier-1 code until, suddenly, they
   // will invoke tier-2 code.  This is benign.
-  uint8_t* base = tier2CodePointer->segment->base();
+  uint8_t* base = tier2CodePointer->base();
   for (const CodeRange& cr : tier2CodePointer->codeRanges) {
     // These are racy writes that we just want to be visible, atomically,
     // eventually.  All hardware we care about will do this right.  But
@@ -925,8 +910,7 @@
     return nullptr;
   }
   const CodeBlock& block = *guard->blocks[fe->lazyStubBlockIndex];
-  const CodeSegment& segment = *block.segment;
-  return segment.base() + block.codeRanges[fe->funcCodeRangeIndex].begin();
+  return block.base() + block.codeRanges[fe->funcCodeRangeIndex].begin();
 }
 
 CodeBlock::~CodeBlock() {
@@ -1025,7 +1009,7 @@
     if (!desc) {
       return;
     }
-    uintptr_t start = uintptr_t(segment->base() + codeRange.begin());
+    uintptr_t start = uintptr_t(base() + codeRange.begin());
     uintptr_t size = codeRange.end() - codeRange.begin();
     funcIonSpewer.spewer.saveWasmProfile(start, size, desc);
   }
@@ -1038,7 +1022,7 @@
     if (!desc) {
       return;
     }
-    uintptr_t start = uintptr_t(segment->base() + codeRange.begin());
+    uintptr_t start = uintptr_t(base() + codeRange.begin());
     uintptr_t size = codeRange.end() - codeRange.begin();
     funcBaselineSpewer.spewer.saveProfile(start, size, desc);
   }
@@ -1061,7 +1045,7 @@
       return;
     }
 
-    uintptr_t start = uintptr_t(segment->base() + codeRange.begin());
+    uintptr_t start = uintptr_t(base() + codeRange.begin());
     uintptr_t size = codeRange.end() - codeRange.begin();
 
 #ifdef MOZ_VTUNE
@@ -1077,27 +1061,6 @@
   }
 }
 
-void CodeBlock::offsetMetadataBy(uint32_t delta) {
-  if (delta == 0) {
-    return;
-  }
-  for (CodeRange& cr : codeRanges) {
-    cr.offsetBy(delta);
-  }
-  callSites.offsetBy(delta);
-  trapSites.offsetBy(delta);
-  for (FuncExport& fe : funcExports) {
-    fe.offsetBy(delta);
-  }
-  stackMaps.offsetBy(delta);
-  for (TryNote& tn : tryNotes) {
-    tn.offsetBy(delta);
-  }
-  for (CodeRangeUnwindInfo& crui : codeRangeUnwindInfos) {
-    crui.offsetBy(delta);
-  }
-}
-
 void CodeBlock::addSizeOfMisc(MallocSizeOf mallocSizeOf, size_t* code,
                               size_t* data) const {
   segment->addSizeOfMisc(mallocSizeOf, code, data);
@@ -1113,12 +1076,12 @@
 }
 
 const CodeRange* CodeBlock::lookupRange(const void* pc) const {
-  CodeRange::OffsetInCode target((uint8_t*)pc - segment->base());
+  CodeRange::OffsetInCode target((uint8_t*)pc - base());
   return LookupInSorted(codeRanges, target);
 }
 
 bool CodeBlock::lookupCallSite(void* pc, CallSite* callSite) const {
-  uint32_t target = ((uint8_t*)pc) - segment->base();
+  uint32_t target = ((uint8_t*)pc) - base();
   return callSites.lookup(target, callSite);
 }
 
@@ -1127,7 +1090,7 @@
 }
 
 const wasm::TryNote* CodeBlock::lookupTryNote(const void* pc) const {
-  size_t target = (uint8_t*)pc - segment->base();
+  size_t target = (uint8_t*)pc - base();
 
   // We find the first hit (there may be multiple) to obtain the innermost
   // handler, which is why we cannot binary search here.
@@ -1143,7 +1106,7 @@
 bool CodeBlock::lookupTrap(void* pc, Trap* kindOut,
                            TrapSiteDesc* trapOut) const {
   MOZ_ASSERT(containsCodePC(pc));
-  uint32_t target = ((uint8_t*)pc) - segment->base();
+  uint32_t target = ((uint8_t*)pc) - base();
   return trapSites.lookup(target, kindOut, trapOut);
 }
 
@@ -1155,7 +1118,7 @@
 };
 
 const CodeRangeUnwindInfo* CodeBlock::lookupUnwindInfo(void* pc) const {
-  uint32_t target = ((uint8_t*)pc) - segment->base();
+  uint32_t target = ((uint8_t*)pc) - base();
   size_t match;
   const CodeRangeUnwindInfo* info = nullptr;
   if (BinarySearch(UnwindInfoPCOffset(codeRangeUnwindInfos), 0,
@@ -1229,7 +1192,7 @@
     return false;
   }
 
-  uint8_t* codeBase = sharedStubs.segment->base();
+  uint8_t* codeBase = sharedStubs.base();
   for (const CodeRange& cr : sharedStubs.codeRanges) {
     if (cr.isFunction()) {
       setTieringEntry(cr.funcIndex(), codeBase + cr.funcTierEntry());
@@ -1238,7 +1201,7 @@
     }
   }
 
-  codeBase = tier1.segment->base();
+  codeBase = tier1.base();
   for (const CodeRange& cr : tier1.codeRanges) {
     if (cr.isFunction()) {
       setTieringEntry(cr.funcIndex(), codeBase + cr.funcTierEntry());
@@ -1275,7 +1238,7 @@
 
   sharedStubs_ = sharedStubs.get();
   completeTier1_ = tier1CodeBlock.get();
-  trapCode_ = sharedStubs_->segment->base() + sharedStubsLinkData->trapOffset;
+  trapCode_ = sharedStubs_->base() + sharedStubsLinkData->trapOffset;
   if (!jumpTables_.initialize(mode_, *codeMeta_, *sharedStubs_,
                               *completeTier1_) ||
       !addCodeBlock(guard, std::move(sharedStubs),
@@ -1544,7 +1507,7 @@
       }
       printString(buf);
 
-      uint8_t* theCode = segment->base() + range.begin();
+      uint8_t* theCode = base() + range.begin();
       jit::Disassemble(theCode, range.end() - range.begin(), printString);
     }
   }